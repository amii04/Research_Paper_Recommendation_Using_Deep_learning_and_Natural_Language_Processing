{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118cec97",
   "metadata": {},
   "source": [
    "# 1 Section:Research Area Prediction (Large Scale classification): Using shallow Multi-Layer Perceptron (MLP) model\n",
    "                                                                 \n",
    "\n",
    "# 2 Section:Research Paper Recommendation for reading: Using sentence transformer model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cb120",
   "metadata": {},
   "source": [
    "## 1 Section:                                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adda69d",
   "metadata": {},
   "source": [
    "## Loading tools and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0cb9160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\AmiMistry\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ast import literal_eval\n",
    "# is used for safely evaluating strings containing Python literals or container displays\n",
    "# (e.g., lists, dictionaries) to their corresponding Python objects.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130dc39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.read_csv(\"C:/Users/AmiMistry/OneDrive/Desktop/SUMMER_INTERNSHIP/Dataset/arxiv_data_210930-054931.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4f201fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             terms  \\\n",
      "0                                        ['cs.LG']   \n",
      "1                               ['cs.LG', 'cs.AI']   \n",
      "2                    ['cs.LG', 'cs.CR', 'stat.ML']   \n",
      "3                               ['cs.LG', 'cs.CR']   \n",
      "4                                        ['cs.LG']   \n",
      "...                                            ...   \n",
      "41100                                  ['stat.ML']   \n",
      "41101                         ['stat.ML', 'cs.LG']   \n",
      "41102                           ['cs.CV', 'cs.IR']   \n",
      "41103  ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']   \n",
      "41104              ['stat.ML', 'cs.LG', 'math.OC']   \n",
      "\n",
      "                                                                                                                 titles  \\\n",
      "0      Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities   \n",
      "1           Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes   \n",
      "2                                                       Power up! Robust Graph Convolutional Network via Graph Powering   \n",
      "3                                                  Releasing Graph Neural Networks with Differential Privacy Guarantees   \n",
      "4                                   Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification   \n",
      "...                                                                                                                 ...   \n",
      "41100              An experimental study of graph-based semi-supervised classification with additional node information   \n",
      "41101                                                          Bayesian Differential Privacy through Posterior Sampling   \n",
      "41102                                       Mining Spatio-temporal Data on Industrialization from Historical Registries   \n",
      "41103                                                 Wav2Letter: an End-to-End ConvNet-based Speech Recognition System   \n",
      "41104                                                                                       Generalized Low Rank Models   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              abstracts  \n",
      "0      Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.  \n",
      "1                                                                                                                                                                  Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.  \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Graph convolutional networks (GCNs) are powerful tools for graph-structured\\ndata. However, they have been recently shown to be vulnerable to topological\\nattacks. To enhance adversarial robustness, we go beyond spectral graph theory\\nto robust graph theory. By challenging the classical graph Laplacian, we\\npropose a new convolution operator that is provably robust in the spectral\\ndomain and is incorporated in the GCN architecture to improve expressivity and\\ninterpretability. By extending the original graph to a sequence of graphs, we\\nalso propose a robust training paradigm that encourages transferability across\\ngraphs that span a range of spatial and spectral characteristics. The proposed\\napproaches are demonstrated in extensive experiments to simultaneously improve\\nperformance in both benign and adversarial situations.  \n",
      "3                                                                                                                                                                                                                        With the increasing popularity of Graph Neural Networks (GNNs) in several\\nsensitive applications like healthcare and medicine, concerns have been raised\\nover the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to\\nprivacy attacks, such as membership inference attacks, even if only blackbox\\naccess to the trained model is granted. To build defenses, differential privacy\\nhas emerged as a mechanism to disguise the sensitive data in training datasets.\\nFollowing the strategy of Private Aggregation of Teacher Ensembles (PATE),\\nrecent methods leverage a large ensemble of teacher models. These teachers are\\ntrained on disjoint subsets of private data and are employed to transfer\\nknowledge to a student model, which is then released with privacy guarantees.\\nHowever, splitting graph data into many disjoint training sets may destroy the\\nstructural information and adversely affect accuracy. We propose a new\\ngraph-specific scheme of releasing a student GNN, which avoids splitting\\nprivate training data altogether. The student GNN is trained using public data,\\npartly labeled privately using the teacher GNN models trained exclusively for\\neach query node. We theoretically analyze our approach in the R\\`{e}nyi\\ndifferential privacy framework and provide privacy guarantees. Besides, we show\\nthe solid experimental performance of our method compared to several baselines,\\nincluding the PATE baseline adapted for graph-structured data. Our anonymized\\ncode is available.  \n",
      "4                                                                                                                                                                                                                                               Machine learning solutions for pattern classification problems are nowadays\\nwidely deployed in society and industry. However, the lack of transparency and\\naccountability of most accurate models often hinders their safe use. Thus,\\nthere is a clear need for developing explainable artificial intelligence\\nmechanisms. There exist model-agnostic methods that summarize feature\\ncontributions, but their interpretability is limited to predictions made by\\nblack-box models. An open challenge is to develop models that have intrinsic\\ninterpretability and produce their own explanations, even for classes of models\\nthat are traditionally considered black boxes like (recurrent) neural networks.\\nIn this paper, we propose a Long-Term Cognitive Network for interpretable\\npattern classification of structured data. Our method brings its own mechanism\\nfor providing explanations by quantifying the relevance of each feature in the\\ndecision process. For supporting the interpretability without affecting the\\nperformance, the model incorporates more flexibility through a quasi-nonlinear\\nreasoning rule that allows controlling nonlinearity. Besides, we propose a\\nrecurrence-aware decision model that evades the issues posed by unique fixed\\npoints while introducing a deterministic learning method to compute the tunable\\nparameters. The simulations show that our interpretable model obtains\\ncompetitive results when compared to the state-of-the-art white and black-box\\nmodels.  \n",
      "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...  \n",
      "41100                                                                                                                                                                                                                                                                                                                                The volume of data generated by internet and social networks is increasing\\nevery day, and there is a clear need for efficient ways of extracting useful\\ninformation from them. As those data can take different forms, it is important\\nto use all the available data representations for prediction.\\n  In this paper, we focus our attention on supervised classification using both\\nregular plain, tabular, data and structural information coming from a network\\nstructure. 14 techniques are investigated and compared in this study and can be\\ndivided in three classes: the first one uses only the plain data to build a\\nclassification model, the second uses only the graph structure and the last\\nuses both information sources. The relative performances in these three cases\\nare investigated. Furthermore, the effect of using a graph embedding and\\nwell-known indicators in spatial statistics is also studied.\\n  Possible applications are automatic classification of web pages or other\\nlinked documents, of people in a social network or of proteins in a biological\\ncomplex system, to name a few.\\n  Based on our comparison, we draw some general conclusions and advices to\\ntackle this particular classification task: some datasets can be better\\nexplained by their graph structure (graph-driven), or by their feature set\\n(features-driven). The most efficient methods are discussed in both cases.  \n",
      "41101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Differential privacy formalises privacy-preserving mechanisms that provide\\naccess to a database. We pose the question of whether Bayesian inference itself\\ncan be used directly to provide private access to data, with no modification.\\nThe answer is affirmative: under certain conditions on the prior, sampling from\\nthe posterior distribution can be used to achieve a desired level of privacy\\nand utility. To do so, we generalise differential privacy to arbitrary dataset\\nmetrics, outcome spaces and distribution families. This allows us to also deal\\nwith non-i.i.d or non-tabular datasets. We prove bounds on the sensitivity of\\nthe posterior to the data, which gives a measure of robustness. We also show\\nhow to use posterior sampling to provide differentially private responses to\\nqueries, within a decision-theoretic framework. Finally, we provide bounds on\\nthe utility and on the distinguishability of datasets. The latter are\\ncomplemented by a novel use of Le Cam's method to obtain lower bounds. All our\\ngeneral results hold for arbitrary database metrics, including those for the\\ncommon definition of differential privacy. For specific choices of the metric,\\nwe give a number of examples satisfying our assumptions.  \n",
      "41102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Despite the growing availability of big data in many fields, historical data\\non socioevironmental phenomena are often not available due to a lack of\\nautomated and scalable approaches for collecting, digitizing, and assembling\\nthem. We have developed a data-mining method for extracting tabulated, geocoded\\ndata from printed directories. While scanning and optical character recognition\\n(OCR) can digitize printed text, these methods alone do not capture the\\nstructure of the underlying data. Our pipeline integrates both page layout\\nanalysis and OCR to extract tabular, geocoded data from structured text. We\\ndemonstrate the utility of this method by applying it to scanned manufacturing\\nregistries from Rhode Island that record 41 years of industrial land use. The\\nresulting spatio-temporal data can be used for socioenvironmental analyses of\\nindustrialization at a resolution that was not previously possible. In\\nparticular, we find strong evidence for the dispersion of manufacturing from\\nthe urban core of Providence, the state's capital, along the Interstate 95\\ncorridor to the north and south.  \n",
      "41103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This paper presents a simple end-to-end model for speech recognition,\\ncombining a convolutional network based acoustic model and a graph decoding. It\\nis trained to output letters, with transcribed speech, without the need for\\nforce alignment of phonemes. We introduce an automatic segmentation criterion\\nfor training from sequence annotation without alignment that is on par with CTC\\nwhile being simpler. We show competitive results in word error rate on the\\nLibrispeech corpus with MFCC features, and promising results from raw waveform.  \n",
      "41104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Principal components analysis (PCA) is a well-known technique for\\napproximating a tabular data set by a low rank matrix. Here, we extend the idea\\nof PCA to handle arbitrary data sets consisting of numerical, Boolean,\\ncategorical, ordinal, and other data types. This framework encompasses many\\nwell known techniques in data analysis, such as nonnegative matrix\\nfactorization, matrix completion, sparse and robust PCA, $k$-means, $k$-SVD,\\nand maximum margin matrix factorization. The method handles heterogeneous data\\nsets, and leads to coherent schemes for compressing, denoising, and imputing\\nmissing entries across all data types simultaneously. It also admits a number\\nof interesting interpretations of the low rank factors, which allow clustering\\nof examples or of features. We propose several parallel algorithms for fitting\\ngeneralized low rank models, and describe implementations and numerical\\nresults.  \n",
      "\n",
      "[41105 rows x 3 columns]\n",
      "                                                                            terms  \\\n",
      "0                                                                       ['cs.LG']   \n",
      "1                                                              ['cs.LG', 'cs.AI']   \n",
      "2                                                   ['cs.LG', 'cs.CR', 'stat.ML']   \n",
      "3                                                              ['cs.LG', 'cs.CR']   \n",
      "4                                                                       ['cs.LG']   \n",
      "5                                                            ['cs.LG', 'stat.ML']   \n",
      "6                                                                       ['cs.LG']   \n",
      "7                                                            ['cs.LG', 'stat.ML']   \n",
      "8                                                                       ['cs.LG']   \n",
      "9                                                              ['cs.LG', 'cs.AI']   \n",
      "10                                                                      ['cs.LG']   \n",
      "11                                                    ['cs.LG', 'cs.AI', 'cs.DC']   \n",
      "12                                                                      ['cs.LG']   \n",
      "13                               ['cs.LG', 'cs.IT', 'math.IT', 'physics.data-an']   \n",
      "14                                                                      ['cs.LG']   \n",
      "15                                                    ['cs.LG', 'cs.AI', 'cs.SI']   \n",
      "16                                                    ['cs.LG', 'cs.AI', 'cs.DS']   \n",
      "17                                                    ['cs.LG', '68T30', 'I.5.4']   \n",
      "18                                                                      ['cs.LG']   \n",
      "19                                                                      ['cs.LG']   \n",
      "20                                      ['cs.LG', 'cs.CG', 'q-bio.QM', 'stat.ML']   \n",
      "21                                                                      ['cs.LG']   \n",
      "22                                                                      ['cs.LG']   \n",
      "23                                                                      ['cs.LG']   \n",
      "24                                                                      ['cs.LG']   \n",
      "25                                                                      ['cs.LG']   \n",
      "26                                                             ['cs.CV', 'cs.CL']   \n",
      "27                                                  ['cs.LG', 'cs.CR', 'stat.ML']   \n",
      "28                                                             ['cs.CV', 'cs.LG']   \n",
      "29                                                                      ['cs.LG']   \n",
      "30                                                             ['cs.LG', 'cs.AI']   \n",
      "31                                                  ['cs.LG', 'stat.ML', '68T05']   \n",
      "32                                                             ['cs.LG', 'cs.DC']   \n",
      "33                                                             ['cs.LG', 'cs.AI']   \n",
      "34                  ['cs.LG', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'stat.ML']   \n",
      "35                                                                      ['cs.CV']   \n",
      "36                                                  ['cs.LG', 'cs.DC', 'stat.ML']   \n",
      "37                                                                      ['cs.LG']   \n",
      "38                                                                      ['cs.LG']   \n",
      "39                                                             ['cs.LG', 'cs.SI']   \n",
      "40                                                  ['cs.LG', 'cs.SD', 'eess.AS']   \n",
      "41                                                             ['cs.LG', 'cs.AI']   \n",
      "42                                                                      ['cs.LG']   \n",
      "43                                                             ['cs.LG', 'cs.AI']   \n",
      "44                                         ['cs.LG', 'cs.IR', 'stat.ML', 'I.2.6']   \n",
      "45                                                             ['cs.LG', 'cs.AI']   \n",
      "46                                                  ['cs.LG', 'cs.SY', 'eess.SY']   \n",
      "47                                                                      ['cs.LG']   \n",
      "48  ['cs.CV', '68T45 (Primary) 68T10, 68T07 (Secondary)', 'I.4.9; I.5.4; I.2.10']   \n",
      "49                                                                      ['cs.LG']   \n",
      "\n",
      "                                                                                                              titles  \\\n",
      "0   Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities   \n",
      "1        Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes   \n",
      "2                                                    Power up! Robust Graph Convolutional Network via Graph Powering   \n",
      "3                                               Releasing Graph Neural Networks with Differential Privacy Guarantees   \n",
      "4                                Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification   \n",
      "5                                                                                            Lifelong Graph Learning   \n",
      "6                                                     Bayesian graph convolutional neural networks via tempered MCMC   \n",
      "7                                Understanding and Resolving Performance Degradation in Graph Convolutional Networks   \n",
      "8                                      Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs   \n",
      "9                                                                         Variational Graph Normalized Auto-Encoders   \n",
      "10                                                                      Local Augmentation for Graph Neural Networks   \n",
      "11                                   FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks   \n",
      "12                                                          Diff-ResNets for Few-shot Learning -- an ODE Perspective   \n",
      "13                                                                Scale-invariant representation of machine learning   \n",
      "14                                           Generation of Synthetic Electronic Health Records Using a Federated GAN   \n",
      "15                                                              Sparsifying the Update Step in Graph Neural Networks   \n",
      "16                                                                       Computing Graph Descriptors on Edge Streams   \n",
      "17                            Deep Dual Support Vector Data Description for Anomaly Detection on Attributed Networks   \n",
      "18                      Learning Fair Graph Neural Networks with Limited and Private Sensitive Attribute Information   \n",
      "19                                                                Adversarial Stein Training for Graph Energy Models   \n",
      "20                                        Parametric UMAP embeddings for representation and semi-supervised learning   \n",
      "21                                                                     Towards Self-Explainable Graph Neural Network   \n",
      "22                                                               Temporal Network Embedding via Tensor Factorization   \n",
      "23                            LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis   \n",
      "24                                             Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems   \n",
      "25                                                GIPA: General Information Propagation Algorithm for Graph Learning   \n",
      "26                                            StrucTexT: Structured Text Understanding with Multi-Modal Transformers   \n",
      "27                                                                                                    Graph Backdoor   \n",
      "28                            LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices   \n",
      "29                 On the Difficulty of Generalizing Reinforcement Learning Framework for Combinatorial Optimization   \n",
      "30              Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks   \n",
      "31                           Manifold Oblique Random Forests: Towards Closing the Gap on Convolutional Deep Networks   \n",
      "32                                       DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs   \n",
      "33                  Grain: Improving Data Efficiency of Graph Neural Networks via Diversified Influence Maximization   \n",
      "34                                                     Data-driven effective model shows a liquid-like deep learning   \n",
      "35                                                    CKConv: Learning Feature Voxelization for Point Cloud Analysis   \n",
      "36                                         Computing Graph Neural Networks: A Survey from Algorithms to Accelerators   \n",
      "37                                                      Ego-GNNs: Exploiting Ego Structures in Graph Neural Networks   \n",
      "38                                                               Adaptive Transfer Learning on Graph Neural Networks   \n",
      "39                        Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning   \n",
      "40                                                Deep Neural Networks and End-to-End Learning for Audio Compression   \n",
      "41                                  Beyond Low-pass Filtering: Graph Convolutional Networks with Automatic Filtering   \n",
      "42                                                                     Inter-domain Multi-relational Link Prediction   \n",
      "43                                                                            Multi-Level Graph Contrastive Learning   \n",
      "44                                                              Overlapping Spaces for Compact Graph Representations   \n",
      "45                                                   ARM-Net: Adaptive Relation Modeling Network for Structured Data   \n",
      "46                                                         Learning deep autoregressive models for hierarchical data   \n",
      "47                                                                     Edge Representation Learning with Hypergraphs   \n",
      "48                          Domain adaptation for person re-identification on new unlabeled data using AlignedReID++   \n",
      "49                         GCN-SL: Graph Convolutional Networks with Structure Learning for Graphs under Heterophily   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                abstracts  \n",
      "0                                                                                                                                                                                                                                        Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                    Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.  \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Graph convolutional networks (GCNs) are powerful tools for graph-structured\\ndata. However, they have been recently shown to be vulnerable to topological\\nattacks. To enhance adversarial robustness, we go beyond spectral graph theory\\nto robust graph theory. By challenging the classical graph Laplacian, we\\npropose a new convolution operator that is provably robust in the spectral\\ndomain and is incorporated in the GCN architecture to improve expressivity and\\ninterpretability. By extending the original graph to a sequence of graphs, we\\nalso propose a robust training paradigm that encourages transferability across\\ngraphs that span a range of spatial and spectral characteristics. The proposed\\napproaches are demonstrated in extensive experiments to simultaneously improve\\nperformance in both benign and adversarial situations.  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                          With the increasing popularity of Graph Neural Networks (GNNs) in several\\nsensitive applications like healthcare and medicine, concerns have been raised\\nover the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to\\nprivacy attacks, such as membership inference attacks, even if only blackbox\\naccess to the trained model is granted. To build defenses, differential privacy\\nhas emerged as a mechanism to disguise the sensitive data in training datasets.\\nFollowing the strategy of Private Aggregation of Teacher Ensembles (PATE),\\nrecent methods leverage a large ensemble of teacher models. These teachers are\\ntrained on disjoint subsets of private data and are employed to transfer\\nknowledge to a student model, which is then released with privacy guarantees.\\nHowever, splitting graph data into many disjoint training sets may destroy the\\nstructural information and adversely affect accuracy. We propose a new\\ngraph-specific scheme of releasing a student GNN, which avoids splitting\\nprivate training data altogether. The student GNN is trained using public data,\\npartly labeled privately using the teacher GNN models trained exclusively for\\neach query node. We theoretically analyze our approach in the R\\`{e}nyi\\ndifferential privacy framework and provide privacy guarantees. Besides, we show\\nthe solid experimental performance of our method compared to several baselines,\\nincluding the PATE baseline adapted for graph-structured data. Our anonymized\\ncode is available.  \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Machine learning solutions for pattern classification problems are nowadays\\nwidely deployed in society and industry. However, the lack of transparency and\\naccountability of most accurate models often hinders their safe use. Thus,\\nthere is a clear need for developing explainable artificial intelligence\\nmechanisms. There exist model-agnostic methods that summarize feature\\ncontributions, but their interpretability is limited to predictions made by\\nblack-box models. An open challenge is to develop models that have intrinsic\\ninterpretability and produce their own explanations, even for classes of models\\nthat are traditionally considered black boxes like (recurrent) neural networks.\\nIn this paper, we propose a Long-Term Cognitive Network for interpretable\\npattern classification of structured data. Our method brings its own mechanism\\nfor providing explanations by quantifying the relevance of each feature in the\\ndecision process. For supporting the interpretability without affecting the\\nperformance, the model incorporates more flexibility through a quasi-nonlinear\\nreasoning rule that allows controlling nonlinearity. Besides, we propose a\\nrecurrence-aware decision model that evades the issues posed by unique fixed\\npoints while introducing a deterministic learning method to compute the tunable\\nparameters. The simulations show that our interpretable model obtains\\ncompetitive results when compared to the state-of-the-art white and black-box\\nmodels.  \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Graph neural networks (GNNs) are powerful models for many graph-structured\\ntasks. Existing models often assume that a complete structure of a graph is\\navailable during training, however, in practice, graph-structured data is\\nusually formed in a streaming fashion, so that learning a graph continuously is\\noften necessary. In this paper, we aim to bridge GNN to lifelong learning by\\nconverting a graph problem to a regular learning problem, so that GNN is able\\nto inherit the lifelong learning techniques developed for convolutional neural\\nnetworks (CNNs). To this end, we propose a new graph topology based on feature\\ncross-correlation, called the feature graph. It takes features as new nodes and\\nturns nodes into independent graphs. This successfully converts the original\\nproblem of node classification to graph classification, in which the increasing\\nnodes are turned into independent training samples. In the experiments, we\\ndemonstrate the efficiency and effectiveness of feature graph networks (FGN) by\\ncontinuously learning a sequence of classical graph datasets. We also show that\\nFGN achieves superior performance in human action recognition with distributed\\nstreaming signals for wearable devices.  \n",
      "6                                                                                                                                                                                                                                                                                                                                                             Deep learning models, such as convolutional neural networks, have long been\\napplied to image and multi-media tasks, particularly those with structured\\ndata. More recently, there has been more attention to unstructured data that\\ncan be represented via graphs. These types of data are often found in health\\nand medicine, social networks, and research data repositories. Graph\\nconvolutional neural networks have recently gained attention in the field of\\ndeep learning that takes advantage of graph-based data representation with\\nautomatic feature extraction via convolutions. Given the popularity of these\\nmethods in a wide range of applications, robust uncertainty quantification is\\nvital. This remains a challenge for large models and unstructured datasets.\\nBayesian inference provides a principled approach to uncertainty quantification\\nof model parameters for deep learning models. Although Bayesian inference has\\nbeen used extensively elsewhere, its application to deep learning remains\\nlimited due to the computational requirements of the Markov Chain Monte Carlo\\n(MCMC) methods. Recent advances in parallel computing and advanced proposal\\nschemes in MCMC sampling methods has opened the path for Bayesian deep\\nlearning. In this paper, we present Bayesian graph convolutional neural\\nnetworks that employ tempered MCMC sampling with Langevin-gradient proposal\\ndistribution implemented via parallel computing. Our results show that the\\nproposed method can provide accuracy similar to advanced optimisers while\\nproviding uncertainty quantification for key benchmark problems.  \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       A Graph Convolutional Network (GCN) stacks several layers and in each layer\\nperforms a PROPagation operation (PROP) and a TRANsformation operation (TRAN)\\nfor learning node representations over graph-structured data. Though powerful,\\nGCNs tend to suffer performance drop when the model gets deep. Previous works\\nfocus on PROPs to study and mitigate this issue, but the role of TRANs is\\nbarely investigated. In this work, we study performance degradation of GCNs by\\nexperimentally examining how stacking only TRANs or PROPs works. We find that\\nTRANs contribute significantly, or even more than PROPs, to declining\\nperformance, and moreover that they tend to amplify node-wise feature variance\\nin GCNs, causing variance inflammation that we identify as a key factor for\\ncausing performance drop. Motivated by such observations, we propose a\\nvariance-controlling technique termed Node Normalization (NodeNorm), which\\nscales each node's features using its own standard deviation. Experimental\\nresults validate the effectiveness of NodeNorm on addressing performance\\ndegradation of GCNs. Specifically, it enables deep GCNs to outperform shallow\\nones in cases where deep models are needed, and to achieve comparable results\\nwith shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and\\ncan well generalize to other GNN architectures. Code is publicly available at\\nhttps://github.com/miafei/NodeNorm.  \n",
      "8                                                                                                                                                                                                                                                                                                                                             Transformer neural networks have achieved state-of-the-art results for\\nunstructured data such as text and images but their adoption for\\ngraph-structured data has been limited. This is partly due to the difficulty of\\nincorporating complex structural information in the basic transformer\\nframework. We propose a simple yet powerful extension to the transformer -\\nresidual edge channels. The resultant framework, which we call Edge-augmented\\nGraph Transformer (EGT), can directly accept, process and output structural\\ninformation as well as node information. It allows us to use global\\nself-attention, the key element of transformers, directly for graphs and comes\\nwith the benefit of long-range interaction among nodes. Moreover, the edge\\nchannels allow the structural information to evolve from layer to layer, and\\nprediction tasks on edges/links can be performed directly from the output\\nembeddings of these channels. In addition, we introduce a generalized\\npositional encoding scheme for graphs based on Singular Value Decomposition\\nwhich can improve the performance of EGT. Our framework, which relies on global\\nnode feature aggregation, achieves better performance compared to\\nConvolutional/Message-Passing Graph Neural Networks, which rely on local\\nfeature aggregation within a neighborhood. We verify the performance of EGT in\\na supervised learning setting on a wide range of experiments on benchmark\\ndatasets. Our findings indicate that convolutional aggregation is not an\\nessential inductive bias for graphs and global self-attention can serve as a\\nflexible and adaptive alternative.  \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Link prediction is one of the key problems for graph-structured data. With\\nthe advancement of graph neural networks, graph autoencoders (GAEs) and\\nvariational graph autoencoders (VGAEs) have been proposed to learn graph\\nembeddings in an unsupervised way. It has been shown that these methods are\\neffective for link prediction tasks. However, they do not work well in link\\npredictions when a node whose degree is zero (i.g., isolated node) is involved.\\nWe have found that GAEs/VGAEs make embeddings of isolated nodes close to zero\\nregardless of their content features. In this paper, we propose a novel\\nVariational Graph Normalized AutoEncoder (VGNAE) that utilize L2-normalization\\nto derive better embeddings for isolated nodes. We show that our VGNAEs\\noutperform the existing state-of-the-art models for link prediction tasks. The\\ncode is available at https://github.com/SeongJinAhn/VGNAE.  \n",
      "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Data augmentation has been widely used in image data and linguistic data but\\nremains under-explored on graph-structured data. Existing methods focus on\\naugmenting the graph data from a global perspective and largely fall into two\\ngenres: structural manipulation and adversarial training with feature noise\\ninjection. However, the structural manipulation approach suffers information\\nloss issues while the adversarial training approach may downgrade the feature\\nquality by injecting noise. In this work, we introduce the local augmentation,\\nwhich enhances node features by its local subgraph structures. Specifically, we\\nmodel the data argumentation as a feature generation process. Given the central\\nnode's feature, our local augmentation approach learns the conditional\\ndistribution of its neighbors' features and generates the neighbors' optimal\\nfeature to boost the performance of downstream tasks. Based on the local\\naugmentation, we further design a novel framework: LA-GNN, which can apply to\\nany GNN models in a plug-and-play manner. Extensive experiments and analyses\\nshow that local augmentation consistently yields performance improvement for\\nvarious GNN architectures across a diverse set of benchmarks. Code is available\\nat https://github.com/Soughing0823/LAGNN.  \n",
      "11                                                                                                                                                                               Graph Neural Network (GNN) research is rapidly growing thanks to the capacity\\nof GNNs in learning distributed representations from graph-structured data.\\nHowever, centralizing a massive amount of real-world graph data for GNN\\ntraining is prohibitive due to privacy concerns, regulation restrictions, and\\ncommercial competitions. Federated learning (FL), a trending distributed\\nlearning paradigm, provides possibilities to solve this challenge while\\npreserving data privacy. Despite recent advances in vision and language\\ndomains, there is no suitable platform for the FL of GNNs. To this end, we\\nintroduce FedGraphNN, an open FL benchmark system that can facilitate research\\non federated GNNs. FedGraphNN is built on a unified formulation of graph FL and\\ncontains a wide range of datasets from different domains, popular GNN models,\\nand FL algorithms, with secure and efficient system support. Particularly for\\nthe datasets, we collect, preprocess, and partition 36 datasets from 7 domains,\\nincluding both publicly available ones and specifically obtained ones such as\\nhERG and Tencent. Our empirical analysis showcases the utility of our benchmark\\nsystem, while exposing significant challenges in graph FL: federated GNNs\\nperform worse in most datasets with a non-IID split than centralized GNNs; the\\nGNN model that attains the best result in the centralized setting may not\\nmaintain its advantage in the FL setting. These results imply that more\\nresearch efforts are needed to unravel the mystery behind federated GNNs.\\nMoreover, our system performance analysis demonstrates that the FedGraphNN\\nsystem is computationally efficient and secure to large-scale graphs datasets.\\nWe maintain the source code at https://github.com/FedML-AI/FedGraphNN.  \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Interpreting deep neural networks from the ordinary differential equations\\n(ODEs) perspective has inspired many efficient and robust network\\narchitectures. However, existing ODE based approaches ignore the relationship\\namong data points, which is a critical component in many problems including\\nfew-shot learning and semi-supervised learning. In this paper, inspired by the\\ndiffusive ODEs, we propose a novel diffusion residual network (Diff-ResNet) to\\nstrengthen the interactions among data points. Under the structured data\\nassumption, it is proved that the diffusion mechanism can decrease the\\ndistance-diameter ratio that improves the separability of inter-class points\\nand reduces the distance among local intra-class points. This property can be\\neasily adopted by the residual networks for constructing the separable\\nhyperplanes. The synthetic binary classification experiments demonstrate the\\neffectiveness of the proposed diffusion mechanism. Moreover, extensive\\nexperiments of few-shot image classification and semi-supervised graph node\\nclassification in various datasets validate the advantages of the proposed\\nDiff-ResNet over existing few-shot learning methods.  \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The success of machine learning stems from its structured data\\nrepresentation. Similar data have close representation as compressed codes for\\nclassification or emerged labels for clustering. We observe that the frequency\\nof the internal representation follows power laws in both supervised and\\nunsupervised learning. The scale-invariant distribution implies that machine\\nlearning largely compresses frequent typical data, and at the same time,\\ndifferentiates many atypical data as outliers. In this study, we derive how the\\npower laws can naturally arise in machine learning. In terms of information\\ntheory, the scale-invariant representation corresponds to a maximally uncertain\\ndata grouping among possible representations that guarantee pre-specified\\nlearning accuracy.  \n",
      "14                                                                                                                                                                                                                                                        Sensitive medical data is often subject to strict usage constraints. In this\\npaper, we trained a generative adversarial network (GAN) on real-world\\nelectronic health records (EHR). It was then used to create a data-set of\\n\"fake\" patients through synthetic data generation (SDG) to circumvent usage\\nconstraints. This real-world data was tabular, binary, intensive care unit\\n(ICU) patient diagnosis data. The entire data-set was split into separate data\\nsilos to mimic real-world scenarios where multiple ICU units across different\\nhospitals may have similarly structured data-sets within their own\\norganisations but do not have access to each other's data-sets. We implemented\\nfederated learning (FL) to train separate GANs locally at each organisation,\\nusing their unique data silo and then combining the GANs into a single central\\nGAN, without any siloed data ever being exposed. This global, central GAN was\\nthen used to generate the synthetic patients data-set. We performed an\\nevaluation of these synthetic patients with statistical measures and through a\\nstructured review by a group of medical professionals. It was shown that there\\nwas no significant reduction in the quality of the synthetic EHR when we moved\\nbetween training a single central model and training on separate data silos\\nwith individual models before combining them into a central model. This was\\ntrue for both the statistical evaluation (Root Mean Square Error (RMSE) of\\n0.0154 for single-source vs. RMSE of 0.0169 for dual-source federated) and also\\nfor the medical professionals' evaluation (no quality difference between EHR\\ngenerated from a single source and EHR generated from multiple sources).  \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                      Message-Passing Neural Networks (MPNNs), the most prominent Graph Neural\\nNetwork (GNN) framework, celebrate much success in the analysis of\\ngraph-structured data. Concurrently, the sparsification of Neural Network\\nmodels attracts a great amount of academic and industrial interest. In this\\npaper, we conduct a structured study of the effect of sparsification on the\\ntrainable part of MPNNs known as the Update step. To this end, we design a\\nseries of models to successively sparsify the linear transform in the Update\\nstep. Specifically, we propose the ExpanderGNN model with a tuneable\\nsparsification rate and the Activation-Only GNN, which has no linear transform\\nin the Update step. In agreement with a growing trend in the literature, the\\nsparsification paradigm is changed by initialising sparse neural network\\narchitectures rather than expensively sparsifying already trained\\narchitectures. Our novel benchmark models enable a better understanding of the\\ninfluence of the Update step on model performance and outperform existing\\nsimplified benchmark models such as the Simple Graph Convolution. The\\nExpanderGNNs, and in some cases the Activation-Only models, achieve performance\\non par with their vanilla counterparts on several downstream tasks while\\ncontaining significantly fewer trainable parameters. In experiments with\\nmatching parameter numbers, our benchmark models outperform the\\nstate-of-the-art GNN models. Our code is publicly available at:\\nhttps://github.com/ChangminWu/ExpanderGNN.  \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Graph feature extraction is a fundamental task in graphs analytics. Using\\nfeature vectors (graph descriptors) in tandem with data mining algorithms that\\noperate on Euclidean data, one can solve problems such as classification,\\nclustering, and anomaly detection on graph-structured data. This idea has\\nproved fruitful in the past, with spectral-based graph descriptors providing\\nstate-of-the-art classification accuracy on benchmark datasets. However, these\\nalgorithms do not scale to large graphs since: 1) they require storing the\\nentire graph in memory, and 2) the end-user has no control over the algorithm's\\nruntime. In this paper, we present single-pass streaming algorithms to\\napproximate structural features of graphs (counts of subgraphs of order $k \\geq\\n4$). Operating on edge streams allows us to avoid keeping the entire graph in\\nmemory, and controlling the sample size enables us to control the time taken by\\nthe algorithm. We demonstrate the efficacy of our descriptors by analyzing the\\napproximation error, classification accuracy, and scalability to massive\\ngraphs. Our experiments showcase the effect of the sample size on approximation\\nerror and predictive accuracy. The proposed descriptors are applicable on\\ngraphs with millions of edges within minutes and outperform the\\nstate-of-the-art descriptors in classification accuracy.  \n",
      "17                                                                                                                                                                                                                                                                                                  Networks are ubiquitous in the real world such as social networks and\\ncommunication networks, and anomaly detection on networks aims at finding nodes\\nwhose structural or attributed patterns deviate significantly from the majority\\nof reference nodes. However, most of the traditional anomaly detection methods\\nneglect the relation structure information among data points and therefore\\ncannot effectively generalize to the graph structure data. In this paper, we\\npropose an end-to-end model of Deep Dual Support Vector Data description based\\nAutoencoder (Dual-SVDAE) for anomaly detection on attributed networks, which\\nconsiders both the structure and attribute for attributed networks.\\nSpecifically, Dual-SVDAE consists of a structure autoencoder and an attribute\\nautoencoder to learn the latent representation of the node in the structure\\nspace and attribute space respectively. Then, a dual-hypersphere learning\\nmechanism is imposed on them to learn two hyperspheres of normal nodes from the\\nstructure and attribute perspectives respectively. Moreover, to achieve joint\\nlearning between the structure and attribute of the network, we fuse the\\nstructure embedding and attribute embedding as the final input of the feature\\ndecoder to generate the node attribute. Finally, abnormal nodes can be detected\\nby measuring the distance of nodes to the learned center of each hypersphere in\\nthe latent structure space and attribute space respectively. Extensive\\nexperiments on the real-world attributed networks show that Dual-SVDAE\\nconsistently outperforms the state-of-the-arts, which demonstrates the\\neffectiveness of the proposed method.  \n",
      "18  Graph neural networks (GNNs) have shown great power in modeling graph\\nstructured data. However, similar to other machine learning models, GNNs may\\nmake biased predictions w.r.t protected sensitive attributes, e.g., skin color\\nand gender. This is because the training data often contains historical bias\\ntowards sensitive attributes. In addition, we empirically show that the\\ndiscrimination in GNNs can be magnified by graph structures and the\\nmessage-passing mechanism of GNNs. As a result, the applications of GNNs in\\nhigh-stake domains such as crime rate prediction would be largely limited.\\nThough extensive studies of fair classification have been conducted on i.i.d\\ndata, methods to address the problem of discrimination on non-i.i.d data are\\nrather limited. Generally, learning fair models require abundant sensitive\\nattributes to regularize the model. However, for many graphs such as social\\nnetworks, users are reluctant to share sensitive attributes. Thus, only limited\\nsensitive attributes are available for fair GNN training in practice. Moreover,\\ndirectly collecting and applying the sensitive attributes in fair model\\ntraining may cause privacy issues, because the sensitive information can be\\nleaked in data breach or attacks on the trained model. Therefore, we study a\\nnovel and crucial problem of learning fair GNNs with limited and private\\nsensitive attribute information. In an attempt to address these problems,\\nFairGNN is proposed to eliminate the bias of GNNs whilst maintaining high\\naccuracy by leveraging graph structures and limited sensitive information. We\\nfurther extend FairGNN to NT-FairGNN which can achieve both fairness and\\nprivacy on sensitive attributes by using limited and private sensitive\\nattributes. Theoretical analysis and extensive experiments on real-world\\ndatasets demonstrate the effectiveness of FairGNN and NT-FairGNN in achieving\\nfair and high-accurate classification.  \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Learning distributions over graph-structured data is a challenging task with\\nmany applications in biology and chemistry. In this work we use an energy-based\\nmodel (EBM) based on multi-channel graph neural networks (GNN) to learn\\npermutation invariant unnormalized density functions on graphs. Unlike standard\\nEBM training methods our approach is to learn the model via minimizing\\nadversarial stein discrepancy. Samples from the model can be obtained via\\nLangevin dynamics based MCMC. We find that this approach achieves competitive\\nresults on graph generation compared to benchmark models.  \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               UMAP is a non-parametric graph-based dimensionality reduction algorithm using\\napplied Riemannian geometry and algebraic topology to find low-dimensional\\nembeddings of structured data. The UMAP algorithm consists of two steps: (1)\\nCompute a graphical representation of a dataset (fuzzy simplicial complex), and\\n(2) Through stochastic gradient descent, optimize a low-dimensional embedding\\nof the graph. Here, we extend the second step of UMAP to a parametric\\noptimization over neural network weights, learning a parametric relationship\\nbetween data and embedding. We first demonstrate that Parametric UMAP performs\\ncomparably to its non-parametric counterpart while conferring the benefit of a\\nlearned parametric mapping (e.g. fast online embeddings for new data). We then\\nexplore UMAP as a regularization, constraining the latent distribution of\\nautoencoders, parametrically varying global structure preservation, and\\nimproving classifier accuracy for semi-supervised learning by capturing\\nstructure in unlabeled data. Google Colab walkthrough:\\nhttps://colab.research.google.com/drive/1WkXVZ5pnMrm17m0YgmtoNjM_XHdnE5Vp?usp=sharing  \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Graph Neural Networks (GNNs), which generalize the deep neural networks to\\ngraph-structured data, have achieved great success in modeling graphs. However,\\nas an extension of deep learning for graphs, GNNs lack explainability, which\\nlargely limits their adoption in scenarios that demand the transparency of\\nmodels. Though many efforts are taken to improve the explainability of deep\\nlearning, they mainly focus on i.i.d data, which cannot be directly applied to\\nexplain the predictions of GNNs because GNNs utilize both node features and\\ngraph topology to make predictions. There are only very few work on the\\nexplainability of GNNs and they focus on post-hoc explanations. Since post-hoc\\nexplanations are not directly obtained from the GNNs, they can be biased and\\nmisrepresent the true explanations. Therefore, in this paper, we study a novel\\nproblem of self-explainable GNNs which can simultaneously give predictions and\\nexplanations. We propose a new framework which can find $K$-nearest labeled\\nnodes for each unlabeled node to give explainable node classification, where\\nnearest labeled nodes are found by interpretable similarity module in terms of\\nboth node similarity and local structure similarity. Extensive experiments on\\nreal-world and synthetic datasets demonstrate the effectiveness of the proposed\\nframework for explainable node classification.  \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Representation learning on static graph-structured data has shown a\\nsignificant impact on many real-world applications. However, less attention has\\nbeen paid to the evolving nature of temporal networks, in which the edges are\\noften changing over time. The embeddings of such temporal networks should\\nencode both graph-structured information and the temporally evolving pattern.\\nExisting approaches in learning temporally evolving network representations\\nfail to capture the temporal interdependence. In this paper, we propose Toffee,\\na novel approach for temporal network representation learning based on tensor\\ndecomposition. Our method exploits the tensor-tensor product operator to encode\\nthe cross-time information, so that the periodic changes in the evolving\\nnetworks can be captured. Experimental results demonstrate that Toffee\\noutperforms existing methods on multiple real-world temporal networks in\\ngenerating effective embeddings for the link prediction tasks.  \n",
      "23      Graph structured data have enabled several successful applications such as\\nrecommendation systems and traffic prediction, given the rich node features and\\nedges information. However, these high-dimensional features and high-order\\nadjacency information are usually heterogeneous and held by different data\\nholders in practice. Given such vertical data partition (e.g., one data holder\\nwill only own either the node features or edge information), different data\\nholders have to develop efficient joint training protocols rather than directly\\ntransfer data to each other due to privacy concerns. In this paper, we focus on\\nthe edge privacy, and consider a training scenario where Bob with node features\\nwill first send training node features to Alice who owns the adjacency\\ninformation. Alice will then train a graph neural network (GNN) with the joint\\ninformation and release an inference API. During inference, Bob is able to\\nprovide test node features and query the API to obtain the predictions for test\\nnodes. Under this setting, we first propose a privacy attack LinkTeller via\\ninfluence analysis to infer the private edge information held by Alice via\\ndesigning adversarial queries for Bob. We then empirically show that LinkTeller\\nis able to recover a significant amount of private edges, outperforming\\nexisting baselines. To further evaluate the privacy leakage, we adapt an\\nexisting algorithm for differentially private graph convolutional network (DP\\nGCN) training and propose a new DP GCN mechanism LapGraph. We show that these\\nDP GCN mechanisms are not always resilient against LinkTeller empirically under\\nmild privacy guarantees ($\\varepsilon>5$). Our studies will shed light on\\nfuture research towards designing more resilient privacy-preserving GCN models;\\nin the meantime, provide an in-depth understanding of the tradeoff between GCN\\nmodel utility and robustness against potential privacy attacks.  \n",
      "24                                                                                                                                                                                                                                                                                                      Spatio-temporal forecasting is of great importance in a wide range of\\ndynamical systems applications from atmospheric science, to recent COVID-19\\nspread modeling. These applications rely on accurate predictions of\\nspatio-temporal structured data reflecting real-world phenomena. A stunning\\ncharacteristic is that the dynamical system is not only driven by some physics\\nlaws but also impacted by the localized factor in spatial and temporal regions.\\nOne of the major challenges is to infer the underlying causes, which generate\\nthe perceived data stream and propagate the involved causal dynamics through\\nthe distributed observing units. Another challenge is that the success of\\nmachine learning based predictive models requires massive annotated data for\\nmodel training. However, the acquisition of high-quality annotated data is\\nobjectively manual and tedious as it needs a considerable amount of human\\nintervention, making it infeasible in fields that require high levels of\\nexpertise. To tackle these challenges, we advocate a spatio-temporal\\nphysics-coupled neural networks (ST-PCNN) model to learn the underlying physics\\nof the dynamical system and further couple the learned physics to assist the\\nlearning of the recurring dynamics. To deal with data-acquisition constraints,\\nan active learning mechanism with Kriging for actively acquiring the most\\ninformative data is proposed for ST-PCNN training in a partially observable\\nenvironment. Our experiments on both synthetic and real-world datasets exhibit\\nthat the proposed ST-PCNN with active learning converges to near optimal\\naccuracy with substantially fewer instances.  \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Graph neural networks (GNNs) have been popularly used in analyzing\\ngraph-structured data, showing promising results in various applications such\\nas node classification, link prediction and network recommendation. In this\\npaper, we present a new graph attention neural network, namely GIPA, for\\nattributed graph data learning. GIPA consists of three key components:\\nattention, feature propagation and aggregation. Specifically, the attention\\ncomponent introduces a new multi-layer perceptron based multi-head to generate\\nbetter non-linear feature mapping and representation than conventional\\nimplementations such as dot-product. The propagation component considers not\\nonly node features but also edge features, which differs from existing GNNs\\nthat merely consider node features. The aggregation component uses a residual\\nconnection to generate the final embedding. We evaluate the performance of GIPA\\nusing the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The\\nexperimental results reveal that GIPA can beat the state-of-the-art models in\\nterms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of\\n$0.8700\\pm 0.0010$ and outperforms all the previous methods listed in the\\nogbn-proteins leaderboard.  \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Structured text understanding on Visually Rich Documents (VRDs) is a crucial\\npart of Document Intelligence. Due to the complexity of content and layout in\\nVRDs, structured text understanding has been a challenging task. Most existing\\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\\nlinking, which require an entire understanding of the context of documents at\\nboth token and segment levels. However, little work has been concerned with the\\nsolutions that efficiently extract the structured data from different levels.\\nThis paper proposes a unified framework named StrucTexT, which is flexible and\\neffective for handling both sub-tasks. Specifically, based on the transformer,\\nwe introduce a segment-token aligned encoder to deal with the entity labeling\\nand entity linking tasks at different levels of granularity. Moreover, we\\ndesign a novel pre-training strategy with three self-supervised tasks to learn\\na richer representation. StrucTexT uses the existing Masked Visual Language\\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\\ntasks to incorporate the multi-modal information across text, image, and\\nlayout. We evaluate our method for structured text understanding at\\nsegment-level and token-level and show it outperforms the state-of-the-art\\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\\nEPHOIE datasets.  \n",
      "27                                                                                                                                                                                                                                                                                                                                                    One intriguing property of deep neural networks (DNNs) is their inherent\\nvulnerability to backdoor attacks -- a trojan model responds to\\ntrigger-embedded inputs in a highly predictable manner while functioning\\nnormally otherwise. Despite the plethora of prior work on DNNs for continuous\\ndata (e.g., images), the vulnerability of graph neural networks (GNNs) for\\ndiscrete-structured data (e.g., graphs) is largely unexplored, which is highly\\nconcerning given their increasing use in security-sensitive domains. To bridge\\nthis gap, we present GTA, the first backdoor attack on GNNs. Compared with\\nprior work, GTA departs in significant ways: graph-oriented -- it defines\\ntriggers as specific subgraphs, including both topological structures and\\ndescriptive features, entailing a large design spectrum for the adversary;\\ninput-tailored -- it dynamically adapts triggers to individual graphs, thereby\\noptimizing both attack effectiveness and evasiveness; downstream model-agnostic\\n-- it can be readily launched without knowledge regarding downstream models or\\nfine-tuning strategies; and attack-extensible -- it can be instantiated for\\nboth transductive (e.g., node classification) and inductive (e.g., graph\\nclassification) tasks, constituting severe threats for a range of\\nsecurity-critical applications. Through extensive evaluation using benchmark\\ndatasets and state-of-the-art models, we demonstrate the effectiveness of GTA.\\nWe further provide analytical justification for its effectiveness and discuss\\npotential countermeasures, pointing to several promising research directions.  \n",
      "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Deep convolutional neural networks (CNNs) have shown outstanding performance\\nin the task of semantically segmenting images. Applying the same methods on 3D\\ndata still poses challenges due to the heavy memory requirements and the lack\\nof structured data. Here, we propose LatticeNet, a novel approach for 3D\\nsemantic segmentation, which takes raw point clouds as input. A PointNet\\ndescribes the local geometry which we embed into a sparse permutohedral\\nlattice. The lattice allows for fast convolutions while keeping a low memory\\nfootprint. Further, we introduce DeformSlice, a novel learned data-dependent\\ninterpolation for projecting lattice features back onto the point cloud. We\\npresent results of 3D segmentation on multiple datasets where our method\\nachieves state-of-the-art performance. We also extend and evaluate our network\\nfor instance and dynamic object segmentation.  \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Combinatorial optimization problems (COPs) on the graph with real-life\\napplications are canonical challenges in Computer Science. The difficulty of\\nfinding quality labels for problem instances holds back leveraging supervised\\nlearning across combinatorial problems. Reinforcement learning (RL) algorithms\\nhave recently been adopted to solve this challenge automatically. The\\nunderlying principle of this approach is to deploy a graph neural network (GNN)\\nfor encoding both the local information of the nodes and the graph-structured\\ndata in order to capture the current state of the environment. Then, it is\\nfollowed by the actor to learn the problem-specific heuristics on its own and\\nmake an informed decision at each state for finally reaching a good solution.\\nRecent studies on this subject mainly focus on a family of combinatorial\\nproblems on the graph, such as the travel salesman problem, where the proposed\\nmodel aims to find an ordering of vertices that optimizes a given objective\\nfunction. We use the security-aware phone clone allocation in the cloud as a\\nclassical quadratic assignment problem (QAP) to investigate whether or not deep\\nRL-based model is generally applicable to solve other classes of such hard\\nproblems. Extensive empirical evaluation shows that existing RL-based model may\\nnot generalize to QAP.  \n",
      "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Deep learning's performance has been extensively recognized recently. Graph\\nneural networks (GNNs) are designed to deal with graph-structural data that\\nclassical deep learning does not easily manage. Since most GNNs were created\\nusing distinct theories, direct comparisons are impossible. Prior research has\\nprimarily concentrated on categorizing existing models, with little attention\\npaid to their intrinsic connections. The purpose of this study is to establish\\na unified framework that integrates GNNs based on spectral graph and\\napproximation theory. The framework incorporates a strong integration between\\nspatial- and spectral-based GNNs while tightly associating approaches that\\nexist within each respective domain.  \n",
      "31                                              Decision forests (Forests), in particular random forests and gradient\\nboosting trees, have demonstrated state-of-the-art accuracy compared to other\\nmethods in many supervised learning scenarios. In particular, Forests dominate\\nother methods in tabular data, that is, when the feature space is unstructured,\\nso that the signal is invariant to a permutation of the feature indices.\\nHowever, in structured data lying on a manifold (such as images, text, and\\nspeech) deep networks (Networks), specifically convolutional deep networks\\n(ConvNets), tend to outperform Forests. We conjecture that at least part of the\\nreason for this is that the input to Networks is not simply the feature\\nmagnitudes, but also their indices. In contrast, naive Forest implementations\\nfail to explicitly consider feature indices. A recently proposed Forest\\napproach demonstrates that Forests, for each node, implicitly sample a random\\nmatrix from some specific distribution. These Forests, like some classes of\\nNetworks, learn by partitioning the feature space into convex polytopes\\ncorresponding to linear functions. We build on that approach and show that one\\ncan choose distributions in a manifold-aware fashion to incorporate feature\\nlocality. We demonstrate the empirical performance on data whose features live\\non three different manifolds: a torus, images, and time-series. Moreover, we\\ndemonstrate its strength in multivariate simulated settings and also show\\nsuperiority in predicting surgical outcome in epilepsy patients and predicting\\nmovement direction from raw stereotactic EEG data from non-motor brain regions.\\nIn all simulations and real data, Manifold Oblique Random Forest (MORF)\\nalgorithm outperforms approaches that ignore feature space structure and\\nchallenges the performance of ConvNets. Moreover, MORF runs fast and maintains\\ninterpretability and theoretical justification.  \n",
      "32                                                                                                    Graph neural networks (GNN) have shown great success in learning from\\ngraph-structured data. They are widely used in various applications, such as\\nrecommendation, fraud detection, and search. In these domains, the graphs are\\ntypically large, containing hundreds of millions of nodes and several billions\\nof edges. To tackle this challenge, we develop DistDGL, a system for training\\nGNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the\\nDeep Graph Library (DGL), a popular GNN development framework. DistDGL\\ndistributes the graph and its associated data (initial features and embeddings)\\nacross the machines and uses this distribution to derive a computational\\ndecomposition by following an owner-compute rule. DistDGL follows a synchronous\\ntraining approach and allows ego-networks forming the mini-batches to include\\nnon-local nodes. To minimize the overheads associated with distributed\\ncomputations, DistDGL uses a high-quality and light-weight min-cut graph\\npartitioning algorithm along with multiple balancing constraints. This allows\\nit to reduce communication overheads and statically balance the computations.\\nIt further reduces the communication by replicating halo nodes and by using\\nsparse embedding updates. The combination of these design choices allows\\nDistDGL to train high-quality models while achieving high parallel efficiency\\nand memory scalability. We demonstrate our optimizations on both inductive and\\ntransductive GNN models. Our results show that DistDGL achieves linear speedup\\nwithout compromising model accuracy and requires only 13 seconds to complete a\\ntraining epoch for a graph with 100 million nodes and 3 billion edges on a\\ncluster with 16 machines. DistDGL is now publicly available as part of\\nDGL:https://github.com/dmlc/dgl/tree/master/python/dgl/distributed.  \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Data selection methods, such as active learning and core-set selection, are\\nuseful tools for improving the data efficiency of deep learning models on\\nlarge-scale datasets. However, recent deep learning models have moved forward\\nfrom independent and identically distributed data to graph-structured data,\\nsuch as social networks, e-commerce user-item graphs, and knowledge graphs.\\nThis evolution has led to the emergence of Graph Neural Networks (GNNs) that go\\nbeyond the models existing data selection methods are designed for. Therefore,\\nwe present Grain, an efficient framework that opens up a new perspective\\nthrough connecting data selection in GNNs with social influence maximization.\\nBy exploiting the common patterns of GNNs, Grain introduces a novel feature\\npropagation concept, a diversified influence maximization objective with novel\\ninfluence and diversity functions, and a greedy algorithm with an approximation\\nguarantee into a unified framework. Empirical studies on public datasets\\ndemonstrate that Grain significantly improves both the performance and\\nefficiency of data selection (including active learning and core-set selection)\\nfor GNNs. To the best of our knowledge, this is the first attempt to bridge two\\nlargely parallel threads of research, data selection, and social influence\\nmaximization, in the setting of GNNs, paving new ways for improving data\\nefficiency.  \n",
      "34                       The geometric structure of an optimization landscape is argued to be\\nfundamentally important to support the success of deep neural network learning.\\nA direct computation of the landscape beyond two layers is hard. Therefore, to\\ncapture the global view of the landscape, an interpretable model of the\\nnetwork-parameter (or weight) space must be established. However, the model is\\nlacking so far. Furthermore, it remains unknown what the landscape looks like\\nfor deep networks of binary synapses, which plays a key role in robust and\\nenergy efficient neuromorphic computation. Here, we propose a statistical\\nmechanics framework by directly building a least structured model of the\\nhigh-dimensional weight space, considering realistic structured data,\\nstochastic gradient descent training, and the computational depth of neural\\nnetworks. We also consider whether the number of network parameters outnumbers\\nthe number of supplied training data, namely, over- or under-parametrization.\\nOur least structured model reveals that the weight spaces of the\\nunder-parametrization and over-parameterization cases belong to the same class,\\nin the sense that these weight spaces are well-connected without any\\nhierarchical clustering structure. In contrast, the shallow-network has a\\nbroken weight space, characterized by a discontinuous phase transition, thereby\\nclarifying the benefit of depth in deep learning from the angle of high\\ndimensional geometry. Our effective model also reveals that inside a deep\\nnetwork, there exists a liquid-like central part of the architecture in the\\nsense that the weights in this part behave as randomly as possible, providing\\nalgorithmic implications. Our data-driven model thus provides a statistical\\nmechanics insight about why deep learning is unreasonably effective in terms of\\nthe high-dimensional weight space, and how deep networks are different from\\nshallow ones.  \n",
      "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Despite the remarkable success of deep learning, optimal convolution\\noperation on point cloud remains indefinite due to its irregular data\\nstructure. In this paper, we present Cubic Kernel Convolution (CKConv) that\\nlearns to voxelize the features of local points by exploiting both continuous\\nand discrete convolutions. Our continuous convolution uniquely employs a 3D\\ncubic form of kernel weight representation that splits a feature into voxels in\\nembedding space. By consecutively applying discrete 3D convolutions on the\\nvoxelized features in a spatial manner, preceding continuous convolution is\\nforced to learn spatial feature mapping, i.e., feature voxelization. In this\\nway, geometric information can be detailed by encoding with subdivided\\nfeatures, and our 3D convolutions on these fixed structured data do not suffer\\nfrom discretization artifacts thanks to voxelization in embedding space.\\nFurthermore, we propose a spatial attention module, Local Set Attention (LSA),\\nto provide comprehensive structure awareness within the local point set and\\nhence produce representative features. By learning feature voxelization with\\nLSA, CKConv can extract enriched features for effective point cloud analysis.\\nWe show that CKConv has great applicability to point cloud processing tasks\\nincluding object classification, object part segmentation, and scene semantic\\nsegmentation with state-of-the-art results.  \n",
      "36                                                                                                                                                                                                                                                                                                                                                                                 Graph Neural Networks (GNNs) have exploded onto the machine learning scene in\\nrecent years owing to their capability to model and learn from graph-structured\\ndata. Such an ability has strong implications in a wide variety of fields whose\\ndata is inherently relational, for which conventional neural networks do not\\nperform well. Indeed, as recent reviews can attest, research in the area of\\nGNNs has grown rapidly and has lead to the development of a variety of GNN\\nalgorithm variants as well as to the exploration of groundbreaking applications\\nin chemistry, neurology, electronics, or communication networks, among others.\\nAt the current stage of research, however, the efficient processing of GNNs is\\nstill an open challenge for several reasons. Besides of their novelty, GNNs are\\nhard to compute due to their dependence on the input graph, their combination\\nof dense and very sparse operations, or the need to scale to huge graphs in\\nsome applications. In this context, this paper aims to make two main\\ncontributions. On the one hand, a review of the field of GNNs is presented from\\nthe perspective of computing. This includes a brief tutorial on the GNN\\nfundamentals, an overview of the evolution of the field in the last decade, and\\na summary of operations carried out in the multiple phases of different GNN\\nalgorithm variants. On the other hand, an in-depth analysis of current software\\nand hardware acceleration schemes is provided, from which a hardware-software,\\ngraph-aware, and communication-centric vision for GNN accelerators is\\ndistilled.  \n",
      "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Graph neural networks (GNNs) have achieved remarkable success as a framework\\nfor deep learning on graph-structured data. However, GNNs are fundamentally\\nlimited by their tree-structured inductive bias: the WL-subtree kernel\\nformulation bounds the representational capacity of GNNs, and polynomial-time\\nGNNs are provably incapable of recognizing triangles in a graph. In this work,\\nwe propose to augment the GNN message-passing operations with information\\ndefined on ego graphs (i.e., the induced subgraph surrounding each node). We\\nterm these approaches Ego-GNNs and show that Ego-GNNs are provably more\\npowerful than standard message-passing GNNs. In particular, we show that\\nEgo-GNNs are capable of recognizing closed triangles, which is essential given\\nthe prominence of transitivity in real-world graphs. We also motivate our\\napproach from the perspective of graph signal processing as a form of multiplex\\ngraph convolution. Experimental results on node classification using synthetic\\nand real data highlight the achievable performance gains using this approach.  \n",
      "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Graph neural networks (GNNs) is widely used to learn a powerful\\nrepresentation of graph-structured data. Recent work demonstrates that\\ntransferring knowledge from self-supervised tasks to downstream tasks could\\nfurther improve graph representation. However, there is an inherent gap between\\nself-supervised tasks and downstream tasks in terms of optimization objective\\nand training data. Conventional pre-training methods may be not effective\\nenough on knowledge transfer since they do not make any adaptation for\\ndownstream tasks. To solve such problems, we propose a new transfer learning\\nparadigm on GNNs which could effectively leverage self-supervised tasks as\\nauxiliary tasks to help the target task. Our methods would adaptively select\\nand combine different auxiliary tasks with the target task in the fine-tuning\\nstage. We design an adaptive auxiliary loss weighting model to learn the\\nweights of auxiliary tasks by quantifying the consistency between auxiliary\\ntasks and the target task. In addition, we learn the weighting model through\\nmeta-learning. Our methods can be applied to various transfer learning\\napproaches, it performs well not only in multi-task learning but also in\\npre-training and fine-tuning. Comprehensive experiments on multiple downstream\\ntasks demonstrate that the proposed methods can effectively combine auxiliary\\ntasks with the target task and significantly improve the performance compared\\nto state-of-the-art methods.  \n",
      "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Graph representation learning plays a vital role in processing\\ngraph-structured data. However, prior arts on graph representation learning\\nheavily rely on labeling information. To overcome this problem, inspired by the\\nrecent success of graph contrastive learning and Siamese networks in visual\\nrepresentation learning, we propose a novel self-supervised approach in this\\npaper to learn node representations by enhancing Siamese self-distillation with\\nmulti-scale contrastive learning. Specifically, we first generate two augmented\\nviews from the input graph based on local and global perspectives. Then, we\\nemploy two objectives called cross-view and cross-network contrastiveness to\\nmaximize the agreement between node representations across different views and\\nnetworks. To demonstrate the effectiveness of our approach, we perform\\nempirical experiments on five real-world datasets. Our method not only achieves\\nnew state-of-the-art results but also surpasses some semi-supervised\\ncounterparts by large margins. Code is made available at\\nhttps://github.com/GRAND-Lab/MERIT  \n",
      "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Recent achievements in end-to-end deep learning have encouraged the\\nexploration of tasks dealing with highly structured data with unified deep\\nnetwork models. Having such models for compressing audio signals has been\\nchallenging since it requires discrete representations that are not easy to\\ntrain with end-to-end backpropagation. In this paper, we present an end-to-end\\ndeep learning approach that combines recurrent neural networks (RNNs) within\\nthe training strategy of variational autoencoders (VAEs) with a binary\\nrepresentation of the latent space. We apply a reparametrization trick for the\\nBernoulli distribution for the discrete representations, which allows smooth\\nbackpropagation. In addition, our approach allows the separation of the encoder\\nand decoder, which is necessary for compression tasks. To our best knowledge,\\nthis is the first end-to-end learning for a single audio compression model with\\nRNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.  \n",
      "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Graph convolutional networks are becoming indispensable for deep learning\\nfrom graph-structured data. Most of the existing graph convolutional networks\\nshare two big shortcomings. First, they are essentially low-pass filters, thus\\nthe potentially useful middle and high frequency band of graph signals are\\nignored. Second, the bandwidth of existing graph convolutional filters is\\nfixed. Parameters of a graph convolutional filter only transform the graph\\ninputs without changing the curvature of a graph convolutional filter function.\\nIn reality, we are uncertain about whether we should retain or cut off the\\nfrequency at a certain point unless we have expert domain knowledge. In this\\npaper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture\\nthe full spectrum of graph signals and automatically update the bandwidth of\\ngraph convolutional filters. While it is based on graph spectral theory, our\\nAutoGCN is also localized in space and has a spatial form. Experimental results\\nshow that AutoGCN achieves significant improvement over baseline methods which\\nonly work as low-pass filters.  \n",
      "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Multi-relational graph is a ubiquitous and important data structure, allowing\\nflexible representation of multiple types of interactions and relations between\\nentities. Similar to other graph-structured data, link prediction is one of the\\nmost important tasks on multi-relational graphs and is often used for knowledge\\ncompletion. When related graphs coexist, it is of great benefit to build a\\nlarger graph via integrating the smaller ones. The integration requires\\npredicting hidden relational connections between entities belonged to different\\ngraphs (inter-domain link prediction). However, this poses a real challenge to\\nexisting methods that are exclusively designed for link prediction between\\nentities of the same graph only (intra-domain link prediction). In this study,\\nwe propose a new approach to tackle the inter-domain link prediction problem by\\nsoftly aligning the entity distributions between different domains with optimal\\ntransport and maximum mean discrepancy regularizers. Experiments on real-world\\ndatasets show that optimal transport regularizer is beneficial and considerably\\nimproves the performance of baseline methods.  \n",
      "43                                                                                                                                                                                                                                                                                                                                                                                                                                                    Graph representation learning has attracted a surge of interest recently,\\nwhose target at learning discriminant embedding for each node in the graph.\\nMost of these representation methods focus on supervised learning and heavily\\ndepend on label information. However, annotating graphs are expensive to obtain\\nin the real world, especially in specialized domains (i.e. biology), as it\\nneeds the annotator to have the domain knowledge to label the graph. To\\napproach this problem, self-supervised learning provides a feasible solution\\nfor graph representation learning. In this paper, we propose a Multi-Level\\nGraph Contrastive Learning (MLGCL) framework for learning robust representation\\nof graph data by contrasting space views of graphs. Specifically, we introduce\\na novel contrastive view - topological and feature space views. The original\\ngraph is first-order approximation structure and contains uncertainty or error,\\nwhile the $k$NN graph generated by encoding features preserves high-order\\nproximity. Thus $k$NN graph generated by encoding features not only provide a\\ncomplementary view, but is more suitable to GNN encoder to extract discriminant\\nrepresentation. Furthermore, we develop a multi-level contrastive mode to\\npreserve the local similarity and semantic similarity of graph-structured data\\nsimultaneously. Extensive experiments indicate MLGCL achieves promising results\\ncompared with the existing state-of-the-art graph representation learning\\nmethods on seven datasets.  \n",
      "44                                                                                                                                                                                                                                                                                                                                                                                                                    Various non-trivial spaces are becoming popular for embedding structured data\\nsuch as graphs, texts, or images. Following spherical and hyperbolic spaces,\\nmore general product spaces have been proposed. However, searching for the best\\nconfiguration of product space is a resource-intensive procedure, which reduces\\nthe practical applicability of the idea. We generalize the concept of product\\nspace and introduce an overlapping space that does not have the configuration\\nsearch problem. The main idea is to allow subsets of coordinates to be shared\\nbetween spaces of different types (Euclidean, hyperbolic, spherical). As a\\nresult, parameter optimization automatically learns the optimal configuration.\\nAdditionally, overlapping spaces allow for more compact representations since\\ntheir geometry is more complex. Our experiments confirm that overlapping spaces\\noutperform the competitors in graph embedding tasks. Here, we consider both\\ndistortion setup, where the aim is to preserve distances, and ranking setup,\\nwhere the relative order should be preserved. The proposed method effectively\\nsolves the problem and outperforms the competitors in both settings. We also\\nperform an empirical analysis in a realistic information retrieval task, where\\nwe compare all spaces by incorporating them into DSSM. In this case, the\\nproposed overlapping space consistently achieves nearly optimal results without\\nany configuration tuning. This allows for reducing training time, which can be\\nsignificant in large-scale applications.  \n",
      "45         Relational databases are the de facto standard for storing and querying\\nstructured data, and extracting insights from structured data requires advanced\\nanalytics. Deep neural networks (DNNs) have achieved super-human prediction\\nperformance in particular data types, e.g., images. However, existing DNNs may\\nnot produce meaningful results when applied to structured data. The reason is\\nthat there are correlations and dependencies across combinations of attribute\\nvalues in a table, and these do not follow simple additive patterns that can be\\neasily mimicked by a DNN. The number of possible such cross features is\\ncombinatorial, making them computationally prohibitive to model. Furthermore,\\nthe deployment of learning models in real-world applications has also\\nhighlighted the need for interpretability, especially for high-stakes\\napplications, which remains another issue of concern to DNNs.\\n  In this paper, we present ARM-Net, an adaptive relation modeling network\\ntailored for structured data, and a lightweight framework ARMOR based on\\nARM-Net for relational data analytics. The key idea is to model feature\\ninteractions with cross features selectively and dynamically, by first\\ntransforming the input features into exponential space, and then determining\\nthe interaction order and interaction weights adaptively for each cross\\nfeature. We propose a novel sparse attention mechanism to dynamically generate\\nthe interaction weights given the input tuple, so that we can explicitly model\\ncross features of arbitrary orders with noisy features filtered selectively.\\nThen during model inference, ARM-Net can specify the cross features being used\\nfor each prediction for higher accuracy and better interpretability. Our\\nextensive experiments on real-world datasets demonstrate that ARM-Net\\nconsistently outperforms existing models and provides more interpretable\\npredictions for data-driven decision making.  \n",
      "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a model for hierarchical structured data as an extension to the\\nstochastic temporal convolutional network. The proposed model combines an\\nautoregressive model with a hierarchical variational autoencoder and\\ndownsampling to achieve superior computational complexity. We evaluate the\\nproposed model on two different types of sequential data: speech and\\nhandwritten text. The results are promising with the proposed model achieving\\nstate-of-the-art performance.  \n",
      "47                                                                                                                                                                                                                                                                             Graph neural networks have recently achieved remarkable success in\\nrepresenting graph-structured data, with rapid progress in both the node\\nembedding and graph pooling methods. Yet, they mostly focus on capturing\\ninformation from the nodes considering their connectivity, and not much work\\nhas been done in representing the edges, which are essential components of a\\ngraph. However, for tasks such as graph reconstruction and generation, as well\\nas graph classification tasks for which the edges are important for\\ndiscrimination, accurately representing edges of a given graph is crucial to\\nthe success of the graph representation learning. To this end, we propose a\\nnovel edge representation learning framework based on Dual Hypergraph\\nTransformation (DHT), which transforms the edges of a graph into the nodes of a\\nhypergraph. This dual hypergraph construction allows us to apply message\\npassing techniques for node representations to edges. After obtaining edge\\nrepresentations from the hypergraphs, we then cluster or drop edges to obtain\\nholistic graph-level edge representations. We validate our edge representation\\nlearning method with hypergraphs on diverse graph datasets for graph\\nrepresentation and generation performance, on which our method largely\\noutperforms existing graph representation learning methods. Moreover, our edge\\nrepresentation learning and pooling method also largely outperforms\\nstate-of-the-art graph pooling methods on graph classification, not only\\nbecause of its accurate edge representation learning, but also due to its\\nlossless compression of the nodes and removal of irrelevant edges for effective\\nmessage passing.  \n",
      "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         In the world where big data reigns and there is plenty of hardware prepared\\nto gather a huge amount of non structured data, data acquisition is no longer a\\nproblem. Surveillance cameras are ubiquitous and they capture huge numbers of\\npeople walking across different scenes. However, extracting value from this\\ndata is challenging, specially for tasks that involve human images, such as\\nface recognition and person re-identification. Annotation of this kind of data\\nis a challenging and expensive task. In this work we propose a domain\\nadaptation workflow to allow CNNs that were trained in one domain to be applied\\nto another domain without the need for new annotation of the target data. Our\\nmethod uses AlignedReID++ as the baseline, trained using a Triplet loss with\\nbatch hard. Domain adaptation is done by using pseudo-labels generated using an\\nunsupervised learning strategy. Our results show that domain adaptation\\ntechniques really improve the performance of the CNN when applied in the target\\ndomain.  \n",
      "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In representation learning on the graph-structured data, under heterophily\\n(or low homophily), many popular GNNs may fail to capture long-range\\ndependencies, which leads to their performance degradation. To solve the\\nabove-mentioned issue, we propose a graph convolutional networks with structure\\nlearning (GCN-SL), and furthermore, the proposed approach can be applied to\\nnode classification. The proposed GCN-SL contains two improvements:\\ncorresponding to node features and edges, respectively. In the aspect of node\\nfeatures, we propose an efficient-spectral-clustering (ESC) and an ESC with\\nanchors (ESC-ANCH) algorithms to efficiently aggregate feature representations\\nfrom all similar nodes. In the aspect of edges, we build a re-connected\\nadjacency matrix by using a special data preprocessing technique and similarity\\nlearning, and the re-connected adjacency matrix can be optimized directly along\\nwith GCN-SL parameters. Considering that the original adjacency matrix may\\nprovide misleading information for aggregation in GCN, especially the graphs\\nbeing with a low level of homophily. The proposed GCN-SL can aggregate feature\\nrepresentations from nearby nodes via re-connected adjacency matrix and is\\napplied to graphs with various levels of homophily. Experimental results on a\\nwide range of benchmark datasets illustrate that the proposed GCN-SL\\noutperforms the stateof-the-art GNN counterparts.  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show= arxiv_data.iloc[:,:50]\n",
    "print(show)\n",
    "\n",
    "len(show)\n",
    "show2=arxiv_data.head(50)\n",
    "print(show2)\n",
    "len(show2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a912841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "      <td>Deep networks and decision forests (such as ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "      <td>With the increasing popularity of Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "      <td>Machine learning solutions for pattern classif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           terms  \\\n",
       "0                      ['cs.LG']   \n",
       "1             ['cs.LG', 'cs.AI']   \n",
       "2  ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3             ['cs.LG', 'cs.CR']   \n",
       "4                      ['cs.LG']   \n",
       "\n",
       "                                              titles  \\\n",
       "0  Multi-Level Attention Pooling for Graph Neural...   \n",
       "1  Decision Forests vs. Deep Networks: Conceptual...   \n",
       "2  Power up! Robust Graph Convolutional Network v...   \n",
       "3  Releasing Graph Neural Networks with Different...   \n",
       "4  Recurrence-Aware Long-Term Cognitive Network f...   \n",
       "\n",
       "                                           abstracts  \n",
       "0  Graph neural networks (GNNs) have been widely ...  \n",
       "1  Deep networks and decision forests (such as ra...  \n",
       "2  Graph convolutional networks (GCNs) are powerf...  \n",
       "3  With the increasing popularity of Graph Neural...  \n",
       "4  Machine learning solutions for pattern classif...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e8c0b",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6ce7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd86c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "terms        0\n",
       "titles       0\n",
       "abstracts    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efcdf227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15054"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c14beaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : ['cs.LG' 'cs.AI' 'cs.CR' ... 'D.1.3; G.4; I.2.8; I.2.11; I.5.3; J.3'\n",
      " '68T07, 68T45, 68T10, 68T50, 68U35' 'I.2.0; G.3']\n",
      "lenght : 1177\n"
     ]
    }
   ],
   "source": [
    "# getting unique labels\n",
    "labels_column = arxiv_data['terms'].apply(literal_eval)\n",
    "labels = labels_column.explode().unique()\n",
    "print(\"labels :\",labels)\n",
    "print(\"lenght :\",len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d52c4165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41105 rows in the deduplicated dataset.\n",
      "2503\n",
      "3401\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate entries based on the \"titles\" (terms) column\n",
    "# This filters the DataFrame, keeping only the rows where the titles are not duplicated.\n",
    "arxiv_data = arxiv_data[~arxiv_data['titles'].duplicated()]\n",
    "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n",
    "# There are some terms with occurrence as low as 1.\n",
    "print(sum(arxiv_data['terms'].value_counts()==1))\n",
    "# how many unique terms\n",
    "print(arxiv_data['terms'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5883bdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38602, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the rare terms. (it keeps only those rows where the \"terms\" value occurs more than once in the original DataFrame.)\n",
    "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x: len(x) > 1)\n",
    "arxiv_data_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e35864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['cs.LG']), list(['cs.LG', 'cs.AI']),\n",
       "       list(['cs.LG', 'cs.CR', 'stat.ML'])], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It evaluates the given string containing a Python literal or container display (e.g., a list or dictionary) and returns the corresponding Python object.\n",
    "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
    "arxiv_data_filtered['terms'].values[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba3861",
   "metadata": {},
   "source": [
    "## train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd52b83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 34741\n",
      "Number of rows in validation set: 1930\n",
      "Number of rows in test set: 1931\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "# The stratify parameter ensures that the splitting is done in a way that preserves the same distribution of labels (terms) in both the training and test sets.\n",
    "train_df, test_df = train_test_split(arxiv_data_filtered,test_size=test_split,stratify=arxiv_data_filtered[\"terms\"].values)\n",
    "\n",
    "# Splitting the test set further into validation\n",
    "# and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee848e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\AmiMistry\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\AmiMistry\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Vocabulary:\n",
      "\n",
      "['[UNK]', 'cs.CV', 'cs.LG', 'stat.ML', 'cs.AI', 'eess.IV', 'cs.RO', 'cs.CL', 'cs.NE', 'cs.GR', 'cs.CR', 'math.OC', 'eess.SP', 'cs.SI', 'cs.MM', 'cs.SY', 'cs.IR', 'eess.SY', 'cs.MA', 'cs.HC', 'math.IT', 'cs.IT', 'cs.DC', 'stat.AP', 'cs.CY', 'stat.ME', 'stat.TH', 'math.ST', 'eess.AS', 'cs.SD', 'cs.DS', 'q-bio.QM', 'q-bio.NC', 'stat.CO', 'cs.CG', 'cs.NI', 'cs.GT', 'math.NA', 'cs.SE', 'cs.NA', 'I.2.6', 'physics.chem-ph', 'cs.DB', 'physics.comp-ph', 'cond-mat.dis-nn', 'q-bio.BM', 'math.PR', 'cs.PL', 'cs.LO', '68T45', 'cs.AR', 'physics.data-an', 'quant-ph', 'I.2.10', 'cs.CE', 'cond-mat.stat-mech', 'q-fin.ST', 'I.4.6', 'math.DS', 'cs.CC', '68T05', 'physics.soc-ph', 'physics.ao-ph', 'physics.med-ph', 'cs.PF', 'q-bio.GN', 'econ.EM', 'cs.DM', 'I.4.8', 'astro-ph.IM', 'physics.geo-ph', 'physics.flu-dyn', 'math.AT', 'hep-ex', 'I.4', '68U10', 'q-fin.TR', 'cs.FL', 'I.5.4', 'I.2', 'physics.optics', 'cond-mat.mtrl-sci', 'I.4.9', '68T10', 'I.4; I.5', '68T07', 'q-fin.CP', 'math.CO', 'math.AP', 'I.2.6; I.2.8', '65D19', 'q-bio.PE', 'physics.app-ph', 'nlin.CD', 'cs.MS', 'I.4.5', 'I.2.6; I.5.1', 'I.2.10; I.4; I.5', 'I.2.0; I.2.6', '68U01', '68T01', 'hep-ph', 'cs.SC', 'cs.ET', 'K.3.2', 'I.2; I.5', 'I.2.8', 'I.2.10; I.4.8', '68T30', '68', 'q-fin.GN', 'q-fin.EC', 'q-bio.MN', 'econ.GN', 'I.4.9; I.5.4', 'I.4.0', 'I.2; I.4; I.5', 'I.2.6; I.2.7', '68T99', '68Q32', '62H30', 'q-fin.RM', 'q-fin.PM', 'q-bio.TO', 'q-bio.OT', 'physics.plasm-ph', 'physics.class-ph', 'physics.bio-ph', 'nlin.AO', 'math.SP', 'math.MP', 'math.LO', 'math.FA', 'math-ph', 'cs.DL', 'cond-mat.soft', 'I.5.2', 'I.4.6; I.4.8', 'I.4.4', 'I.4.3', 'I.4.1', 'I.3.7', 'I.2; J.2', 'I.2; I.2.6; I.2.7', 'I.2.7', 'I.2.6; I.5.4', 'I.2.6; I.2.9', 'I.2.6; I.2.7; H.3.1; H.3.3', 'I.2.6; I.2.10', 'I.2.6, I.5.4', 'I.2.1; J.3', 'I.2.10; I.5.1; I.4.8', 'I.2.10; I.4.8; I.5.4', 'I.2.10; I.2.6', 'I.2.1', 'H.3.1; I.2.6; I.2.7', 'H.3.1; H.3.3; I.2.6; I.2.7', 'G.3', 'F.2.2; I.2.7', 'E.5; E.4; E.2; H.1.1; F.1.1; F.1.3', '68Txx', '62H99', '62H35', '60L10, 60L20', '14J60 (Primary) 14F05, 14J26 (Secondary)']\n"
     ]
    }
   ],
   "source": [
    "# creates a TensorFlow RaggedTensor (terms) from the values in the \"terms\" column of the train_df DataFrame. A RaggedTensor is a tensor with non-uniform shapes\n",
    "terms = tf.ragged.constant(train_df['terms'].values)\n",
    "# This line creates a StringLookup layer in TensorFlow. The purpose of this layer is to map strings to integer indices and vice versa. The output_mode=\"multi_hot\" indicates that the layer will output a multi-hot encoded representation of the input strings.\n",
    "lookup = tf.keras.layers.StringLookup(output_mode='multi_hot')\n",
    "# This step adapts the StringLookup layer to the unique values in the \"terms\" column, building the vocabulary.\n",
    "lookup.adapt(terms)\n",
    "# retrieve vocabulary\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd32a8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: ['cs.LG', 'stat.ML']\n",
      "Label-binarized representation: [[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sample_label = train_df[\"terms\"].iloc[0]\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "120e17de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn summary, the make_dataset function is designed to create a \\ndataset suitable for training a model. It takes a dataframe as input, \\nassumes it has \"abstracts\" and \"terms\" columns, and creates a dataset of \\nbatches where each batch consists of abstract \\nsequences and their corresponding binarized label sequences. \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# following lines::\n",
    "# which is used for automatic adjustment of resource usage by TensorFlow's data loading pipeline.\n",
    "\n",
    "#max_seqlen: Maximum sequence length. It indicates the maximum length allowed for sequences.\n",
    "max_seqlen = 150\n",
    "#batch_size: Batch size. It specifies the number of samples to use in each iteration.\n",
    "batch_size = 128\n",
    "#padding_token: A token used for padding sequences.\n",
    "padding_token = \"<pad>\"\n",
    "#auto = tf.data.AUTOTUNE: auto is assigned the value tf.data.AUTOTUNE,\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    # creating sequences of labesls\n",
    "    labels = tf.ragged.constant(dataframe[\"terms\"].values)\n",
    "    #This line uses the previously defined lookup layer to convert the ragged tensor of labels into a binarized representation. The resulting label_binarized is a NumPy array.\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    # creating sequences of text.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe[\"abstracts\"].values, label_binarized))\n",
    "    # shuffling data basis on condition\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "In summary, the make_dataset function is designed to create a \n",
    "dataset suitable for training a model. It takes a dataframe as input, \n",
    "assumes it has \"abstracts\" and \"terms\" columns, and creates a dataset of \n",
    "batches where each batch consists of abstract \n",
    "sequences and their corresponding binarized label sequences. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c36ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d409c7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: Abstract 1\n",
      "Label(s): [[b'[UNK]']]\n",
      " \n",
      "Abstract: Abstract 2\n",
      "Label(s): [[b'label1']]\n",
      " \n",
      "Abstract: Abstract 3\n",
      "Label(s): [[b'label2']]\n",
      " \n",
      "Abstract: Abstract 4\n",
      "Label(s): [[b'[UNK]']\n",
      " [b'label1']]\n",
      " \n",
      "Abstract: Abstract 5\n",
      "Label(s): [[b'label1']\n",
      " [b'label2']]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This code snippet is iterating through batches of the training dataset and printing the abstract text along with the corresponding labels.\n",
    "text_batch = [\"Abstract 1\", \"Abstract 2\", \"Abstract 3\", \"Abstract 4\", \"Abstract 5\"]\n",
    "label_batch = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [0, 1, 1]])\n",
    "\n",
    "# Create the StringLookup layer and its inverse\n",
    "label_lookup = tf.keras.layers.StringLookup(vocabulary=[\"label1\", \"label2\", \"label3\"], mask_token=None)\n",
    "inverse_label_lookup = tf.keras.layers.StringLookup(vocabulary=label_lookup.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "# Define the invert_multi_hot function\n",
    "def invert_multi_hot(label):\n",
    "    indices = tf.where(label > 0)  # Get indices of non-zero elements\n",
    "    inverted_labels = inverse_label_lookup(indices)\n",
    "    return inverted_labels\n",
    "\n",
    "# This code snippet is iterating through batches of the training dataset and printing the abstract text along with the corresponding labels.\n",
    "# Iterate through the batch and print the abstract and corresponding labels\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    print(\" \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3881b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159102\n"
     ]
    }
   ],
   "source": [
    "# This code calculates the size of the vocabulary in the \"abstracts\" column of the train_df DataFrame.\n",
    "\n",
    "# Creating vocabulary with uniques words\n",
    "vocabulary = set()\n",
    "train_df[\"abstracts\"].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c291e",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13305e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes a TextVectorization layer\n",
    "text_vectorizer = layers.TextVectorization(max_tokens=vocabulary_size,ngrams=2,output_mode=\"tf_idf\")\n",
    "# `TextVectorization` layer needs to be adapted as per the vocabulary from our\n",
    "# training set.\n",
    "text_vectorizer.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b82682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mapping Vectorization to Datasets: The code maps the text vectorization operation to \n",
    "each element of the training, validation, and test datasets. This ensures that the text\n",
    "data in each dataset is transformed into numerical vectors using the adapted TextVectorization layer.\n",
    "The num_parallel_calls parameter is used to parallelize the mapping process, and prefetch is \n",
    "applied to prefetch data batches \n",
    "for better performance.\n",
    "\"\"\"\n",
    "train_dataset = train_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "validation_dataset = validation_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "test_dataset = test_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85485df2",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1595fa1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\AmiMistry\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\AmiMistry\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "272/272 [==============================] - 216s 771ms/step - loss: 0.0514 - binary_accuracy: 0.9818 - val_loss: 0.0188 - val_binary_accuracy: 0.9945\n",
      "Epoch 2/20\n",
      "272/272 [==============================] - 233s 857ms/step - loss: 0.0176 - binary_accuracy: 0.9950 - val_loss: 0.0184 - val_binary_accuracy: 0.9945\n",
      "Epoch 3/20\n",
      "272/272 [==============================] - 211s 760ms/step - loss: 0.0137 - binary_accuracy: 0.9959 - val_loss: 0.0188 - val_binary_accuracy: 0.9945\n",
      "Epoch 4/20\n",
      "272/272 [==============================] - 231s 847ms/step - loss: 0.0114 - binary_accuracy: 0.9965 - val_loss: 0.0194 - val_binary_accuracy: 0.9944\n",
      "Epoch 5/20\n",
      "272/272 [==============================] - 192s 705ms/step - loss: 0.0097 - binary_accuracy: 0.9971 - val_loss: 0.0197 - val_binary_accuracy: 0.9945\n",
      "Epoch 6/20\n",
      "272/272 [==============================] - 193s 708ms/step - loss: 0.0088 - binary_accuracy: 0.9974 - val_loss: 0.0204 - val_binary_accuracy: 0.9946\n",
      "Epoch 7/20\n",
      "272/272 [==============================] - 214s 787ms/step - loss: 0.0079 - binary_accuracy: 0.9977 - val_loss: 0.0206 - val_binary_accuracy: 0.9944\n"
     ]
    }
   ],
   "source": [
    "# creating shallow_mlp_model  (MLP)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Creating shallow_mlp_model (MLP) with dropout layers\n",
    "model1 = keras.Sequential([\n",
    "    # First hidden layer: 512 neurons, ReLU activation function, with dropout.\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),  # Adding dropout for regularization.\n",
    "\n",
    "    # Second hidden layer: 256 neurons, ReLU activation function, with dropout.\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),  # Adding dropout for regularization.\n",
    "\n",
    "    # Output layer: The number of neurons equals the vocabulary size (output vocabulary of the StringLookup layer), with a sigmoid activation function.\n",
    "    layers.Dense(lookup.vocabulary_size(), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "# Add early stopping\n",
    "# Number of epochs with no improvement after which training will be stopped.\n",
    "# Restore weights from the epoch with the best value of the monitored quantity.\n",
    "early_stopping = EarlyStopping(patience=5,restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "# Add early stopping callback.verbose=1\n",
    "history = model1.fit(train_dataset,validation_data=validation_dataset,epochs=20,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ccc3be2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAHICAYAAABahH7UAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAACSsUlEQVR4nOzdeVhTR/s38G8SQhL2HVFQFhVxV1DE3T4V3HCprbhURXFBtC709/hAK3VprdW+Iq2tVLSgoq22tdXa2iq11YqoKGLrVje0KIIIIvsSknn/wERiAiQxmAD357q4JOdM5sy5E5LbmTlzOIwxBkIIIYQQooCr7wYQQgghhBgiSpIIIYQQQlSgJIkQQgghRAVKkgghhBBCVKAkiRBCCCFEBUqSCCGEEEJUoCSJEEIIIUQFSpIIIYQQQlSgJIkQQgghRAVKkpoRDoeDoUOH6rsZjcKQzs3V1RWurq4K23bs2AEOh4MdO3a8UD26ZkhxO378ODgcDlatWqX2c4YOHQoOh9N4jSKEGBRtPicaEyVJOsbhcDT6Ibrz5MkTmJiYwNraGhUVFfWW3bx5MzgcDhYvXvySWtc4KIkgRDcKCwvx/vvvo0+fPrCysoJQKISbmxtmzpyJCxcu6Lt5Glm1alWD3z0xMTH6bmaTYKTvBjQ3K1euVNq2evVqWFpaYunSpY167GvXrsHExKRRj2HIrKys8Nprr2HPnj344YcfMGXKlDrLxsfHAwBCQkJ0cuwJEyagX79+cHJy0kl9utLU3xO7du1CWVmZvptBmrlz585h7NixyMnJQdeuXTFjxgyYmJjg2rVr2Lt3LxITE7Fy5UqVn++GbOLEiejatavKff369XvJrWmaKEnSMVVdhKtXr4aVlVWjdx926tSpUetvCkJCQrBnzx7Ex8fXmSRdvHgRFy9ehLe3N3r06KGT41paWsLS0lIndelSU39PtG3bVt9NIM3cvXv3MGLECDx58gSxsbEIDQ1V2H/9+nWMHj0aq1atgr29PcLCwvTUUs29/vrrmDx5sr6b0aTRcJue3L17FxwOB8HBwfjnn3/w2muvwc7ODhwOB3fv3gUAeW9I+/btYWJiAktLSwwaNAj79+9XWaeq+SfBwcHyOrds2QIvLy8IhUK0a9cOq1evhlQqVau9VVVV2Lx5MwICAuDi4gKBQAAHBwe89tprSE9PVypfe47OsWPHMHDgQJiamsLW1hYzZ85Efn6+yuNs374dXbt2hVAohIuLC5YvX97g0FltQ4cOhYeHB37//XdkZmaqLPPll18CeNaL9Mcff2D27Nnw9PSEmZkZzMzM4OPjg7i4OLWPW9+cpIMHD6JPnz4QiURwdHTE3LlzUVBQoLKeGzduYPny5ejduzdsbW0hFArRsWNHREREoKSkRKEsh8PBiRMn5L/LfoKDgxXKqJqTlJ+fj2XLlsHNzU3+WgYFBeHq1atKZXX1Hqrtzz//xJAhQ2BmZgYbGxtMnToV9+/fVyqnajhRm/dWfHw8xo0bB1dXVwiFQtjY2CAgIAB//PGHUtnacyJOnz6NgIAAWFlZgcPhICMjA1wuF6NHj1Z5XgUFBRAKhVol32lpaVi0aBG6du0KS0tLiEQidOvWDR999BHEYrHK5+Tm5uL//u//4OnpKT+vfv36YePGjUpl//77b7z55ptwdnaGQCCAk5MTRowYgUOHDsnLyIZpjh8/rvR8Ve/xxvgcU6etCQkJ4HA4+Pjjj1U+//Dhw+BwOFiyZEmdx5B555138PjxY0RGRiolSADg6emJgwcPgs/nIzIyEoWFhQBqejk5HA7ef/99lfWeOnUKHA5Hqbc6NzcXy5YtQ/v27SEQCGBnZ4eJEyfi8uXLSnXI5i0+efIEixcvhouLC4yMjDSa+6iO2q/tDz/8gD59+sDExAStWrXCggUL6vy8SklJwejRo2FjYwOhUIhOnTph1apVdfb+3rlzB6GhoQqfO0OHDq3zfC5cuICAgACYm5vD0tISEyZMkL+vni/3+uuvo23bthAIBHB0dISfnx8++ugjbUPyDCONDgBr166dwrY7d+4wAGzAgAHM0tKS9e/fn4WHh7Pg4GCWlZXFGGPM09OTdevWjc2cOZNFRESwkJAQZm9vzwCwTz/9VOVxhgwZorBt5syZDAB7/fXXmZ2dHQsODmaLFy9mbdu2ZQDYO++8o9Y5ZGdnMy6Xy4YMGcLmzZvH/ve//7E33niDCQQCJhQKWWpqqkL5hIQEBoC99tprzNjYmE2cOJG9/fbbrE+fPvLzft6aNWsYAObo6MgWLVrEli1bxtq2bcvGjBmj8tzq8sEHHzAAbPXq1Ur7KisrmY2NDROJRKygoIAxxlhAQADz8PBg06ZNY//73//Y/PnzWbt27RgAFh4erlRHu3btlF5P2fkmJCQobN+5cycDwCwsLNjcuXPZf//7X+bl5cV69+7NnJyclOpZt24ds7GxYRMnTmTLli1jS5YsYb6+vgwA69evH6uqqpKXXblypbydK1eulP/88MMP8jKq4paXl8fat2/PALChQ4eyiIgINnnyZGZkZMRMTU1ZSkqKQnldvYf++OMPBoAFBAQwY2NjNmHCBBYZGckCAgIYAObi4sJycnIUnjNkyBD2/MeUNu8toVDIfH19WUhICIuIiGDTp09n5ubmjMvlsgMHDqhs5/Dhwxmfz2f+/v7sv//9LwsKCmKMMTZ8+HDG5XLZvXv3lI7zySefMABs8+bNasWktvnz57PWrVuzyZMns//+979s4cKFrEuXLvJzfd6NGzdYmzZtGAA2cOBAtnz5crZw4UI2ZMgQZm1trVD2+++/ZwKBgPH5fPbaa6+xyMhIFhISwrp27crGjRsnL7dy5UoGgP3xxx9Kx1P1Hm+MzzF12lpaWsosLS1Zx44dVcZywoQJDAD7+++/6415SUkJ4/P5TCgUyj8P6hIUFMQAsG3btjHGGCsuLmYmJibM09NTZfnQ0FClWN66dYs5OzszDofDAgIC2Ntvv82mT5/OTExMmKmpKTtz5oxCHe3atWOtWrVivXr1Yu3bt2cLFixgS5YsYYcPH663rbLX8euvv663nIzstR09ejQzNjZm06ZNYxEREczPz48BYD169GBlZWUKz/nuu++YkZERMzExYbNmzWL/+9//mLe3NwPA/Pz8WEVFhUL5lJQUZmlpyTgcDhsxYgSLiIhg8+fPZ3379mU9e/aUl5P9/Y0ePZqZmJiwUaNGsbfffpu98sorDADz8PBg5eXl8vLp6elMIBAwExMTNmXKFBYREcFCQ0PZoEGDmLu7u1rnXx9Kkl6C+pIkACwqKkrl827fvq20rbi4mHXr1o1ZWlqy0tJSpePUlSS5ubmxBw8eyLc/evSIWVlZMXNzc1ZZWdngOVRUVLD79+8rbb98+TIzMzNjr776qsJ22R+dkZERS05Olm+vrq5mQ4cOZQDY6dOn5dtv3rzJjIyMWJs2bdjDhw/l2wsLC5mnp6dGSdL9+/cZj8djbm5uTCqVKuzbt28fA8DefPNN+baMjAylOsRiMRs+fDjj8Xjs33//VdinbpJUWFjILCwsmKmpKbt+/bp8e1VVFRs8eLDK98X9+/dVvh6rV69mANju3bsVtqtKImpTFbfZs2czACwyMlJh+6+//soAsA4dOjCJRCLfrqv3kOzDDwDbvn27yvObPXt2g+en6XuLMdWv8YMHD1jr1q1Zhw4d6mznl19+qfS8b7/9ts4kvHv37mp94apy9+5dVl1drbBNKpXKX6/a58oYY3379mUAWFxcnFJdtRO4hw8fMjMzM2ZqasouXLhQb1ltkyRdfY5p0taFCxcyAOzEiRMKZR4+fMj4fD7z9fVV2Z7ajh8/Xmdi/by4uDil9+i0adMYAKX/JFZVVTFbW1vm4uKi8BnUv39/ZmRkxI4ePapQ/vr168zc3Jx169ZNYbvsP0H+/v5KSUp9ZK/jxIkTFf4DVfsnOztbXl722gJgv/32m0Jds2bNYgDYmjVr5NuKioqYlZUVEwgE7K+//pJvl0qlbOrUqQwAe//99+XbKyoqmIuLC+NyueyXX35Ram/t17X239/evXsVyk2fPl0p+QsPD2cA2MGDB5XqzcvLUydc9aIk6SWoL0lq1aqVWl8wtW3cuJEBYMePH1c6Tl1JUnx8vFI9sn0N/W+rIYGBgczY2Fihl0P2Rzdjxgyl8rJ9tf8XKfuS3Lhxo1L5xMREjZIkxhgbOXIkA8B+//13he2yXgtVXwLP279/PwPAduzYobBd3SRJ1ov01ltvKdV98uRJle+LuuTn5zMALDg4WGG7pklSZWUlE4lEzNbWVinJZuxZfE6ePCnfpqv3kOzDz9PTUyl5LSsrY/b29kwkEin8PdSXJKn73qrPW2+9xQCwu3fvKrWzV69eKp9TVVXFHB0dmaurq8J5pKamKiXgupCWlsYAsFWrVikda/DgwQ0+f8OGDQwAe++99xosq22SpKvPMU3a+vfffzMAbPr06QrbZXU8n4irsnfvXgaATZ48ucGyv/zyCwPARo4cqbRt8eLFCmUPHDjAALCIiAj5tgsXLjAALCQkRGX9si/7S5cuybfJkqTaiYg6ZK9jfT/p6eny8rLXdvjw4Up1ZWVlMT6fzzw8POTbdu3axQCwBQsWKJXPzMxkRkZGCuW/+eabOv9mnyf7+1P13pbtq93DL4vb84mnrtCcJD3r0aMHjI2NVe7Lzc1FeHg4vLy8YGJiIp9z8vbbbwMAHjx4oPZxevfurbTN2dkZQM2l8+q4ePEipk6dirZt28LY2FjenkOHDqGqqgp5eXlaH/evv/4CAAwaNEipvKptDZHNA0hISJBvu3//PpKSkuDu7o4hQ4bItxcXF2PlypXo0aMHzMzM5Oc1ceJEAJrFubb6zsnPzw9GRsrXTTDGEB8fj8GDB8PGxgY8Hg8cDge2trYv1BaZf/75B+Xl5ejbt6/Kq95k85cuXryotE8X7yEAGDBggNI8I5FIBG9vb5SXl+PGjRtq1aNJezIyMjB37lx4eHhAKBTKX+PNmzcDUB3Xvn37qjwun8/H7NmzcffuXSQlJcm3y+a6zZkzR632P6+qqgrR0dHo27cvLCwswOVyweFw4O3trdTG1NRUAIC/v3+D9WpSVlu6+hzTpK3dunWDn58fvvvuO/k8IaBm/pmZmRmCgoJe5JSUMMYAQOG9O3z4cLRq1Qp79+6FRCKRb09MTAQATJ8+Xb7tzJkzAICcnBysWrVK6eeff/4BAPm/MkKhEN26ddOqzV9//TVYTWeI0k/Pnj2Vyqv6rGrdujU8PDxw+/ZtFBcXA4B8Hqqq+Y4uLi5K5bV5D6r79/3666+Dy+Vi/PjxmDVrFr766qs656Nqg65u0zNHR0eV2x8/fow+ffogMzMTAwYMwKuvvgorKyvweDxcvHgRBw8eRGVlpdrHUXXllexLuvYfd11SUlLwyiuvAKh5o3fo0EGeUBw4cAB//fWXyvaoe1zZh5yDg4NS+bpiVJ+xY8fC3t4e+/fvx2effQYLCwvs2LEDUqkUs2fPln/QVVVVYejQobhw4QJ69eqF6dOnw9bWFkZGRrh79y527typUZxrq++ceDyePPGpbfHixfjss8/g4uKCsWPHwsnJCQKBAEDNVZLatkWmqKgIQN0xbdWqlULba3vR95CMqnjUbpOqY6uibntu3bqFvn37oqioCMOGDUNgYKA8CTl+/DhOnDihMq71ve/mzp2Ljz76CNu3b4e/vz/Kysrw9ddfo2PHjgoJuCZef/11HDp0CB07dkRQUBAcHBzA5/Px5MkTfPLJJwptlH1JtGnTpsF6NSmrLV19jmna1nnz5mHWrFnYs2cPwsLCkJycjH/++Qdz586FmZlZg8+Xvd/v3bvXYFnZhQWy5wA1f8dTpkzBpk2bkJSUhBEjRqCwsBA///wzevfujc6dOyvEAgB+/vln/Pzzz3Uep7S0VOGxg4PDS1sLrb6/zX/++QdFRUUwNzdX63Pk+vXr8vLavAfV/fv28/PD77//jnXr1uHrr7+WTwL39vbGxx9/jGHDhql9TFUoSdKzut78X375JTIzM/HBBx/g3XffVdj30Ucf4eDBgy+jeXJr165FZWUlkpOTMWDAAIV9Z86ckfeaaEv2B5Gbm4t27dop7Hv48KHG9fH5fEyfPh3R0dHYt28f5s6dix07doDH4ylc/XXw4EFcuHABc+bMwbZt2xTq2Lt3L3bu3Kn5yTxV+5yeJ5FIkJ+fr/ChkZubi88//xzdu3fH6dOnFXp6cnJysHr1aq3bImNhYQGg7pjKtsvKNQZV8ah9bF0vpbBp0yYUFBRg9+7dmDZtmsK+0NBQ+RWCz6vvi8nNzQ3Dhw/HwYMHkZeXh59++glFRUVYsWKFVm08d+4cDh06hICAAPz888/g8XjyfWfOnMEnn3yiUN7KygoAkJWV1WDdtcs2tMI7l1szuFBdXa20r77kVVefY5q0FQCCgoKwbNkybN++HWFhYdi+fTuAmiRWHT4+PuDz+UhLS0NhYWG9771jx44BqPlSrm369OnYtGkTdu/ejREjRuDbb79FRUWFQi8S8OxvavPmzVi0aJFa7QPqfx/qWkN/m7Jz0PRzRJP3qzaGDBmCIUOGoLy8HGfPnsWhQ4ewZcsWjB49GpcuXYKHh4fWddNwm4G6ffs2gJoekeedPHnyZTcHt2/fho2NjVKCVFZWppPVaGWXTKs6N23Pt/aQ24kTJ3D79m0EBAQoJCaNGef6zun06dNKX0QZGRlgjOHVV19VGgqrqy2yL1N1e3I6deoEoVCIc+fOqbxMV5YwqOqK15VTp07Jhy5kysvLkZaWBpFIhI4dO+r0eHW9xlKpFKdOndK63nnz5qGqqgq7du3Cl19+CT6fj5kzZ75QG0ePHq2QIAGqX3vZUODRo0cbrFuTstbW1gBUf5mpWuqjIZr+fWnSVqBmmPbNN99Eeno6Tpw4gW+//Rbdu3dHnz591Hq+qakp3njjDVRUVKhcNkHm2rVr+OGHH2Bubo7XX39dYV+vXr3QuXNnHDhwAKWlpdi9e7e8h6k2X19fADV/+4ZK1Wvy4MED3L59Gx4eHjA3NwdQc84AVC4VkZWVhdu3b8Pd3V1eXtPXVVsikQhDhw7Fxo0b8c4776C8vBy//fbbC9VJSZKBkvWmJCcnK2z/6quvcPjwYb20p6CgAFeuXJFvk0gk+L//+z88evToheufOnUqeDweoqOjFf43U1RUhA8++ECrOjt37ox+/frh9OnT8v/FPr9mSV1xPnHihFLPkqbGjRsHCwsLxMfHK8yzEYvFKnscZG1JSUlRWHvo/v37iIiIUHkMGxsbeRl1GBsbY8qUKcjLy8O6desU9v3222/45Zdf0L59e6VkWJeuX78uX/Fc5uOPP8ajR48wZcqUOue2aKuu13j9+vUq16ZR17hx49CqVSts3LgRycnJGDt2bJ3DFdq28cqVK0qvEwD06dMHffv2xZ9//qnyfVo7yZk5cybMzMywceNGlXPNapf18fEBULMGUO334OnTp7Fnzx7NTgqaf45p0laZ+fPnA6j5DCkrK1O7F0nmww8/hLW1NT788EN5T1RtN2/exLhx41BVVYWPPvpI3itS2/Tp01FaWopPPvkEf/75J4YPH640FNW3b1/4+vri66+/xr59+5TqkEqldfZqvixJSUnyHjOZFStWQCwWK/wHYNy4cbC0tERCQoLCdwJjDJGRkRCLxQo99mPHjoWzszN2796NI0eOKB33RXqYTp48KR/+q03WmyUSibSuG6DhNoM1ffp0rF+/Hm+99Rb++OMPtGvXDn///Td+++03vPbaa/j+++9fanveeustHD16FAMHDsSkSZMgFApx/PhxZGVlYejQoSr/R6GJ9u3b47333sPKlSvRvXt3TJo0CUZGRti/fz+6deuG69eva1VvSEgIzpw5g1OnTsHe3h6BgYEK+wMDA+Hq6ooNGzbg8uXL6Nq1K65fv46ffvoJ48ePr3fBu4ZYWlri008/RXBwMPr06YPJkyfD0tISP/30E0QikdItTJycnDBx4kTs378fPj4++M9//oOHDx/ip59+wiuvvIKMjAylY7zyyiv47rvv8MYbb2DUqFHySZ51LXYI1CQHJ06cwAcffICUlBT4+vri7t27+O6772BiYoKEhAT5sEtj8Pf3R1hYGH7++Wd06tQJFy5cwJEjR+Di4oIPP/xQ58cLDQ1FQkICXnvtNQQFBcHW1hZnzpzBhQsXMHr06Hrnh9THyMgIs2fPlrdZ2wnbQM0XaN++ffHNN98gOzsb/fr1Q2ZmJn788UeMHj0a3333ndJzdu/ejaFDh2LevHlITEyEn58fKioqcOXKFaSnp8sX1XRwcMCuXbswefJk9O3bF2PHjoWnpyfy8vJw9uxZuLq64sCBAwBqblUhm+Ph5+eHwYMH499//8WPP/6IwMBA/PDDDxqdl6afY5q0VaZr167o378/UlJSIBQK8eabb2rUxnbt2uHw4cMYN24c5s6di82bN2Po0KHy25L88ssvEIvFWLVqVZ2rbU+bNg3vvPMOVq1aBcaY0lCbzNdff41hw4Zh8uTJiImJgbe3N4RCITIzM3H69Gk8evRIo8VzG/Ldd98pTQSX6dmzJ8aPH6+wbfTo0Rg1ahTeeOMNuLi44MSJEzh9+jR69OiB//u//5OXs7CwwLZt2zBlyhT4+voiKCgI9vb2OHbsGM6fP4++ffviv//9r7y8QCDAN998gxEjRmDkyJEYMWIEevTogaKiIly8eBFlZWVa9VQCwMaNG5GUlIRhw4bB3d0dQqEQFy5cwLFjx9C+fXtMmDBBq3rlGuWaOaIA9SwBMHPmzDqfd/HiRebv78+sra2Zubk5GzJkCPvtt9/qXLgQ9SwBcOfOHaX667vcV5XvvvuO9e7dm5mYmDA7Ozs2adIkdvv2bZXHqKuNjD27jHPlypVK+7Zt28Y6d+7MjI2NmbOzM/u///s/VlZWpvESADJFRUXM1NRU6bLR2jIyMtjEiROZvb09MzExYX369GF79+6ts52aLCbJGGM//PAD8/b2ZgKBgDk4OLA5c+awx48fq6ynuLiYvf3228zV1ZUJBALWoUMH9v7777OqqiqVMRCLxWz58uWsbdu2zMjISOk9VVfcHj16xBYvXszatWvH+Hw+s7OzY6+//rrC5ccyunoP1Y7niRMn2KBBg5iJiQmzsrJikydPZpmZmUrPqW8JAE3eW3/88QcbMGAAMzc3Z1ZWVmzUqFEsLS1NZfvre38+7/r16wwAa9u2rcLaUtrIzc1ls2fPZq1bt2ZCoZB169aNff755ywjI6POz4qcnBy2ZMkS5u7uzoyNjZmNjQ3z9fVl0dHRSmXT09PZpEmTmKOjI+Pz+czJyYmNHDmS/fTTTwrlHj16xKZPny5fdLVfv37syJEj9S4BoMvPMU3aKrN169YXXn7h8ePHbNWqVax3797MwsKCGRsbs7Zt27IZM2aw8+fPN/j8YcOGMQDMzMxM5fIatY+zYsUK1rVrVyYSiZiZmRnr0KEDmzp1Kvv+++8Vyqr6jFCHOksA1H7Nar8W33//PfP29mZCoZA5ODiw+fPns/z8fJXH+fPPP9nIkSOZlZUVMzY2Zh07dmRRUVGspKREZflbt26xkJAQ5uzszPh8PnNwcGBDhw5lu3btkpep7+9P1fvt119/ZTNmzGCenp7M3NycmZmZsc6dO7MVK1boZJ0kDmPPTQ4ghBCilm+++QZBQUFYvXo13nvvPX03p8UKCwtDbGwsTpw4gcGDB+u7OU3Ojh07MGvWLCQkJCgMkxGak0QIIVphjCE6OhpGRkZKc93Iy/Po0SPs2rULXl5elCARnaM5SYQQooFLly7hp59+QkpKCs6ePYvQ0NBGXYOIqPbzzz/jwoUL+O6771BaWoqVK1fqu0mkGaIkiRBCNJCWloZ33nkHVlZWmDFjRp13oj9+/LhaFzSomkBLGvbtt99i586daN26NT788EOdr7BNCADQnCRCCGkEq1atUmsB0JkzZ8pXCSaEGBZKkgghhBBCVKCJ24QQQgghKtCcJC1JpVI8ePAA5ubmL/XeOoQQQgjRHmMMxcXFaN26dYML51KSpKUHDx7AxcVF380ghBBCiBbu3bsHZ2fnestQkqQl2Y377t27p/M7povFYhw9ehT+/v7g8/k6rbu5oVipj2KlPoqV+ihWmqF4qa+xYlVUVAQXFxf593h9KEnSkmyIzcLColGSJBMTE1hYWNAfUQMoVuqjWKmPYqU+ipVmKF7qa+xYqTNVhiZuE0IIIYSoQEkSIYQQQogKlCQRQgghhKhgEEnSli1b4ObmBqFQCG9vb5w8ebLe8p9//jm8vLwgEong6emJXbt2KewXi8VYs2YNPDw8IBQK0aNHD/z6668KZaqrq7FixQq4ublBJBLB3d0da9asgVQq1fn5EUIIIaTp0fvE7X379mHp0qXYsmULBgwYgK1bt2LkyJG4evUq2rZtq1Q+NjYWkZGR2LZtG/r06YPU1FTMnTsX1tbWCAwMBACsWLECu3fvxrZt29CpUyccOXIEEyZMQEpKCnr16gUAWL9+Pb744gvs3LkTXbp0wfnz5zFr1ixYWlpiyZIlLzUGhBBCCDE8eu9Jio6ORkhICObMmQMvLy/ExMTAxcUFsbGxKssnJiZi/vz5CAoKgru7OyZPnoyQkBCsX79eocw777yDUaNGwd3dHQsWLEBAQAA2btwoL3P69GmMGzcOo0ePhqurK15//XX4+/vj/PnzjX7OhBBCCDF8eu1JqqqqQlpaGiIiIhS2+/v7IyUlReVzKisrIRQKFbaJRCKkpqZCLBaDz+fXWSY5OVn+eODAgfjiiy9w48YNdOzYEX/99ReSk5MRExNT53ErKyvlj4uKigDUDO2JxWK1z1kdsvp0XW9zRLFSH8VKfRQr9VGsNEPxUl9jxUqT+vSaJOXl5UEikcDR0VFhu6OjI3JyclQ+JyAgANu3b8f48ePRu3dvpKWlIT4+HmKxGHl5eXByckJAQACio6MxePBgeHh44NixYzh48CAkEom8nv/9738oLCxEp06dwOPxIJFIsHbtWkyZMkXlcdetW6fyjt5Hjx6FiYnJC0ShbklJSY1Sb3NEsVIfxUp9FCv1Uaw0Q/FSn65jVVZWpnZZvc9JApQXdGKM1bnIU1RUFHJyctCvXz8wxuDo6Ijg4GBs2LABPB4PAPDJJ59g7ty56NSpEzgcDjw8PDBr1iwkJCTI69m3bx92796Nr776Cl26dMHFixexdOlStG7dGjNnzlQ6bmRkJMLDw+WPZSt2+vv7N8pikklJSRg+fDgtNtYAipX6KFbqo1ipj2KlGYqX+horVrKRIHXoNUmys7MDj8dT6jXKzc1V6l2SEYlEiI+Px9atW/Hw4UM4OTkhLi4O5ubmsLOzAwDY29vjwIEDqKioQH5+Plq3bo2IiAi4ubnJ6/nvf/+LiIgITJ48GQDQrVs3/Pvvv1i3bp3KJEkgEEAgECht5/P5jfZGb8y6mxuKlfooVuqjWKmPYqUZipf6dB0rTerS68RtY2NjeHt7K3WlJSUloX///vU+l8/nw9nZGTweD3v37sWYMWOU7uYrFArRpk0bVFdXY//+/Rg3bpx8X1lZmVJ5Ho9HSwAQQgghBIABDLeFh4dj+vTp8PHxgZ+fH+Li4pCZmYnQ0FAANcNcWVlZ8rWQbty4gdTUVPj6+qKgoADR0dG4fPkydu7cKa/z7NmzyMrKQs+ePZGVlYVVq1ZBKpVi+fLl8jKBgYFYu3Yt2rZtiy5duiA9PR3R0dGYPXv2yw0AIYQQQgyS3pOkoKAg5OfnY82aNcjOzkbXrl1x+PBhtGvXDgCQnZ2NzMxMeXmJRIKNGzfi+vXr4PP5GDZsGFJSUuDq6iovU1FRgRUrViAjIwNmZmYYNWoUEhMTYWVlJS+zefNmREVFISwsDLm5uWjdujXmz5+P995772WdOiGEEEJUqBBL8KioAvkV+m2H3pMkAAgLC0NYWJjKfTt27FB47OXlhfT09HrrGzJkCK5evVpvGXNzc8TExNR5yT8hhBBCXlyFWIInZWI8Lq3Ck7IqPC6rQkFpFQqebisoq3q679njsqqaq9E9zHmYrse2G0SSRAghhBDD9yIJj6Z4XA6kYDo+A81QkkQIIYS0QHUlPI9LxSgo033CY23Ch7WJMaxNjWFtwoeNqXHN46fbbEz5sDIxhs3Tx0Iuw6+//qLjs9YMJUmEEEJIE1c74ZElOI2b8NQkOtamsqSmJgGyMTWuSXSePpYlQBZCozrXP6yLIaxKTkkSIYQQYkD0kfA834uj0NPzXAKkTcLTVFGSRAghhDSSqmopnpRVIb+0JtF5VFSOkzkc3P7jNooqJHpNeKxNjWEuaDkJjzYoSSKEEELUIJUyFFdU43FZFR6XVtb07JQ+TYCeJjuyn4KyKjwuqUJxZbWKmnjAndv1HsuIy1E5bFX7cU3PDp8SnkZESRIhhJAWqUIsUUxqZL/XSnzyS2T7aoa6JFLNr7aqPWnZyoSPqqJ8dHJvCztzgULCI+v1oYTHcFCSRAghpMmTSBkKy8XyHh5Vyc/j5xIfbYe1zARG8qTGxoQPG1MBbEyf/WttYgxbs2fJj4WQDy63JuERi8U4fPgwRo3qTPduawIoSSKEEGJQGGMoq5I8S2pKZROXVSc+j0ur8KRcDKbFkjp8HkeezMgSH1tTY4Vtsvk8tmY1PUECI57uT5oYJEqSCCGENCqxRPr0Cq1nPTzP1uRR7uF5XFqFymrtbjZuITSCrZng6WTlpz07dSU+NKxFGkBJEiGEEK0Ulovxz4MnSM3lICv5DgorJAqJj+ynqELV5OWGGRtxYftcb059PT5WJnzweVwdnyVpyShJIoQQUq+iCjFuPizBzYfFuPGwBDdzi3HzYQlyimR3H+UBt2/WWweHA/kChM8PY9WV+JgY86iXh+gVJUmEEEIAKCZDN3NLcOPh88mQMkcLASw55fBybQM7c6FS4iOb0Gwp4oPHpYSHNC2UJBFCSAtTVCHGrdxnPUM3HhbjVm4JsgvrToZaWQjRwdEMHR3N0cHBDB0czdHB0QwiHp5erdWNrtYizQ4lSYQQ0kwVV4hx82kydPNhCW48/V2dZKiDgzk6OtYkQ+0dzGApUp0AGcL9tQhpLJQkEUJIE1cs7xmq6RW6kVuCWw+L8aCeZMjRQvC0V8j8aQ+RGdo7mNeZDBHSElGSRAghTURJZbW8V+hm7tNJ1GokQ88SoZreofb25rA0oWSIkIZQkkQIIQampLIat+QTp2uSoVu5Jch6Ul7ncxzMn/YM1R4qc6BkiJAXQUkSIYToSWlltfwqslu1riZrKBl6lghRMkRIY6IkiRBCGllp7Z4hNZMhe3OBPAGqfVWZlYnxS2w5IS0bJUmEEKIjtZMh2b831EiGOjiYyYfKKBkixHBQkkQIIRoqq5IlQ7K1hmp6iO4X1J0M2ZnV9AzVnjfUwcEM1qaUDBFiqChJIoSQOpRVVePfh6UKt+K48bBYrWRItuCirGeIkiFCmh5KkgghLZ5UynC/oBxXswtxNbsYV7Ke4OIdHpac/r3O59iZGSssuCgbMqNkiJDmg5IkQkiLUiGW4HpOMa5mF+FadhGuPijCPznFKKl8/k71NfcZkyVDHZ4mQx2f9hDZUDJESLNHSRIhpFlijOFRcSWuZhc9TYiKcS27CBmPSiBlyuWNjbjo6GiGzk4W6Ohgiid3r2L62P/A0cr05TeeEGIQKEkihDR51RIpMvJKcfXB096hp71EeSVVKsvbmhqjc2sLeDlZoLNTzb/u9qbg87gAau5HdrjgCvUWEdLCUZJECGlSCsvF+KdWInQ1uwg3HpagqlqqVJbLAdzsTNG5tSW8nMzR+WlSZG8uAIfD0UPrCSFNCSVJhBCDxBjDvcflCsnQteyiOq8sMxMYoVMrc3kPkZeTBTwdzSEy5r3klhNCmgtKkgghelchluDGw2KF4bJ/sotRrDSZukYbK9HTobJnSZGLtQm4XOodIoToDiVJhJCXKre4AteyFROiOidT87jo8HQytax3qLOTBd2njBDyUhhEkrRlyxZ8/PHHyM7ORpcuXRATE4NBgwbVWf7zzz/HZ599hrt376Jt27Z49913MWPGDPl+sViMdevWYefOncjKyoKnpyfWr1+PESNGKNSTlZWF//3vf/jll19QXl6Ojh074ssvv4S3t3ejnSshLYVsMrUsEapJioqRV1KpsryNqfHTZOhZ75CHvZl8MjUhhLxsek+S9u3bh6VLl2LLli0YMGAAtm7dipEjR+Lq1ato27atUvnY2FhERkZi27Zt6NOnD1JTUzF37lxYW1sjMDAQALBixQrs3r0b27ZtQ6dOnXDkyBFMmDABKSkp6NWrFwCgoKAAAwYMwLBhw/DLL7/AwcEBt2/fhpWV1cs8fUKahaIKMf7JLsbVB4U1vUTZRbj+sFjlZGqObDJ1rZ6hzq0t4ECTqQkhBkbvSVJ0dDRCQkIwZ84cAEBMTAyOHDmC2NhYrFu3Tql8YmIi5s+fj6CgIACAu7s7zpw5g/Xr18uTpMTERLz77rsYNWoUAGDBggU4cuQINm7ciN27dwMA1q9fDxcXFyQkJMjrdnV1bcxTJaTJY0y2MnWRwnBZXZOpTYx5T4fJzNHZqeYKM89W5jAx1vtHDyGENEivn1RVVVVIS0tDRESEwnZ/f3+kpKSofE5lZSWEQqHCNpFIhNTUVIjFYvD5/DrLJCcnyx//+OOPCAgIwBtvvIETJ06gTZs2CAsLw9y5c3V0doQ0bbLJ1NeeLsR49UERruUUobhC9WTq1pZChSvLOjtZoK0NTaYmhDRdek2S8vLyIJFI4OjoqLDd0dEROTk5Kp8TEBCA7du3Y/z48ejduzfS0tIQHx8PsViMvLw8ODk5ISAgANHR0Rg8eDA8PDxw7NgxHDx4EBKJRF5PRkYGYmNjER4ejnfeeQepqalYvHgxBAKBwvwmmcrKSlRWPptLUVRUBKBm/pNYLNZFOORk9em63uaIYqW++mKVV1KJaznFuJZdjH+e/nsnvwwSFbOp+TwO2tuboZOTObxa1fx0amUOKxWTqSWSatT6s2sy6H2lPoqVZihe6musWGlSH4cxpuKakpfjwYMHaNOmDVJSUuDn5yffvnbtWiQmJuKff/5Rek55eTkWLlyIxMREMMbg6OiIN998Exs2bMDDhw/h4OCAR48eYe7cuTh06BA4HA48PDzw6quvIiEhAWVlZQAAY2Nj+Pj4KPRYLV68GOfOncPp06eVjrtq1SqsXr1aaftXX30FExMTXYSDkEYnYcCjciCrjIOsUg6ySmt+Lxar7u0xNWJoY8rQ2gRwNmVobcLgKAKMaC41IaSJKisrw9SpU1FYWAgLC4t6y+q1J8nOzg48Hk+p1yg3N1epd0lGJBIhPj4eW7duxcOHD+Hk5IS4uDiYm5vDzs4OAGBvb48DBw6goqIC+fn5aN26NSIiIuDm5iavx8nJCZ07d1ao28vLC/v371d53MjISISHh8sfFxUVwcXFBf7+/g0GWVNisRhJSUkYPnw4+Hy61Lk+FKv6lVZW48K9Jzh3pwBn7zzGpftPIGbKCRGHA7jamKBTK3N4OZnL/3VsoZOp6X2lPoqVZihe6musWMlGgtSh1yTJ2NgY3t7eSEpKwoQJE+Tbk5KSMG7cuHqfy+fz4ezsDADYu3cvxowZAy5X8b+3QqEQbdq0gVgsxv79+zFp0iT5vgEDBuD69esK5W/cuIF27dqpPJ5AIIBAIFDZjsZ6ozdm3c0NxapGcYUY5+8W4MydfJzNeIxLWYXPDZlxYGLMe5oEWcjnEHWiydQq0ftKfRQrzVC81KfrWGlSl94/FcPDwzF9+nT4+PjAz88PcXFxyMzMRGhoKICaHpysrCzs2rULQE0ik5qaCl9fXxQUFCA6OhqXL1/Gzp075XWePXsWWVlZ6NmzJ7KysrBq1SpIpVIsX75cXmbZsmXo378/PvzwQ0yaNAmpqamIi4tDXFzcyw0AIS+gsEyM1LuPcTYjH2fvPMaVB4VKizK2sRLB190GPm2tUHL3L8ycMBwCAd24lRBCGqL3JCkoKAj5+flYs2YNsrOz0bVrVxw+fFjeo5OdnY3MzEx5eYlEgo0bN+L69evg8/kYNmwYUlJSFC7fr6iowIoVK5CRkQEzMzOMGjUKiYmJCmsg9enTBz/88AMiIyOxZs0auLm5ISYmBtOmTXtZp06Ixh6XViH1Tj7OZDzG2TuP8U9OEZ6fVdjWxgS+bjbwdbeFr5sNXGxq5syJxWIcfvgXXW1GCCFq0nuSBABhYWEICwtTuW/Hjh0Kj728vJCenl5vfUOGDMHVq1cbPO6YMWMwZswYtdtJyMv2qLgSqXce4+zT4bPrD4uVyrjbmcLX3Qa+brbwdbeBk6VIDy0lhJDmxyCSJEJIjYdFFTjzdOjsbEY+bj8qVSrTwcHsWVLkZgMHC6GKmgghhLwoSpII0aOsJ+U184kyanqL7uaXKZXp1Moc/Z4OnfV1s4GtmfIFBIQQQnSPkiRCXhLZLT1O10qKnr+dB4cDdGltIe8l6utmAysTmmRNCCH6QEkSIY2EMYa7+WXyK8/OZuTjQWGFQhkel4OurS3kk6x9XG1gKaLLggkhxBBQkkSIjjDGcPtRifzKs7MZ+cgtrlQoY8TloLuzpUJSZCagP0NCCDFE9OlMiJakUoYbucXyobPUO4+RV1KlUMaYx0VPFyv5ROve7axo0UZCCGki6NOaEDVJpAzXsovkvUTn7j5GQZnijRIFRlz0bmstT4p6tbWCkM/TU4sJIYS8CEqSCKlDtUSKq9lFCj1FRRXVCmVEfB58XK3lizd2d7aEwIiSIkIIaQ4oSSLkKbFEiktZhfKk6PzdApRUKiZFpsY8+LjayHuKujtbgs/j1lEjIYSQpoySJNJiVVZL8Pf9QvnVZ2n/FqCsSqJQxlxohL61kqIurS1gREkRIYS0CJQkkRajQixBeuYT+S0+LmQWoLJaqlDGyoT/NCmqufrMy8kCPLrXGSGEtEiUJJFmq7xKgguZBTibkY8zdx7j4r0nqHouKbI1NVa471lHB3O6ASwhhBAAlCSRZqS0shrn/y2QD5/9ff8JxBKmUMbBXCDvJernbgMPezNwOJQUEUIIUUZJEmnS/r5fiIP/cvHl1jO48qAYEqliUuRkKXyaENnC190WrrYmlBQRQghRCyVJpMkqLBdj6pfnUFnNBVAEAHC2FsmHzvq52cLFRkRJESGEEK1QkkSarNO381FZLYWlMcOKwG7o38EBbaxE+m4WIYSQZoKSJNJkJd96BADobsMwvmdr8Pl0Y1hCCCG6Qwu+kCYr+WYeAMDTkjVQkhBCCNEcJUmkSbr3uAx388vA43LQwYKSJEIIIbpHSRJpkpJv1fQi9XS2hJAGjQkhhDQCSpJIkyQbahvgYavnlhBCCGmuKEkiTY5EynDq9tMkqT0lSYQQQhoHDVSQJufKg0I8KRPDTGCEbm0skHNZ3y0ihBAVpFJAUvX0RwxIKoHKchiLi4DqCsDICKB13AwaJUmkyTn5dKitn7st+DzqDCWkRWIMkEqeJiCVT5OQKqC61u+1f6qrlLcpbH+axMh+r658bruq+mtvV1G3tFqp2XwAIwHg8iKAywcE5k9/LACBWa3HT3+MzZW3KZUxA7i8l/0KtAiUJBmagn/BO7EBPe7dA++nIwCXAzAAYDUfCqr+Berep/AvNCxbzz61yzbUzjqeW087JxaWY4yxBDY5fBh9boRXy8ph9O+qmg8cLg/g8Gr+5fIArpHiY87TbVwewOE++11ejlvrd3XLybbXKlf7+Qptqqc96pZT2R4e/Y+U6A5jNV/wCr0gtX8Xy3/nVJXBvugSODd5ACRqJBOaJDENJCuyz4qmgmsExjUCp7qi5rFUDJQ/rvl5UXzThpMpWUIlsKh7v5GQPktqoSTJ0JTlgXtxN1wBIF/PbTFQrYCa2XTlNT+mAPA4V59NMgwc7nPJlGKyaMTh4tWKqpqEkmf8tNzTHx5f9e9qP+Y9TVKfe6xVXWo8boof4lKJQnKhMgGpNzF5PkGpo4y0rjIa1qEmIwD9AeB2YwVOAzxBzXubxweMBDX/8oyVf4yMn5VTek7t7U+fr1BX7e0q6q5ze83fRbVYjMM//4RR/xkMvrQcqCwGKkuAyqKnvz/9qVKxTVVZ2WslLq35Kcl5sRhyjerpwVKRYKlMusxqns9r+ilG0z+D5saiDSRD3sH1Gzfg6ekJHo8HgPP0S6Gef4E69qnYXmdZFc9Vp746yzbUzgaeo+JY6fcL8f5P12BnJsDWGT6QSKRISUlG/36+MOJyar5kpJKaH/b0X2l1rd9l29UoV9/z9FJOueteAZPW/NTxBceBLKF8VH89TQGH92LJm8rE8Nk+LoeLLvfvgnvkT4BVA5KGkpe6Epxa+5lU31F7MbW/7Gv9zrh8FJWWw9zaDlx5klGrbJ0JRh1JjKaJh+yYTSl55nABoQXAf8ELTxir6VFTmVCpm3iVPN1XXFOntBooL6j5eVF8k+eSKfPnEioV22qX4wnBk1a+eDteACVJhsa8FaQDw3Gz6DA6DBgFHt1qQ8Hhy1dxgUkwydMZHJceYGIxCkwfgrn0A1pCrKRSNZO/6qcJ07Ny1VUVSDl1Ev19+8KIi5ovcKnkWc+DLBGTVtfx+GlZrR7L6lLnsaTW8evo0WASoFrSaGHmAWgPAI2ZT3KNavUw8KEqAXn2u5GK7bX2c1XtV6eOWr/Lev5UlalnOLdaLMbxw4cxatQocFvC36Ah4XAAvrDmx9TuxeqSSp8mUcXP/lVIqFRsq6uc5GliIy6r+Sl5qFWT+AAGidoCmPBi5/YCKEkiTYps0vbADvZ6bomecLkAuDVfYBqqSShzwNr6NZ2EkrGaZK920vR8ElXnYzUSsDoeS6qrkHHrJtw7dgKPL6wn8Wgowakj8eDyn76WhBgI7tPeLaHFi9dVXanYkyVPpmonVCq2KZQrBqsshpir35uWU5JEmoxHxZX4J6emS5gWkWwhOJxnc6teIqlYjKvlh+E6hHpzCdGYkaDmx/TFPqerqypx+ucfMUJHzdIG/VeGNBmnnt6KpEtrC9iaCfTcGkIIIY2Kw4WUa6zXJlCSRJoM+VBb+xcceyeEEELUYBBJ0pYtW+Dm5gahUAhvb2+cPHmy3vKff/45vLy8IBKJ4OnpiV27dinsF4vFWLNmDTw8PCAUCtGjRw/8+uuvdda3bt06cDgcLF26VBenQxoBYwzJt2pm0Q7sQEkSIYSQxqf3JGnfvn1YunQp3n33XaSnp2PQoEEYOXIkMjMzVZaPjY1FZGQkVq1ahStXrmD16tVYuHAhDh06JC+zYsUKbN26FZs3b8bVq1cRGhqKCRMmID09Xam+c+fOIS4uDt27d2+0cyQv7vajEjwsqoSxERd9XG303RxCCCEtgN6TpOjoaISEhGDOnDnw8vJCTEwMXFxcEBsbq7J8YmIi5s+fj6CgILi7u2Py5MkICQnB+vXrFcq88847GDVqFNzd3bFgwQIEBARg48aNCnWVlJRg2rRp2LZtG6ytrRv1PMmLkQ219XW1gZBPy+8TQghpfHq9uq2qqgppaWmIiIhQ2O7v74+UlBSVz6msrIRQKFTYJhKJkJqaCrFYDD6fX2eZ5ORkhW0LFy7E6NGj8eqrr+KDDz6ot62VlZWorHy2qFVRURGAmqE9sVj91WnVIatP1/U2ZX/eqFlR28/dWiEuFCv1UazUR7FSH8VKMxQv9TVWrDSpT69JUl5eHiQSCRwdHRW2Ozo6IidH9dLqAQEB2L59O8aPH4/evXsjLS0N8fHxEIvFyMvLg5OTEwICAhAdHY3BgwfDw8MDx44dw8GDByGRPFt8bu/evbhw4QLOnTunVlvXrVuH1atXK20/evQoTExMNDhr9SUlJTVKvU2NRAqk3Hy68njONRw+fE2pDMVKfRQr9VGs1Eex0gzFS326jlVZWZnaZQ1inSTOcyu5MsaUtslERUUhJycH/fr1A2MMjo6OCA4OxoYNG57ewgP45JNPMHfuXHTq1AkcDgceHh6YNWsWEhISAAD37t3DkiVLcPToUaUep7pERkYiPDxc/rioqAguLi7w9/eHhYUOFt+qRSwWIykpCcOHDwef1mjBubsFqDx7DjamfMyZOBxc7rP3BsVKfRQr9VGs1Eex0gzFS32NFSvZSJA69Jok2dnZgcfjKfUa5ebmKvUuyYhEIsTHx2Pr1q14+PAhnJycEBcXB3Nzc9jZ1Vz1ZG9vjwMHDqCiogL5+flo3bo1IiIi4ObmBgBIS0tDbm4uvL295fVKJBL8+eef+Oyzz1BZWSlPuGQEAgEEAuW1efh8fqO90Ruz7qbkzJ2aewgNaG8PgUD1mhkUK/VRrNRHsVIfxUozFC/16TpWmtSl14nbxsbG8Pb2VupKS0pKQv/+/et9Lp/Ph7OzM3g8Hvbu3YsxY8aA+9wy/0KhEG3atEF1dTX279+PcePGAQD+85//4NKlS7h48aL8x8fHB9OmTcPFixeVEiSiXyefLiI5iNZHIoQQ8hLpfbgtPDwc06dPh4+PD/z8/BAXF4fMzEyEhoYCqBnmysrKkq+FdOPGDaSmpsLX1xcFBQWIjo7G5cuXsXPnTnmdZ8+eRVZWFnr27ImsrCysWrUKUqkUy5cvBwCYm5uja9euCu0wNTWFra2t0naiX4XlYvx17wkAYACtj0QIIeQl0nuSFBQUhPz8fKxZswbZ2dno2rUrDh8+jHbt2gEAsrOzFdZMkkgk2LhxI65fvw4+n49hw4YhJSUFrq6u8jIVFRVYsWIFMjIyYGZmhlGjRiExMRFWVlYv+ezIizp9Ox9SBrjbmaKNlX5vdEgIIaRl0XuSBABhYWEICwtTuW/Hjh0Kj728vFQuClnbkCFDcPXqVY3acPz4cY3Kk5dDdr82WmWbEELIy6b3xSQJqU/yLbpfGyGEEP2gJIkYrPsFZbiTVwoel4N+Hrb6bg4hhJAWhpIkYrCSn96KpKeLFSyEdKksIYSQl4uSJGKwTtJQGyGEED2iJIkYJKmUIUW2PhJN2iaEEKIHlCQRg3TlQREKysQwExihh4uVvptDCCGkBaIkiRikk7ceAQD6uduAz6O3KSGEkJePvn2IQZJN2qb5SIQQQvSFkiRicMqrJDh/t+amtgM72Ou5NYQQQloqSpKIwTl39zGqJFI4WQrhYW+q7+YQQghpoShJIgan9irbHA5Hz60hhBDSUlGSRAzOyZt0vzZCCCH6R0kSMSiPiitxLbsIADCAJm0TQgjRI0qSiEFJuV3Ti9TZyQJ2ZgI9t4YQQkhLRkkSMSiyoTZaZZsQQoi+UZJEDAZj7Nn6SJQkEUII0TNKkojBuP2oBDlFFTA24qKPq42+m0MIIaSFoySJGAzZUFsfV2sI+Tw9t4YQQkhLR0kSMRin5Osj0SrbhBBC9I+SJGIQxBIpzmQ8BkCTtgkhhBgGSpKIQbh47wlKKqthY2qMzk4W+m4OIYQQQkkSMQyy+Uj9PWzB5dKtSAghhOgfJUnEICTffASAhtoIIYQYDkqSiN4VVYjx1/1CAMDADjRpmxBCiGGgJIno3enb+ZBIGdztTNHGSqTv5hBCCCEAKEkiBkC2yjbd0JYQQoghoSSJ6F3yLboVCSGEEMNDSRLRq/sFZbiTVwoelwM/D1t9N4cQQgiRoySJ6JVsle0ezpawEPL13BpCCCHkGa2SpB07dqCsrEzXbSEtkGx9JLqqjRBCiKHRKkmKjIxEq1atEBISgpSUFF23ibQQUilDyu18ALQ+EiGEEMOjVZJ0//597N69GwUFBRg2bBg6deqE9evXIycnR6tGbNmyBW5ubhAKhfD29sbJkyfrLf/555/Dy8sLIpEInp6e2LVrl8J+sViMNWvWwMPDA0KhED169MCvv/6qUGbdunXo06cPzM3N4eDggPHjx+P69etatZ9o52p2ER6XVsFMYISeLlb6bg4hhBCiQKskicfjYezYsfj+++9x7949zJs3D3v27EHbtm0xduxYHDx4EFKpVK269u3bh6VLl+Ldd99Feno6Bg0ahJEjRyIzM1Nl+djYWERGRmLVqlW4cuUKVq9ejYULF+LQoUPyMitWrMDWrVuxefNmXL16FaGhoZgwYQLS09PlZU6cOIGFCxfizJkzSEpKQnV1Nfz9/VFaWqpNSIgWZENt/dxtwOfR9DhCCCGG5YW/mRwcHDBgwAD4+fmBy+Xi0qVLCA4OhoeHB44fP97g86OjoxESEoI5c+bAy8sLMTExcHFxQWxsrMryiYmJmD9/PoKCguDu7o7JkycjJCQE69evVyjzzjvvYNSoUXB3d8eCBQsQEBCAjRs3ysv8+uuvCA4ORpcuXdCjRw8kJCQgMzMTaWlpLxoSoqbkWzW3IhlI6yMRQggxQEbaPvHhw4dITExEQkICMjIyMH78ePz000949dVXUV5ejhUrVmDmzJn4999/66yjqqoKaWlpiIiIUNju7+9f51ynyspKCIVChW0ikQipqakQi8Xg8/l1lklOTq6zLYWFNbfFsLGxqfO4lZWV8sdFRUUAaob2xGJxnfVqQ1afrus1JBViCc7dLQAA+LpaaX2uLSFWukKxUh/FSn0UK81QvNTXWLHSpD4OY4xpeoDAwEAcOXIEHTt2xJw5czBjxgyl5OLBgwdwdnaud9jtwYMHaNOmDU6dOoX+/fvLt3/44YfYuXOnyjlC77zzDhISEvDTTz+hd+/eSEtLw+jRo5Gbm4sHDx7AyckJU6dOxV9//YUDBw7Aw8MDx44dw7hx4yCRSBQSHRnGGMaNG4eCgoI650OtWrUKq1evVtr+1VdfwcTEpM5zJKr984SD2Gs8WBozrO4tAYej7xYRQghpCcrKyjB16lQUFhbCwsKi3rJa9SQ5ODjgxIkT8PPzq7OMk5MT7ty5o1Z9nOe+IRljSttkoqKikJOTg379+oExBkdHRwQHB2PDhg3g8XgAgE8++QRz585Fp06dwOFw4OHhgVmzZiEhIUFlnYsWLcLff/9db09TZGQkwsPD5Y+Liorg4uICf3//BoOsKbFYjKSkJAwfPhx8fvNcO+jSkRsA7uI/Xdpg9OiuWtfTEmKlKxQr9VGs1Eex0gzFS32NFSvZSJA6tEqSvvzyywbLcDgctGvXrt4ydnZ24PF4SlfF5ebmwtHRUeVzRCIR4uPjsXXrVjx8+BBOTk6Ii4uDubk57Oxq5rbY29vjwIEDqKioQH5+Plq3bo2IiAi4ubkp1ffWW2/hxx9/xJ9//glnZ+c62yoQCCAQCJS28/n8RnujN2bd+pZy+zEAYIing07OsTnHStcoVuqjWKmPYqUZipf6dB0rTerSauL24sWL8emnnypt/+yzz7B06VK16zE2Noa3tzeSkpIUticlJSkMv6nC5/Ph7OwMHo+HvXv3YsyYMeByFU9HKBSiTZs2qK6uxv79+zFu3Dj5PsYYFi1ahO+//x6///67ygSKNI68kkpcza7J5OmmtoQQQgyVVknS/v37MWDAAKXt/fv3x3fffadRXeHh4di+fTvi4+Nx7do1LFu2DJmZmQgNDQVQM8w1Y8YMefkbN25g9+7duHnzJlJTUzF58mRcvnwZH374obzM2bNn8f333yMjIwMnT57EiBEjIJVKsXz5cnmZhQsXYvfu3fjqq69gbm6OnJwc5OTkoLy8XNNwEA3JbkXi5WQBOzPl3jlCCCHEEGg13Jafnw9LS0ul7RYWFsjLy9OorqCgIOTn52PNmjXIzs5G165dcfjwYflQXXZ2tsKaSRKJBBs3bsT169fB5/MxbNgwpKSkwNXVVV6moqICK1asQEZGBszMzDBq1CgkJibCyspKXka2xMDQoUMV2pOQkIDg4GCNzoFoJvnp+ki0yjYhhBBDplWS1L59e/z6669YtGiRwvZffvkF7u7uGtcXFhaGsLAwlft27Nih8NjLy0thUUhVhgwZgqtXr9ZbRouL+ogOMMaQ/LQnidZHIoQQYsi0SpLCw8OxaNEiPHr0CK+88goA4NixY9i4cSNiYmJ02T7SzNx+VIrswgoYG3HR1031mlSEEEKIIdAqSZo9ezYqKyuxdu1avP/++wAAV1dXxMbGKswfIuR5yTdrVtn2aWcNIZ+n59YQQgghddN6xe0FCxZgwYIFePToEUQiEczMzHTZLtJMyYfaaD4SIYQQA6d1kiRjb2+vi3aQFkAskeJMRs36SIPa0/uGEEKIYdM6Sfruu+/wzTffIDMzE1VVVQr7Lly48MINI83PX/eeoKSyGtYmfHRprdtVygkhhBBd02qdpE8//RSzZs2Cg4MD0tPT0bdvX9ja2iIjIwMjR47UdRtJM3Hy6aX//dvbgculm7URQggxbFolSVu2bEFcXBw+++wzGBsbY/ny5UhKSsLixYtRWFio6zaSZkI2H2kQXfpPCCGkCdAqScrMzJTfNkQkEqG4uBgAMH36dHz99de6ax1pNooqxLh47wkAmrRNCCGkadAqSWrVqhXy8/MBAO3atcOZM2cAAHfu3KFFGolKZ27nQyJlcLMzhbO1ib6bQwghhDRIqyTplVdewaFDhwAAISEhWLZsGYYPH46goCBMmDBBpw0kzQOtsk0IIaSp0erqtri4OEilUgBAaGgobGxskJycjMDAQPmNaQmpTXa/tgGUJBFCCGkiNE6SqqursXbtWsyePRsuLi4AgEmTJmHSpEk6bxxpHrKelCMjrxRcDuDnYavv5hBCCCFq0Xi4zcjICB9//DEkEkljtIc0Q6ee9iL1cLGCpYiv59YQQggh6tFqTtKrr76K48eP67gppLk6SZf+E0IIaYK0mpM0cuRIREZG4vLly/D29oapqanC/rFjx+qkcaTpk0oZTsnv10a3IiGEENJ0aJUkLViwAAAQHR2ttI/D4dBQHJG7ml2Ex6VVMDXmoVdbK303hxBCCFGbVkmS7Mo2Qhoiu/S/n7st+DytRncJIYQQvaBvLdKoZJf+0yrbhBBCmhqtepLWrFlT7/733ntPq8aQ5qVCLEHq3ccAgEGUJBFCCGlitEqSfvjhB4XHYrEYd+7cgZGRETw8PChJIgCAc3cfo6pailYWQnjYm+m7OYQQQohGtEqS0tPTlbYVFRUhODiYbktC5Gqvss3hcPTcGkIIIUQzOpuTZGFhgTVr1iAqKkpXVZIm7uTTJImG2gghhDRFOp24/eTJExQWFuqyStJE5ZdU4mp2EQC6XxshhJCmSavhtk8//VThMWMM2dnZSExMxIgRI3TSMNK0nbqdDwDo1Moc9uYCPbeGEEII0ZxWSdKmTZsUHnO5XNjb22PmzJmIjIzUScNI05Z88xEAGmojhBDSdGmVJN25c0fX7SDNCGOs1vpIdCsSQgghTZNWc5IKCwvx+PFjpe2PHz9GUVHRCzeKNG0ZeaV4UFgBYx4XfV1t9N0cQgghRCtaJUmTJ0/G3r17lbZ/8803mDx58gs3ijRtsl4kH1driIx5em4NIYQQoh2tkqSzZ89i2LBhStuHDh2Ks2fPvnCjSNN2km5FQgghpBnQKkmqrKxEdXW10naxWIzy8vIXbhRpusQSKc5k1FzZNpAu/SeEENKEaZUk9enTB3FxcUrbv/jiC3h7e79wo0jT9de9JyiprIaVCR9dWlvquzmEEEKI1rRKktauXYvt27dj8ODBWL16NVavXo3BgwcjPj4eH374ocb1bdmyBW5ubhAKhfD29sbJkyfrLf/555/Dy8sLIpEInp6e2LVrl8J+sViMNWvWwMPDA0KhED169MCvv/76wsclDUu+9fRWJB524HHpViSEEEKaLq2SpAEDBuD06dNwcXHBN998g0OHDqF9+/b4+++/MWjQII3q2rdvH5YuXYp3330X6enpGDRoEEaOHInMzEyV5WNjYxEZGYlVq1bhypUrWL16NRYuXIhDhw7Jy6xYsQJbt27F5s2bcfXqVYSGhmLChAkK95zT9LhEPck0H4kQQkgzofVtSXr27Ik9e/bgypUrOH/+POLj49GhQweN64mOjkZISAjmzJkDLy8vxMTEwMXFBbGxsSrLJyYmYv78+QgKCoK7uzsmT56MkJAQrF+/XqHMO++8g1GjRsHd3R0LFixAQEAANm7cqPVxScOKK8RIv/cEAM1HIoQQ0vRptZjk4cOHwePxEBAQoLD9yJEjkEqlGDlypFr1VFVVIS0tDREREQrb/f39kZKSovI5lZWVEAqFCttEIhFSU1MhFovB5/PrLJOcnPxCx62srJQ/lq0HJRaLIRaL1Thb9cnq03W9je3UjVxIpAztbEzQypz/UtrfVGOlDxQr9VGs1Eex0gzFS32NFStN6tMqSYqIiMBHH32ktJ0xhoiICLWTpLy8PEgkEjg6Oipsd3R0RE5OjsrnBAQEYPv27Rg/fjx69+6NtLQ0xMfHQywWIy8vD05OTggICEB0dDQGDx4MDw8PHDt2DAcPHoREItH6uOvWrcPq1auVth89ehQmJiZqna+mkpKSGqXexvLdHS4ALpz5JTh8+PBLPXZTi5U+UazUR7FSH8VKMxQv9ek6VmVlZWqX1SpJunnzJjp37qy0vVOnTrh165bG9XE4ihN8GWNK22SioqKQk5ODfv36gTEGR0dHBAcHY8OGDeDxahYu/OSTTzB37lx06tQJHA4HHh4emDVrFhISErQ+bmRkJMLDw+WPi4qK4OLiAn9/f1hYWGh8zvURi8VISkrC8OHDwefzdVp3Y/rkk2QAZZj6Si/4d3ZssLwuNNVY6QPFSn0UK/VRrDRD8VJfY8VKkzuDaJUkWVpaIiMjA66urgrbb926BVNTU7XrsbOzA4/HU+q9yc3NVerlkRGJRIiPj8fWrVvx8OFDODk5IS4uDubm5rCzq5kHY29vjwMHDqCiogL5+flo3bo1IiIi4ObmpvVxBQIBBALlu9nz+fxGe6M3Zt269uBJOTLyysDlAAM7Or70djelWOkbxUp9FCv1Uaw0Q/FSn65jpUldWk3cHjt2LJYuXYrbt2/Lt926dQtvv/02xo4dq3Y9xsbG8Pb2VupKS0pKQv/+/et9Lp/Ph7OzM3g8Hvbu3YsxY8aAy1U8HaFQiDZt2qC6uhr79+/HuHHjXvi4RDXZVW3dna1gKaI/fEIIIU2fVj1JH3/8MUaMGIFOnTrB2dkZAHD//n0MGjQI/+///T+N6goPD8f06dPh4+MDPz8/xMXFITMzE6GhoQBqhrmysrLkayHduHEDqamp8PX1RUFBAaKjo3H58mXs3LlTXufZs2eRlZWFnj17IisrC6tWrYJUKsXy5cvVPi7RzMmn6yMNokv/CSGENBNaD7elpKQgKSkJf/31F0QiEbp3747BgwdrXFdQUBDy8/OxZs0aZGdno2vXrjh8+DDatWsHAMjOzlZYu0gikWDjxo24fv06+Hw+hg0bhpSUFIWhv4qKCqxYsQIZGRkwMzPDqFGjkJiYCCsrK7WPS9QnlTKkPE2S6NJ/QgghzYVWSRJQM+nZ398f/v7+L9yIsLAwhIWFqdy3Y8cOhcdeXl4Ki0KqMmTIEFy9evWFjkvUdy2nCPmlVTAx5qFXW2t9N4cQQgjRCa2TpNLSUpw4cQKZmZmoqqpS2Ld48eIXbhhpOmTzkfq528LYSOv1SQkhhBCDolWSlJ6ejlGjRqGsrAylpaWwsbFBXl4eTExM4ODgQElSC5NMQ22EEEKaIa3+279s2TIEBgbi8ePHEIlEOHPmDP799194e3trPHGbNG0VYglS7zwGQJO2CSGENC9aJUkXL17E22+/DR6PBx6Ph8rKSri4uGDDhg145513dN1GYsDO3y1AZbUUjhYCtHcw03dzCCGEEJ3RKkni8/nylakdHR3lV59ZWloqXIlGmr+Ttx4BAAa2t69ztXJCCCGkKdJqTlKvXr1w/vx5dOzYEcOGDcN7772HvLw8JCYmolu3brpuIzFgsknbAzvY6rklhBBCiG5p1ZP04YcfwsnJCQDw/vvvw9bWFgsWLEBubi7i4uJ02kBiuPJLKnHlQc09cAbQpG1CCCHNjFY9ST4+PvLf7e3t67zj+6lTp+Dj46Pynmek6Uu5nQ8A6NTKHA7mQj23hhBCCNGtRl3UZuTIkcjKymrMQxA9kg+1US8SIYSQZqhRkyTGWGNWT/SIMfZsfSS69J8QQkgzRMsjE63cyStF1pNyGPO48HWjSduEEEKaH0qSiFZkvUje7awhMubpuTWEEEKI7lGSRLRy8iYNtRFCCGneGjVJosUFm6dqiRRnnl7ZRrciIYQQ0lzRxG2isb/uP0FxZTUsRXx0aW2p7+YQQgghjUKrJGnVqlX4999/GyxXXFwMd3d3bQ5BDJhsqG1Ae1vwuNRbSAghpHnSKkk6dOgQPDw88J///AdfffUVKioqdN0uYsCerY9kr+eWEEIIIY1HqyQpLS0NFy5cQPfu3bFs2TI4OTlhwYIFOHfunK7bRwxMcYUY6feeAKD5SIQQQpo3reckde/eHZs2bUJWVhbi4+ORlZWFAQMGoFu3bvjkk09QWFioy3YSA3E24zEkUoZ2tiZwsTHRd3MIIYSQRvPCE7elUimqqqpQWVkJxhhsbGwQGxsLFxcX7Nu3TxdtJAZEvso23YqEEEJIM6d1kpSWloZFixbByckJy5YtQ69evXDt2jWcOHEC//zzD1auXInFixfrsq3EAJy8+QgADbURQghp/rRKkrp3745+/frhzp07+PLLL3Hv3j189NFHaN++vbzMjBkz8OjRI501lOhfdmE5bj8qBZcD+HlQkkQIIaR5M9LmSW+88QZmz56NNm3a1FnG3t4eUqlU64YRwyO79L+7sxUsRXw9t4YQQghpXBr3JInFYiQkJNDE7BZIduk/DbURQghpCTROkvh8PiorK+mWIy2MVMpw6pZsEUlKkgghhDR/Ws1Jeuutt7B+/XpUV1fruj3EQF3LKUJ+aRVMjHno3dZa380hhBBCGp1Wc5LOnj2LY8eO4ejRo+jWrRtMTU0V9n///fc6aRwxHLJeJF83GxgbNeot/wghhBCDoFWSZGVlhYkTJ+q6LcSAySZtD+xAtyIhhBDSMmiVJCUkJOi6HcSAVYglSL3zGABN2iaEENJy0LgJaVDavwWorJbC0UKADg5m+m4OIYQQ8lJo1ZMEAN999x2++eYbZGZmoqqqSmHfhQsXXrhhxHDIhtoGtLejqxoJIYS0GFr1JH366aeYNWsWHBwckJ6ejr59+8LW1hYZGRkYOXKkxvVt2bIFbm5uEAqF8Pb2xsmTJ+st//nnn8PLywsikQienp7YtWuXUpmYmBh4enpCJBLBxcUFy5YtQ0VFhXx/dXU1VqxYATc3N4hEIri7u2PNmjW0AKYKybfoViSEEEJaHq16krZs2YK4uDhMmTIFO3fuxPLly+Hu7o733nsPjx8/1qiuffv2YenSpdiyZQsGDBiArVu3YuTIkbh69Sratm2rVD42NhaRkZHYtm0b+vTpg9TUVMydOxfW1tYIDAwEAOzZswcRERGIj49H//79cePGDQQHBwMANm3aBABYv349vvjiC+zcuRNdunTB+fPnMWvWLFhaWmLJkiXahKVZelxahSsPigDQ+kiEEEJaFq16kjIzM9G/f38AgEgkQnFxMQBg+vTp+PrrrzWqKzo6GiEhIZgzZw68vLwQExMDFxcXxMbGqiyfmJiI+fPnIygoCO7u7pg8eTJCQkKwfv16eZnTp09jwIABmDp1KlxdXeHv748pU6bg/PnzCmXGjRuH0aNHw9XVFa+//jr8/f0VypCaS/8ZAzwdzeFgLtR3cwghhJCXRqskqVWrVsjPzwcAtGvXDmfOnAEA3LlzB4wxteupqqpCWloa/P39Fbb7+/sjJSVF5XMqKyshFCp+WYtEIqSmpkIsFgMABg4ciLS0NKSmpgIAMjIycPjwYYwePVr+nIEDB+LYsWO4ceMGAOCvv/5CcnIyRo0apXb7W4Jk+aX/1ItECCGkZdFquO2VV17BoUOH0Lt3b4SEhGDZsmX47rvvcP78ebz22mtq15OXlweJRAJHR0eF7Y6OjsjJyVH5nICAAGzfvh3jx49H7969kZaWhvj4eIjFYuTl5cHJyQmTJ0/Go0ePMHDgQDDGUF1djQULFiAiIkJez//+9z8UFhaiU6dO4PF4kEgkWLt2LaZMmaLyuJWVlaisrJQ/LiqqGYISi8Xy5ExXZPXpul5NMcZw8mbNfCQ/Nyu9t0cVQ4lVU0CxUh/FSn0UK81QvNTXWLHSpD6tkqS4uDj5BOfQ0FDY2NggOTkZgYGBCA0N1bi+56+YYozVeRVVVFQUcnJy0K9fPzDG4OjoiODgYGzYsAE8Hg8AcPz4caxduxZbtmyBr68vbt26hSVLlsDJyQlRUVEAauZC7d69G1999RW6dOmCixcvYunSpWjdujVmzpypdNx169Zh9erVStuPHj0KExMTjc9ZHUlJSY1Sr7pyy4EHhUbgcRgeXz+Hw7f02px66TtWTQnFSn0UK/VRrDRD8VKfrmNVVlamdlkO02R8TMeqqqpgYmKCb7/9FhMmTJBvX7JkCS5evIgTJ07U+VyxWIyHDx/CyckJcXFx+N///ocnT56Ay+Vi0KBB6NevHz7++GN5+d27d2PevHkoKSkBl8uFi4sLIiIisHDhQnmZDz74ALt378Y///yjdDxVPUkuLi7Iy8uDhYXFi4ZC6dySkpIwfPhw8Pl8ndatiT1nM7Hqp3/g62aN3bP76K0d9TGUWDUFFCv1UazUR7HSDMVLfY0Vq6KiItjZ2aGwsLDB72+t10l68uQJUlNTkZubq3TZ/IwZM9Sqw9jYGN7e3khKSlJIkpKSkjBu3Lh6n8vn8+Hs7AwA2Lt3L8aMGQMut2aKVVlZmfx3GR6PB8aYfM5UXWXqWgJAIBBAIBCobEdjvdEbs251pGQUAAAGd3Qw+D9mfceqKaFYqY9ipT6KlWYoXurTdaw0qUurJOnQoUOYNm0aSktLYW5urjA0xuFw1E6SACA8PBzTp0+Hj48P/Pz8EBcXh8zMTPmwXWRkJLKysuRrId24cQOpqanw9fVFQUEBoqOjcfnyZezcuVNeZ2BgIKKjo9GrVy/5cFtUVBTGjh0rH5ILDAzE2rVr0bZtW3Tp0gXp6emIjo7G7NmztQlJs1MtkeL07ZrJ+QPp0n9CCCEtkFZJ0ttvv43Zs2fjww8/fOH5OEFBQcjPz8eaNWuQnZ2Nrl274vDhw2jXrh0AIDs7G5mZmfLyEokEGzduxPXr18Hn8zFs2DCkpKTA1dVVXmbFihXgcDhYsWIFsrKyYG9vL0+KZDZv3oyoqCiEhYUhNzcXrVu3xvz58/Hee++90Pk0F3/dL0RxZTUsRXx0bWOp7+YQQgghL51WSVJWVhYWL16sswnLYWFhCAsLU7lvx44dCo+9vLyQnp5eb31GRkZYuXIlVq5cWWcZc3NzxMTEICYmRtPmtgjJ8luR2ILHpVuREEIIaXm0WicpICCAFl1s5mS3IqFVtgkhhLRUWvUkjR49Gv/9739x9epVdOvWTWkS1NixY3XSOKIfJZXVSM98AgAY1N5ev40hhBBC9ESrJGnu3LkAgDVr1ijt43A4kEgkL9YqoldnM/JRLWVoa2OCtraNswYUIYQQYui0SpLqukyeNA8n6VYkhBBCiHZzkkjzlnyrJkkaRPORCCGEtGBq9yR9+umnmDdvHoRCIT799NN6yy5evPiFG0b0I7uwHLdyS8DlAP09KEkihBDScqmdJG3atAnTpk2DUCjEpk2b6izH4XAoSWrCZJf+d3O2gqUJrQZLCCGk5VI7Sbpz547K32W3+ajrhrSkaaGhNkIIIaSG1nOSvvzyS3Tt2hVCoRBCoRBdu3bF9u3bddk28pJJpQynbtGkbUIIIQTQ8uq2qKgobNq0CW+99Rb8/PwAAKdPn8ayZctw9+5dfPDBBzptJHk5/skpRl5JFUR8Hnq1tdJ3cwghhBC90ipJio2NxbZt2zBlyhT5trFjx6J79+546623KElqomSrbPu620BgxNNzawghhBD90mq4TSKRwMfHR2m7t7c3qqurX7hRRD/k6yPRfCRCCCFEuyTpzTffRGxsrNL2uLg4TJs27YUbRV6+CrEE5+4+BgAM6kC3IiGEEELUHm4LDw+X/87hcLB9+3YcPXoU/fr1AwCcOXMG9+7dw4wZM3TfStLoLvxbgAqxFA7mAnR0NNN3cwghhBC9UztJSk9PV3js7e0NALh9+zYAwN7eHvb29rhy5YoOm0delpO3ng210XIOhBBCiAZJ0h9//NGY7SB6lkz3ayOEEEIU0L3bCApKq3D5QSEAmrRNCCGEyFCSRHDqdh4YAzwdzeFgIdR3cwghhBCDQEkSkQ+1DaBeJEIIIUSOkqQWjjEmXx9pEM1HIoQQQuQoSWrh7uaXIetJOfg8DnzdbfTdHEIIIcRgUJLUwiU/vfS/d1trmBhrdZcaQgghpFmiJKmFS75Zc782GmojhBBCFFGS1IJVS6RIuZ0PABhItyIhhBBCFFCS1IL9nVWI4opqWIr46NbGUt/NIYQQQgwKJUktmOzS//4etuBx6VYkhBBCSG2UJLVgdCsSQgghpG6UJLVQJZXVuJBZAAAY1J7mIxFCCCHPoySphTqbkY9qKYOLjQhtbU303RxCCCHE4FCS1ELJVtkeSL1IhBBCiEqUJLVQp27RrUgIIYSQ+lCS1ALlFFbgZm4JOJyaK9sIIYQQoswgkqQtW7bAzc0NQqEQ3t7eOHnyZL3lP//8c3h5eUEkEsHT0xO7du1SKhMTEwNPT0+IRCK4uLhg2bJlqKioUCiTlZWFN998E7a2tjAxMUHPnj2Rlpam03MzRLJbkXRvYwkrE2M9t4YQQggxTHq/Wde+ffuwdOlSbNmyBQMGDMDWrVsxcuRIXL16FW3btlUqHxsbi8jISGzbtg19+vRBamoq5s6dC2trawQGBgIA9uzZg4iICMTHx6N///64ceMGgoODAQCbNm0CABQUFGDAgAEYNmwYfvnlFzg4OOD27duwsrJ6WaeuN7JbkdCl/4QQQkjd9J4kRUdHIyQkBHPmzAFQ0wN05MgRxMbGYt26dUrlExMTMX/+fAQFBQEA3N3dcebMGaxfv16eJJ0+fRoDBgzA1KlTAQCurq6YMmUKUlNT5fWsX78eLi4uSEhIkG9zdXVtrNM0GIwxJN96eisSmrRNCCGE1EmvSVJVVRXS0tIQERGhsN3f3x8pKSkqn1NZWQmhUKiwTSQSITU1FWKxGHw+HwMHDsTu3buRmpqKvn37IiMjA4cPH8bMmTPlz/nxxx8REBCAN954AydOnECbNm0QFhaGuXPn1nncyspK+eOioiIAgFgshlgs1ur86yKrT9f1AsA/OcXIK6mEiM9Ft9ZmjXKMl6kxY9XcUKzUR7FSH8VKMxQv9TVWrDSpT69JUl5eHiQSCRwdHRW2Ozo6IicnR+VzAgICsH37dowfPx69e/dGWloa4uPjIRaLkZeXBycnJ0yePBmPHj3CwIEDwRhDdXU1FixYoJCMZWRkIDY2FuHh4XjnnXeQmpqKxYsXQyAQYMaMGUrHXbduHVavXq20/ejRozAxaZx1hpKSknRe5+8POAB4cDWtxrGjv+q8fn1pjFg1VxQr9VGs1Eex0gzFS326jlVZWZnaZfU+3AYAHI7ifcMYY0rbZKKiopCTk4N+/fqBMQZHR0cEBwdjw4YN4PF4AIDjx49j7dq12LJlC3x9fXHr1i0sWbIETk5OiIqKAgBIpVL4+Pjgww8/BAD06tULV65cQWxsrMokKTIyEuHh4fLHRUVFcHFxgb+/PywsLHQSBxmxWIykpCQMHz4cfD5fp3V/tzMNQD7G9+uEUQNcdVq3PjRmrJobipX6KFbqo1hphuKlvsaKlWwkSB16TZLs7OzA4/GUeo1yc3OVepdkRCIR4uPjsXXrVjx8+BBOTk6Ii4uDubk57OxqJiJHRUVh+vTp8nlO3bp1Q2lpKebNm4d3330XXC4XTk5O6Ny5s0LdXl5e2L9/v8rjCgQCCAQCpe18Pr/R3ui6rrtCLMG5f2tuRTKkk2Oz+gNtzNehuaFYqY9ipT6KlWYoXurTdaw0qUuvSwAYGxvD29tbqSstKSkJ/fv3r/e5fD4fzs7O4PF42Lt3L8aMGQMut+Z0ysrK5L/L8Hg8MMbAGAMADBgwANevX1coc+PGDbRr1+5FT8tgXcgsQIVYCntzATwdzfXdHEIIIcSg6X24LTw8HNOnT4ePjw/8/PwQFxeHzMxMhIaGAqgZ5srKypKvhXTjxg2kpqbC19cXBQUFiI6OxuXLl7Fz5055nYGBgYiOjkavXr3kw21RUVEYO3asfEhu2bJl6N+/Pz788ENMmjQJqampiIuLQ1xc3MsPwkuSLL8ViV2dw5mEEEIIqaH3JCkoKAj5+flYs2YNsrOz0bVrVxw+fFjeo5OdnY3MzEx5eYlEgo0bN+L69evg8/kYNmwYUlJSFC7fX7FiBTgcDlasWIGsrCzY29sjMDAQa9eulZfp06cPfvjhB0RGRmLNmjVwc3NDTEwMpk2b9tLO/WWTLSI5sD2tj0QIIYQ0RO9JEgCEhYUhLCxM5b4dO3YoPPby8kJ6enq99RkZGWHlypVYuXJlveXGjBmDMWPGaNTWpqqgtAqXsgoB0CKShBBCiDoM4rYkpPGl3M4HY0BHRzM4WggbfgIhhBDSwlGS1EIk33p6KxJaZZsQQghRCyVJLQBjDCefTtoeRENthBBCiFooSWoB/s0vw/2CcvB5HPR1s9F3cwghhJAmgZKkFuDk06vaerW1hqnAIObqE0IIIQaPkqQWIPlmzXykQXTpPyGEEKI2SpKaOYmUIeV2PgC69J8QQgjRBCVJzdzf95+guKIaFkIjdHe20ndzCCGEkCaDkqRmTnYrkv4eduBx6VYkhBBCiLooSWrmZJO2aaiNEEII0QwlSc1YaWU10jMLAND6SIQQQoimKElqxs7eyYdYwuBiI0I7W1N9N4cQQghpUihJasZkq2wPpEv/CSGEEI1RktSMJcuTJLpfGyGEEKIpSpKaqZzCCtzMLQGHA/T3sNV3cwghhJAmh5KkZurU06vaurWxhLWpsZ5bQwghhDQ9lCQ1U8m3aD4SIYQQ8iIoSWqGGGPPkiS69J8QQgjRCiVJzdD1h8V4VFwJEZ8H73bW+m4OIYQQ0iRRktQMya5q6+tmA4ERT8+tIYQQQpomSpKaIdn6SLTKNiGEEKI9SpKamcpqCc7eyQdA85EIIYSQF0FJUjOT9m8BKsRS2JkJ4Oloru/mEEIIIU0WJUnNzLNVtm3B4XD03BpCCCGk6aIkqZk5Jb/0n25FQgghhLwISpKakSdlVfg7qxAALSJJCCGEvChKkpqRlNv5YAzo4GCGVpZCfTeHEEIIadIoSWpGZJf+01VthBBCyIujJKkZSb71CACtj0QIIYToAiVJzcS/+aW497gcfB4Hvm62+m4OIYQQ0uRRktRMyIbaerW1hqnASM+tIYQQQpo+g0iStmzZAjc3NwiFQnh7e+PkyZP1lv/888/h5eUFkUgET09P7Nq1S6lMTEwMPD09IRKJ4OLigmXLlqGiokJlfevWrQOHw8HSpUt1cTp68Wx9JBpqI4QQQnRB710O+/btw9KlS7FlyxYMGDAAW7duxciRI3H16lW0bdtWqXxsbCwiIyOxbds29OnTB6mpqZg7dy6sra0RGBgIANizZw8iIiIQHx+P/v3748aNGwgODgYAbNq0SaG+c+fOIS4uDt27d2/0c20sEilDym2atE0IIYTokt57kqKjoxESEoI5c+bAy8sLMTExcHFxQWxsrMryiYmJmD9/PoKCguDu7o7JkycjJCQE69evl5c5ffo0BgwYgKlTp8LV1RX+/v6YMmUKzp8/r1BXSUkJpk2bhm3btsHa2rpRz7MxXcoqRFFFNcyFRujexlLfzSGEEEKaBb32JFVVVSEtLQ0REREK2/39/ZGSkqLyOZWVlRAKFdcAEolESE1NhVgsBp/Px8CBA7F7926kpqaib9++yMjIwOHDhzFz5kyF5y1cuBCjR4/Gq6++ig8++KDetlZWVqKyslL+uKioCAAgFoshFovVPmd1yOpTt94T/zwEAPRzswGTSiCWSnTaHkOmaaxaMoqV+ihW6qNYaYbipb7GipUm9ek1ScrLy4NEIoGjo6PCdkdHR+Tk5Kh8TkBAALZv347x48ejd+/eSEtLQ3x8PMRiMfLy8uDk5ITJkyfj0aNHGDhwIBhjqK6uxoIFCxSSsb179+LChQs4d+6cWm1dt24dVq9erbT96NGjMDEx0eCs1ZeUlKRWuUNXeAA4sKrIxuHDDxqlLYZO3VgRipUmKFbqo1hphuKlPl3HqqysTO2yep+TBEDpRqyMsTpvzhoVFYWcnBz069cPjDE4OjoiODgYGzZsAI/HAwAcP34ca9euxZYtW+Dr64tbt25hyZIlcHJyQlRUFO7du4clS5bg6NGjSr1SdYmMjER4eLj8cVFREVxcXODv7w8LCwstz1w1sViMpKQkDB8+HHw+v96ypZXV+L/UPwAwzB83BO1sGydhM1SaxKqlo1ipj2KlPoqVZihe6musWMlGgtSh1yTJzs4OPB5PqdcoNzdXqXdJRiQSIT4+Hlu3bsXDhw/h5OSEuLg4mJubw86uZtJyVFQUpk+fjjlz5gAAunXrhtLSUsybNw/vvvsu0tLSkJubC29vb3m9EokEf/75Jz777DNUVlbKEy4ZgUAAgUCg1B4+n99ob3R16k6/XQCxhMHZWgQPR4s6k8vmrjFfh+aGYqU+ipX6KFaaoXipT9ex0qQuvU7cNjY2hre3t1JXWlJSEvr371/vc/l8PpydncHj8bB3716MGTMGXG7N6ZSVlcl/l+HxeGCMgTGG//znP7h06RIuXrwo//Hx8cG0adNw8eJFpQTJkMnWRxrUwa7FJkiEEEJIY9D7cFt4eDimT58OHx8f+Pn5IS4uDpmZmQgNDQVQM8yVlZUlXwvpxo0bSE1Nha+vLwoKChAdHY3Lly9j586d8joDAwMRHR2NXr16yYfboqKiMHbsWPB4PJibm6Nr164K7TA1NYWtra3SdkMnuxXJwPb2em4JIYQQ0rzoPUkKCgpCfn4+1qxZg+zsbHTt2hWHDx9Gu3btAADZ2dnIzMyUl5dIJNi4cSOuX78OPp+PYcOGISUlBa6urvIyK1asAIfDwYoVK5CVlQV7e3sEBgZi7dq1L/v0GtXDogrceFgCDgfo70G3IiGEEEJ0Se9JEgCEhYUhLCxM5b4dO3YoPPby8kJ6enq99RkZGWHlypVYuXKl2m04fvy42mUNhWyV7a6tLWFtaqzn1hBCCCHNi94XkyTaS75Fq2wTQgghjYWSpCaKMSZPkgbR/doIIYQQnTOI4TaiuRsPS/CouBJCPhferk33liqEkMYnlUpRVVXVKHWLxWIYGRmhoqICEknLWe1fWxQv9b1IrIyNjZWuctcGJUlN1MmbNVe19XWzhcCo6SxZQAh5uaqqqnDnzh1IpdJGqZ8xhlatWuHevXu0DIkaKF7qe5FYcblcuLm5wdj4xebrUpLURNFQGyGkIYwxZGdng8fjwcXFRSf/s36eVCpFSUkJzMzMGqX+5obipT5tYyWVSvHgwQNkZ2ejbdu2L5SMUpLUBFVWS3A24zEAmrRNCKlbdXU1ysrK0Lp160a7x6RsKE8oFNKXvhooXup7kVjZ29vjwYMHqK6ufqHVuukVaoIu/PsE5WIJ7MyM0amVub6bQwgxULJ5HC865EBIUyN7z7/ovC9Kkpog2SrbA9rTrUgIIQ2jzwnS0ujqPU9JUhMkW0RyIM1HIoQQQhoNJUlNzJOyKvydVQgAGNSB7tdGCGl+hg4diqVLl9a539XVFTExMS+tPaTloonbTczp2/lgDGjvYIZWlkJ9N4cQQl66c+fOwdTUVN/NIC0A9SQ1MSdv0VAbIaRls7e3b7Sr9WQaa/FNfROLxfpuQpNCSVITI5uPNIgu/SeENGPV1dVYtGgRrKysYGtrixUrVoAxBkB5uI3D4WD79u2YMGECTExM0KFDB/z444/y/RKJBCEhIXBzc4NIJIKnpyc++eQTheMFBwdj/PjxWLduHVq3bo2OHTtizZo16Natm1LbvL298d577zV4DufOncPw4cNhZ2cHS0tLDBkyBBcuXFAo8+TJE8ybNw+Ojo4QCoXo2rUrfvrpJ/n+U6dOYciQITAxMYG1tTUCAgJQUFCgMg4A0LNnT6xatUohNl988QXGjRsHU1NTfPDBB2rFAwDi4+PRpUsXCAQCODk5YdGiRQCA2bNnY8yYMQplq6ur0apVK8THxzcYl6aEhtuakMz8MmQ+LoMRlwNfd1t9N4cQ0sQwxlAu1u2tMKRSKcqrJDCqqq53LRsRn6fRFUc7d+5ESEgIzp49i/Pnz2PevHlo164d5s6dq7L86tWrsWHDBnz88cfYvHkzpk2bhn///Rc2NjaQSqVwdnbGN998Azs7O6SkpGDevHlwcnLCpEmT5HUcO3YMFhYWSEpKAmMMVlZWWL16Nc6dO4c+ffoAAP7++2+kp6fj22+/bfAciouLMXPmTHz66acAgI0bN2LMmDE4d+4cLCwsIJVKMXLkSBQXF2P37t3w8PDA1atXwePV3EXh4sWL+M9//oPZs2fj008/hZGREf744w+NL2tfuXIl1q1bh02bNoHH46kVj9jYWISHh+Ojjz7CyJEjUVhYiFOnTgEA5syZg8GDByM7OxtOTk4AgMOHD6OkpEQhns0BJUlNyMmnl/73bmsNMwG9dIQQzZSLJej83hG9HPvqmgCYGKv/ueXi4oJNmzaBw+HA09MTly5dwqZNm+pMkoKDgzFlyhQAwIcffojNmzcjNTUVI0aMAJ/Px+rVq+Vl3dzckJKSgm+++UbhS93U1BTbt29XWFcqICAACQkJ8iQpISEBQ4YMgbu7e4Pn8Morryg83rp1K6ytrXHq1ClMmjQJv/32G1JTU3Ht2jV07NgRABTq3bBhA3x8fLBlyxb5ti5dujR43OdNnToVs2fPVtjWUDw++OADvP3221iyZIm8nCwG/fv3h6enJxITE7F8+XIANXF54403YGZmpnH7DBkNtzUh8kv/aaiNENLM9evXT6Hnyc/PDzdv3qyzF6V79+7y301NTWFubo7c3Fz5ti+++AI+Pj6wt7eHmZkZtm3bhszMTIU6unXrprTw5ty5c/H111+joqICYrEYe/bsUUo46pKbm4vQ0FB07NgRlpaWsLS0RElJCe7fvw+gpqfI2dlZniA9T9aT9KJ8fHyUttUXj9zcXDx48KDeY8+ZMwcJCQny8j///LPacWlKqDuiiZBIGVJu5wOoWUSSEEI0JeLzcHVNgE7rlEqlKC4qhrmFeYPDbY3p+VtPcDgc+U19v/nmGyxbtgwbN26En58fzM3N8fHHH+Ps2bMKz1F1xVxgYCAEAgF++OEHCAQCVFZWYuLEiWq1KTg4GI8ePUJMTAzatWsHgUAAPz8/+eRpkUhU7/Mb2s/lcuXztGRUTcx+/rwaikdDxwWAGTNmICIiAqdPn8bp06fh6uqKQYMGNfi8poaSpCbiUlYhCsvFMBcaoYezpb6bQwhpgjgcjkZDXuqQSqWoNubBxNhIp/ciO3PmjNLjDh06yOfraOLkyZPo378/wsLC5Ntu376t1nONjIwwc+ZMJCQkQCAQYPLkyWpfWXfy5Els2bIFo0aNAgDcu3cPeXl58v3du3fH/fv3cePGDZW9Sd27d8exY8cUhsZqs7e3R3Z2tvxxUVER7ty5o1a76ouHubk5XF1dcezYMQwbNkxlHba2thg/fjwSEhJw+vRpzJo1q8HjNkWUJDURyTdr5iP5udvCiEejpISQ5u3evXsIDw/H/PnzceHCBWzevBkbN27Uqq727dtj165dOHLkCNzc3JCYmIhz587Bzc1NrefPmTMHXl5eACCfvKzucRMTE+Hj44OioiL897//VeilGTJkCAYPHoyJEyciOjoa7du3xz///AMOh4MRI0YgMjIS3bp1Q1hYGEJDQ2FsbIw//vgDb7zxBuzs7PDKK69gx44dCAwMhLW1NaKiotRKItWJx6pVqxAaGgoHBwf55PJTp07hrbfeUojLmDFjIJFIMHPmTLXj0pTQt20TkXyLLv0nhLQcM2bMQHl5Ofr27YuFCxfirbfewrx587SqKzQ0FK+99hqCgoLg6+uL/Px8hV6UhnTo0EE+WdnX11ft58XHx6OgoAC9evXC9OnTsXjxYjg4OCiU2b9/P/r06YMpU6agc+fOWL58uXzeVceOHXH06FH89ddf6Nu3L/z8/HDw4EEYGdX0b0RGRmLw4MEYM2YMRo0ahfHjx8PDw0Mn8Zg5cyZiYmKwZcsWdOnSBWPGjMHNmzcVyrz66qtwcnJCQEAAWrdurXZcmhIOe35Ak6ilqKgIlpaWKCwshIWFhU7rFovFOHz4MEaNGgU+n4+yqmr0WH0UYgnDH/83FG52tNKszPOxInWjWKmvucSqoqICd+7cgZubG4TCxlmhXyqVoqioCBYWFjodbjMkjDF06tQJ8+fPR3h4+AvV1ZziVVZWhtatWyM+Ph6vvfaazut/kVjV997X5PubhtuagLN3HkMsYWhjJYKrbeOuMksIIeSZ3NxcJCYmIisrq9nOu9GUVCpFTk4ONm7cCEtLS4wdO1bfTWo0lCQ1AbVX2dZkMTZCCCEvxtHREXZ2doiLi4O1tbXCvvrWBPrll1+a5dVeAJCZmQk3Nzc4Oztjx44d8uG/5qj5nlkzQusjEUKIftQ3I+XixYt17mvTpk0jtMYwuLq61huX5oSSJAOXW1SB6w+LweEAAzwoSSKEEEPRvn17fTeBNLKmPWusBZBd1da1tSWsTY0bKE0IIYQQXaEkycDJhtpolW1CCCHk5aIkyYAxxmh9JEIIIURPKEkyYLdyS5FbXAmBERfe7awbfgIhhBBCdIaSJAOW/PSGtn3dbCBs5JtDEkIIIUQRJUkG7NTTJImG2gghRH2urq6IiYlRqyyHw8GBAwfq3H/37l1wOJx6L/cnzZdBJElbtmyRLx3u7e2NkydP1lv+888/h5eXF0QiETw9PbFr1y6lMjExMfD09IRIJIKLiwuWLVuGiooK+f5169ahT58+MDc3h4ODA8aPH4/r16/r/Ny0VS0Fzt0tAAAMbG+v59YQQkjL5OLiguzsbHTt2lXfTSF6oPckad++fVi6dCneffddpKenY9CgQRg5ciQyMzNVlo+NjUVkZCRWrVqFK1euYPXq1Vi4cCEOHTokL7Nnzx5ERERg5cqVuHbtGr788kvs27cPkZGR8jInTpzAwoULcebMGSQlJaG6uhr+/v4oLS1t9HNWx91ioKxKAjszY3RqZa7v5hBCSIvE4/HQqlWrRl9VuqqqqlHr1wfGGKqrq/XdjBei9yQpOjoaISEhmDNnDry8vBATEwMXFxfExsaqLJ+YmIj58+cjKCgI7u7umDx5MkJCQrB+/Xp5mdOnT2PAgAGYOnUqXF1d4e/vjylTpuD8+fPyMr/++iuCg4PRpUsX9OjRAwkJCcjMzERaWlqjn7M6rhfWvDQD2tuBy6VbkRBCWoatW7eiTZs2kEqlCtvHjh2LmTNn4vbt2xg3bhwcHR1hZmaGPn364LfffnuhY2ZnZ2PkyJEQiURwc3PDt99+K9/3/HDb8ePHweFwcOzYMfj4+MDExAT9+/dXGIlQp43u7u744IMPEBwcDEtLS8ydOxevvPIKFi1apFAuPz8fAoEAv//+e4PnsXv3bvj4+MDc3BytWrXC1KlTkZubq1DmypUrGD16NCwsLGBubo5Bgwbh9u3b8v3x8fHo0qULBAIBnJyc5O1RNez45MkTcDgcHD9+XCE2R44cgY+PDwQCAU6ePKlWPCorK7F8+XK4uLhAIBCgQ4cO+PLLL8EYQ8eOHfH//t//Uyh/+fJlcLlchbY3Br0mSVVVVUhLS4O/v7/Cdn9/f6SkpKh8TmVlpdIdfUUiEVJTUyEWiwEAAwcORFpaGlJTUwEAGRkZOHz4MEaPHl1nWwoLCwEANjY2Wp+PLl0vrEmMBtL6SIQQXWEMqCrV/Y+4rOEyat7G4o033kBeXh7++OMP+baCggIcOXIE06ZNQ0lJCUaNGoXffvsN6enpCAgIQGBgYJ2jD+qIiorCxIkT8ddff+HNN9/ElClTcO3atXqf8+6772Ljxo04f/48jIyMMHv2bPk+ddv48ccfo2vXrkhLS0NUVBTmzJmDr776CpWVlfIye/bsQevWrTFs2LAGz6Oqqgrvv/8+/vrrLxw4cAB37txBcHCwfH9WVhYGDx4MoVCI33//HWlpaZg9e7a8tyc2NhYLFy7EvHnzcOnSJfz4449arSq+fPlyrFu3DteuXUP37t3ViseMGTOwd+9efPrpp7h27Rq++OILmJmZgcPhYNasWUhISFA4Rnx8PAYNGgQPDw+N26cJvd6WJC8vDxKJBI6OjgrbHR0dkZOTo/I5AQEB2L59O8aPH4/evXsjLS0N8fHxEIvFyMvLg5OTEyZPnoxHjx5h4MCB8u6+BQsWICIiQmWdjDGEh4dj4MCBdY47V1ZWKrxxi4qKAABisVienOlKXlEZMktqfvd1tdJ5/c2JLDYUo4ZRrNTXXGIlFovBGINUKq3pmakqBfcjZ50egwvASo1y0oj7gLFpg+WsrKwQEBCAPXv2yBODffv2wcbGBsOGDQOPx0O3bt3k5desWYMffvgBBw8exMKFC+XbZeetjtdff12e5KxevRpJSUn49NNP8fnnn8vrkMVQ9vj999+X38B2+fLlCAwMRFlZGYRCIbp161ZnG8PCwuTbhw0bhvDwcPnjNm3a4K233sIPP/yASZMmAQASEhIwc+ZMMMYavF9a7YRINnm9X79+KCoqgpmZGT777DNYWlriq6++Ap/PB/Ds1ipSqRQffPABwsPD8dZbb8nr8fb2Vjjv539XFZtVq1bhP//5j7wOa2vrel+zGzdu4JtvvsGRI0fw6quvytvPGENxcTFmzpyJlStX4syZM+jbty/EYjF2796N9evX1/kaS6VSMMYgFovB4yleHa7J37VB3Lvt+TvbM8bqvNt9VFQUcnJy0K9fPzDG4OjoiODgYGzYsEEeiOPHj2Pt2rXYsmULfH19cevWLSxZsgROTk6IiopSqnPRokX4+++/kZycXGcb161bh9WrVyttP3r0KExMTDQ53QZdzOeAgQdHEUP6qd+RrtPam6ekpCR9N6HJoFipr6nHysjICK1atUJJSUnNnBdxmVoJTWMoKi4G+BK1yk6YMAFLly7FunXrIBAIkJiYiAkTJqC0tBSlpaVYv349jh49iuzsbEgkEpSXl+PmzZvy/7xKpVJUVFTIHzekZ8+eCmV79+6NS5cuoaioCCUlNf9jLS0tRVFREcrKygAAbm5u8udYWFgAqBlmc3FxqbeNxcXF8jZ27dpVqY1vvPEGtm/fjhEjRuDSpUv466+/sHPnTrXO5e+//8ZHH32ES5cu4cmTJ/IE4urVq+jUqRPOnz8PX19flJeXo7y8XOG5jx49woMHD+RJ1fOejwMA+bmUlZUpxMbT01OhjoZes9OnT4PH46FXr14qj21mZgZ/f39s3boVnTp1wk8//YSKigoEBATUGZeqqiqUl5fjzz//VJoXJWunOvSaJNnZ2YHH4yn1GuXm5ir1LsmIRCLEx8dj69atePjwIZycnBAXFwdzc3PY2dUMTUVFRWH69OmYM2cOAKBbt24oLS3FvHnz8O6774LLfTbK+NZbb+HHH3/En3/+CWfnuv+HFRkZqZDxFxUVwcXFBf7+/vI/EF05deAygAd4tZszRo3qotO6mxuxWIykpCQMHz5c/j8johrFSn3NJVYVFRW4d+8ezMzMaqYpMPOaHh0dYoyhuKQE5k+HRupiwTcB6tlf26RJk7BkyRKcPHkSffr0wenTpxETEwMLCwtERkbi6NGj2LBhA9q3bw+RSIRJkyaBw+HIP4u5XC6EQqHan83PlzU2Ngafz4eFhQXMzMwAAKamprCwsJD/p9jGxkb+nOfL1NdGc3NzFBcXg8vlwtbWVqmNCxYsQO/evVFUVIRvvvkGr7zyilpX1pWWlmLixIkYPnw4du/eDXt7e2RmZmLkyJEwNjaWz0GSndfzZK+diYmJyv2ybbX3y0ZXZNtksWnVqpVCHQ29ZrJpLhYWFgp/b7KeJHNzc8yfPx8zZ87EZ599hn379mHSpElo1apVnfGoqKiASCSSDy/Wpm7yDOg5STI2Noa3tzeSkpIwYcIE+fakpCSMGzeu3ufy+Xx5UrN3716MGTNGnvyUlZUpJEJAzRUKtbsrGWPybs3jx4/Dzc2t3uMJBAIIBAKV7dD1h+iZOzWX/g/qYN+kP6BfpsZ4HZoripX6mnqsJBIJOBwOuFzus89Enm6vlpVKpUClFByBmdLnrrZMTU3x2muv4euvv0ZGRgY6duyIPn36AACSk5MRHByMiRMnAqjp4bh79y6GDh2qcHzZeasjNTVVYajq7Nmz6NWrl0LcZL8//1j2e+1t9bWxdiKpqo09evSAj48PvvzyS3z99dfYvHmzWudx48YN5OXlYf369XBxcQEAXLhwQaFdPXr0wM6dOyGRSJTe15aWlnB1dcUff/yhMFQmI+u4ePjwobw9f//9d4OxARp+zXr06AGpVIqTJ0/Kh9uAZ8N5HA4HY8aMgampKbZu3Ypff/0Vf/75Z71x4XK54HA4Kv+GNfmb1vtwW3h4OKZPnw4fHx/4+fkhLi4OmZmZCA0NBVCTgWZlZcnXQrpx4wZSU1Ph6+uLgoICREdH4/Lly9i5c6e8zsDAQERHR6NXr17y4baoqCiMHTtWPiS3cOFCfPXVVzh48CDMzc3lvVmWlpYQiUQvOQrP3HtchszH5eByGHzdDGMSOSGEvGzTpk1DYGAgrly5gjfffFO+vX379vj+++8RGBgIDoeDqKgotece1eXbb7+Fj48PBg4ciD179iA1NRVffvml1vW9aBvnzJmDRYsWwcTERKEDoT5t27aFsbExNm/ejNDQUFy+fBnvv/++QplFixZh8+bNmDx5MiIjI2FpaSmf5+Pp6YlVq1YhNDQUDg4OGDlyJIqLi3Hq1Cm89dZbEIlE6NevHz766CO4uroiLy8PK1as0Ek8XF1dMXPmTMyePRuffvopevTogX///Rc5OTkYMWIEgJqOjuDgYERGRqJ9+/bw8/NTM5ovRu9LAAQFBSEmJgZr1qxBz5498eeff+Lw4cNo164dgJpLM2vPgJdIJNi4cSN69OiB4cOHo6KiAikpKXB1dZWXWbFiBd5++22sWLECnTt3RkhICAICArB161Z5mdjYWBQWFmLo0KFwcnKS/+zbt++lnbsq9x6Xwc7MGK5mgJlA7zksIYToxSuvvAIbGxtcv34dU6dOlW/ftGkTrK2t0b9/fwQGBiIgIAC9e/d+oWOtXr0ae/fuRffu3bFz507s2bMHnTt31rq+F23jlClTYGRkhKlTpyoNFdXF3t4eO3bswLfffovOnTvjo48+Urps3tbWFr///jtKSkowZMgQeHt7Y9u2bfKelZkzZyImJgZbtmxBly5dMGbMGNy8eVP+fNlFUj4+PliyZAk++OADtdqmTjxiY2Px+uuvIywsDJ06dcLcuXOV1i0MCQlBVVWVwpWEjY3DGpouT1QqKiqCpaUlCgsLdT4nqaqqCt/++AsmjRvVpLv6XwaxWIzDhw9j1CiKVUMoVuprLrGqqKjAnTt35Hc0aAxSqRRFRUWwsLDQ2XBbc6ZOvO7duwdXV1ecO3fuhRPApuz5WJ06dQpDhw7F/fv365y3LFPfe1+T72/qqjBAHA4HZk33c5kQQogWxGIxsrOzERERgX79+rXoBKm2yspKZGVlISoqCpMmTWowQdIlSvsJIYQ0S3v27IGZmZnKny5dDO/K4VOnTqFdu3ZIS0vDF198obDv5MmTdZ6L7Oq65urrr7+Gp6cnCgsLsWHDhpd6bOpJIoQQ0iyNHTsWvr6+KvcZ4jDq0KFD61ww0sfHR+GWIC1JcHDwS52HVBslSYQQQpolc3NzmJs3jxuEi0QirW4RQl4MDbcRQgghhKhASRIhhDRzdBEzaWl09Z6nJIkQQpop2eK5VVVVem4JIS+X7D3//M1tNUVzkgghpJkyMjKCiYkJHj16BD6f3yjrGEmlUlRVVaGiooLWSVIDxUt92sZKKpXi0aNHMDExgZHRi6U5lCQRQkgzxeFw4OTkhDt37uDff/9tlGMwxlBeXg6RSFTvDW5JDYqX+l4kVlwuF23btn3hGFOSRAghzZixsTE6dOjQaENuYrEYf/75JwYPHmyQl9UbGoqX+l4kVsbGxjrpqaMkiRBCmjkul9totyXh8Xiorq6GUCikL301ULzUZwixogFRQgghhBAVKEkihBBCCFGBkiRCCCGEEBVoTpKWZAtVFRUV6bxusViMsrIyFBUV0Zh1AyhW6qNYqY9ipT6KlWYoXuprrFjJvrfVWXCSkiQtFRcXAwBcXFz03BJCCCGEaKq4uBiWlpb1luEwWq9eK1KpFA8ePIC5ubnO17ooKiqCi4sL7t27BwsLC53W3dxQrNRHsVIfxUp9FCvNULzU11ixYoyhuLgYrVu3bnCZAOpJ0hKXy4Wzs3OjHsPCwoL+iNREsVIfxUp9FCv1Uaw0Q/FSX2PEqqEeJBmauE0IIYQQogIlSYQQQgghKlCSZIAEAgFWrlwJgUCg76YYPIqV+ihW6qNYqY9ipRmKl/oMIVY0cZsQQgghRAXqSSKEEEIIUYGSJEIIIYQQFShJIoQQQghRgZIkQgghhBAVKEkyMFu2bIGbmxuEQiG8vb1x8uRJfTfJIP35558IDAxE69atweFwcODAAX03yWCtW7cOffr0gbm5ORwcHDB+/Hhcv35d380ySLGxsejevbt88To/Pz/88ssv+m5Wk7Bu3TpwOBwsXbpU300xOKtWrQKHw1H4adWqlb6bZbCysrLw5ptvwtbWFiYmJujZsyfS0tL00hZKkgzIvn37sHTpUrz77rtIT0/HoEGDMHLkSGRmZuq7aQantLQUPXr0wGeffabvphi8EydOYOHChThz5gySkpJQXV0Nf39/lJaW6rtpBsfZ2RkfffQRzp8/j/Pnz+OVV17BuHHjcOXKFX03zaCdO3cOcXFx6N69u76bYrC6dOmC7Oxs+c+lS5f03SSDVFBQgAEDBoDP5+OXX37B1atXsXHjRlhZWemlPbQEgAHx9fVF7969ERsbK9/m5eWF8ePHY926dXpsmWHjcDj44YcfMH78eH03pUl49OgRHBwccOLECQwePFjfzTF4NjY2+PjjjxESEqLvphikkpIS9O7dG1u2bMEHH3yAnj17IiYmRt/NMiirVq3CgQMHcPHiRX03xeBFRETg1KlTBjOKQj1JBqKqqgppaWnw9/dX2O7v74+UlBQ9tYo0R4WFhQBqvvxJ3SQSCfbu3YvS0lL4+fnpuzkGa+HChRg9ejReffVVfTfFoN28eROtW7eGm5sbJk+ejIyMDH03ySD9+OOP8PHxwRtvvAEHBwf06tUL27Zt01t7KEkyEHl5eZBIJHB0dFTY7ujoiJycHD21ijQ3jDGEh4dj4MCB6Nq1q76bY5AuXboEMzMzCAQChIaG4ocffkDnzp313SyDtHfvXly4cIF6uhvg6+uLXbt24ciRI9i2bRtycnLQv39/5Ofn67tpBicjIwOxsbHo0KEDjhw5gtDQUCxevBi7du3SS3uM9HJUUicOh6PwmDGmtI0QbS1atAh///03kpOT9d0Ug+Xp6YmLFy/iyZMn2L9/P2bOnIkTJ05QovSce/fuYcmSJf+/vbsNaeptwAB+HZ1b2xjhWzkpTdLyJZPaIjaLqEG4QLAWRiyb9UE0FUn80rtF2JfoDWIwKAkyBKEXSzS1zA9CGMVKbL1BLx9CVhTkivYh7/+HYLD/zvM8PVmeM7l+cOCcc+9s1wbC5b17G/r7+zFnzhyl46ia0+mM7BcXF8Nms2Hx4sW4dOkSmpubFUymPlNTU7BarWhrawMArFixAuPj4/B6vdi5c+eM5+FMkkqkpaUhMTExZtYoGAzGzC4R/Y7GxkZ0d3djaGgICxYsUDqOamm1WuTm5sJqteLEiRMoKSnB2bNnlY6lOg8fPkQwGITFYoFGo4FGo8Hw8DDOnTsHjUaDHz9+KB1RtYxGI4qLi/Hy5Uulo6iO2WyO+YekoKBAsQ8wsSSphFarhcViwcDAQNT5gYEB2O12hVLRbCCEQENDA65evYq7d+8iJydH6UhxRQiBcDisdAzVcTgcGBsbg9/vj2xWqxVutxt+vx+JiYlKR1StcDiMQCAAs9msdBTVKS0tjfmKkhcvXiA7O1uRPHy7TUWam5tRVVUFq9UKm80Gn8+Hd+/eoba2VuloqhMKhfDq1avI8evXr+H3+5GSkoKsrCwFk6lPfX09rly5ghs3bsBkMkVmK+fOnQu9Xq9wOnXZv38/nE4nFi5ciMnJSXR2duLevXvo6+tTOprqmEymmHVtRqMRqampXO/2Ly0tLSgvL0dWVhaCwSCOHz+OL1++wOPxKB1Ndfbu3Qu73Y62tjZUVlZidHQUPp8PPp9PmUCCVOX8+fMiOztbaLVasXLlSjE8PKx0JFUaGhoSAGI2j8ejdDTVkXudAIj29nalo6nO7t27I39/6enpwuFwiP7+fqVjxY1169aJpqYmpWOozrZt24TZbBZJSUkiMzNTbNmyRYyPjysdS7Vu3rwpli1bJnQ6ncjPzxc+n0+xLPyeJCIiIiIZXJNEREREJIMliYiIiEgGSxIRERGRDJYkIiIiIhksSUREREQyWJKIiIiIZLAkEREREclgSSIimgZJknD9+nWlYxDRX8CSRERxq7q6GpIkxWxlZWVKRyOiWYC/3UZEca2srAzt7e1R53Q6nUJpiGg24UwSEcU1nU6HjIyMqC05ORnAz7fCvF4vnE4n9Ho9cnJy0NXVFXX92NgYNmzYAL1ej9TUVNTU1CAUCkXd5uLFiygqKoJOp4PZbEZDQ0PU+MePH7F582YYDAbk5eWhu7s7Mvb582e43W6kp6dDr9cjLy8vptQRkTqxJBHRrHbo0CG4XC48fvwYO3bswPbt2xEIBAAA3759Q1lZGZKTk/HgwQN0dXVhcHAwqgR5vV7U19ejpqYGY2Nj6O7uRm5ubtRjHD16FJWVlXjy5Ak2bdoEt9uNT58+RR7/6dOn6O3tRSAQgNfrRVpa2sy9AET0+xT7aV0iomnyeDwiMTFRGI3GqO3YsWNCCCEAiNra2qhrVq9eLerq6oQQQvh8PpGcnCxCoVBkvKenRyQkJIiJiQkhhBCZmZniwIED/zEDAHHw4MHIcSgUEpIkid7eXiGEEOXl5WLXrl1/5gkT0YzimiQiimvr16+H1+uNOpeSkhLZt9lsUWM2mw1+vx8AEAgEUFJSAqPRGBkvLS3F1NQUnj9/DkmS8P79ezgcjv+aYfny5ZF9o9EIk8mEYDAIAKirq4PL5cKjR4+wceNGVFRUwG63/9ZzJaKZxZJERHHNaDTGvP31v0iSBAAQQkT25W6j1+t/6f6SkpJirp2amgIAOJ1OvH37Fj09PRgcHITD4UB9fT1Onjz5f2UmopnHNUlENKvdv38/5jg/Px8AUFhYCL/fj69fv0bGR0ZGkJCQgCVLlsBkMmHRokW4c+fOtDKkp6ejuroaly9fxpkzZ+Dz+aZ1f0Q0MziTRERxLRwOY2JiIuqcRqOJLI7u6uqC1WrFmjVr0NHRgdHRUVy4cAEA4Ha7ceTIEXg8HrS2tuLDhw9obGxEVVUV5s+fDwBobW1FbW0t5s2bB6fTicnJSYyMjKCxsfGX8h0+fBgWiwVFRUUIh8O4desWCgoK/uArQER/C0sSEcW1vr4+mM3mqHNLly7Fs2fPAPz85FlnZyf27NmDjIwMdHR0oLCwEABgMBhw+/ZtNDU1YdWqVTAYDHC5XDh16lTkvjweD75//47Tp0+jpaUFaWlp2Lp16y/n02q12LdvH968eQO9Xo+1a9eis7PzDzxzIvrbJCGEUDoEEdHfIEkSrl27hoqKCqWjEFEc4pokIiIiIhksSUREREQyuCaJiGYtriYgoungTBIRERGRDJYkIiIiIhksSUREREQyWJKIiIiIZLAkEREREclgSSIiIiKSwZJEREREJIMliYiIiEgGSxIRERGRjH8A8+z4rHyhl6oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting loss\n",
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"binary_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864d7ce",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cebc1c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 4s 203ms/step - loss: 0.0172 - binary_accuracy: 0.9949\n",
      "16/16 [==============================] - 3s 151ms/step - loss: 0.0184 - binary_accuracy: 0.9945\n",
      "Categorical accuracy on the test set: 99.49%.\n",
      "Categorical accuracy on the validation set: 99.45%.\n"
     ]
    }
   ],
   "source": [
    "# model evaltuation on test and val dataset\n",
    "_, binary_acc1 = model1.evaluate(test_dataset)\n",
    "_, binary_acc2 = model1.evaluate(validation_dataset)\n",
    "\n",
    "print(f\"Categorical accuracy on the test set: {round(binary_acc1 * 100, 2)}%.\")\n",
    "print(f\"Categorical accuracy on the validation set: {round(binary_acc2 * 100, 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f447f1",
   "metadata": {},
   "source": [
    "## Save Model and Text Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4434b752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AmiMistry\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save the model\n",
    "model1.save(\"models/model.h5\")\n",
    "\n",
    "# Save the configuration of the text vectorizer\n",
    "saved_text_vectorizer_config = text_vectorizer.get_config()\n",
    "with open(\"models/text_vectorizer_config.pkl\", \"wb\") as f:\n",
    "    pickle.dump(saved_text_vectorizer_config, f)\n",
    "\n",
    "\n",
    "# Save the vocabulary\n",
    "with open(\"models/vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceee66a",
   "metadata": {},
   "source": [
    "## Load Model and Text Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34e7d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"models/modelt.h5\")\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Load the configuration of the text vectorizer\n",
    "with open(\"models/text_vectorizer_configt.pkl\", \"rb\") as f:\n",
    "    saved_text_vectorizer_config = pickle.load(f)\n",
    "\n",
    "# Create a new TextVectorization layer with the saved configuration\n",
    "loaded_text_vectorizer = TextVectorization.from_config(saved_text_vectorizer_config)\n",
    "\n",
    "# Load the saved weights into the new TextVectorization layer\n",
    "with open(\"text_vectorizer_weights.pkl\", \"rb\") as f:\n",
    "    weights = pickle.load(f)\n",
    "\n",
    "# Assuming loaded_text_vectorizer is your TextVectorization layer\n",
    "# Check the expected number of weights in the loaded_text_vectorizer\n",
    "expected_num_weights = len(loaded_text_vectorizer.weights)\n",
    "\n",
    "if len(weights) != expected_num_weights:\n",
    "    print(f\"Expected {expected_num_weights} weights, but found {len(weights)}.\")\n",
    "    print(\"Make sure the weights file matches the structure of the loaded_text_vectorizer.\")\n",
    "else:\n",
    "    loaded_text_vectorizer.set_weights(weights)\n",
    "    print(\"Weights loaded successfully.\")\n",
    "\"\"\"\n",
    "    # this is the original code above 3 cells 30,32,33 are the replaced code\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"models/model.h5\")\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Load the configuration of the text vectorizer\n",
    "with open(\"text_vectorizer_config.pkl\", \"rb\") as f:\n",
    "    saved_text_vectorizer_config = pickle.load(f)\n",
    "\n",
    "# Create a new TextVectorization layer with the saved configuration\n",
    "loaded_text_vectorizer = TextVectorization.from_config(saved_text_vectorizer_config)\n",
    "\n",
    "# Load the saved weights into the new TextVectorization layer\n",
    "with open(\"text_vectorizer_weights.pkl\", \"rb\") as f:\n",
    "    weights = pickle.load(f)\n",
    "    loaded_text_vectorizer.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27e3fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary\n",
    "with open(\"models/vocab.pkl\", \"rb\") as f:\n",
    "    loaded_vocab = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f9a17",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e2985e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_multi_hot(encoded_labels):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
    "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
    "    return np.take(loaded_vocab, hot_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76f5b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def adjust_shape(preprocessed_abstract, target_shape):\n",
    "    current_shape = preprocessed_abstract.shape[1]\n",
    "    if current_shape < target_shape:\n",
    "        # Pad with zeros if the current shape is smaller\n",
    "        padding = np.zeros((preprocessed_abstract.shape[0], target_shape - current_shape))\n",
    "        adjusted_abstract = np.hstack((preprocessed_abstract, padding))\n",
    "    elif current_shape > target_shape:\n",
    "        # Trim if the current shape is larger\n",
    "        adjusted_abstract = preprocessed_abstract[:, :target_shape]\n",
    "    else:\n",
    "        adjusted_abstract = preprocessed_abstract\n",
    "    return adjusted_abstract\n",
    "\n",
    "def predict_category(abstract, model, vectorizer, label_lookup):\n",
    "    # Preprocess the abstract using the loaded text vectorizer\n",
    "    preprocessed_abstract = vectorizer([abstract])  # Using vectorizer as a callable to preprocess the text\n",
    "\n",
    "    # Debugging information to check the shapes\n",
    "    print(f\"Shape of preprocessed abstract: {preprocessed_abstract.shape}\")\n",
    "    print(f\"Expected input shape of the model: {model.input_shape}\")\n",
    "\n",
    "    # Adjust the shape to match the model input shape\n",
    "    adjusted_abstract = adjust_shape(preprocessed_abstract, model.input_shape[1])\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(adjusted_abstract)\n",
    "\n",
    "    # Convert predictions to human-readable labels\n",
    "    predicted_labels = label_lookup(np.round(predictions).astype(int)[0])\n",
    "\n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f91a6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of preprocessed abstract: (1, 100)\n",
      "Expected input shape of the model: (None, 159102)\n",
      "1/1 [==============================] - 0s 470ms/step\n",
      "Predicted Categories: ['cs.CV' 'cs.LG']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "new_abstract = '''Deep networks and decision forests (such as random forests and gradient\n",
    "boosted trees) are the leading machine learning methods for structured and\n",
    "tabular data, respectively. Many papers have empirically compared large numbers\n",
    "of classifiers on one or two different domains (e.g., on 100 different tabular\n",
    "data settings). However, a careful conceptual and empirical comparison of these\n",
    "two strategies using the most contemporary best practices has yet to be\n",
    "performed. Conceptually, we illustrate that both can be profitably viewed as\n",
    "\"partition and vote\" schemes. Specifically, the representation space that they\n",
    "both learn is a partitioning of feature space into a union of convex polytopes.\n",
    "For inference, each decides on the basis of votes from the activated nodes.\n",
    "This formulation allows for a unified basic understanding of the relationship\n",
    "between these methods. Empirically, we compare these two strategies on hundreds\n",
    "of tabular data settings, as well as several vision and auditory settings. Our\n",
    "focus is on datasets with at most 10,000 samples, which represent a large\n",
    "fraction of scientific and biomedical datasets. In general, we found forests to\n",
    "excel at tabular and structured data (vision and audition) with small sample\n",
    "sizes, whereas deep nets performed better on structured data with larger sample\n",
    "sizes. This suggests that further gains in both scenarios may be realized via\n",
    "further combining aspects of forests and networks. We will continue revising\n",
    "this technical report in the coming months with updated results.'''\n",
    "\n",
    "# Assuming 'loaded_model', 'loaded_text_vectorizer', and 'invert_multi_hot' are defined and loaded appropriately\n",
    "try:\n",
    "    predicted_categories = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
    "    print(\"Predicted Categories:\", predicted_categories)\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e13ef48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of preprocessed abstract: (1, 100)\n",
      "Expected input shape of the model: (None, 159102)\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Predicted Categories: ['cs.CV' 'cs.LG']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "new_abstract = 'Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.'\n",
    "predicted_categories = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
    "print(\"Predicted Categories:\", predicted_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77b90c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# great resutls..................................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c90e37",
   "metadata": {},
   "source": [
    "# =======Section 2========"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfe2d1",
   "metadata": {},
   "source": [
    "## 2 Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c94e6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.drop_duplicates(inplace= True)\n",
    "arxiv_data.reset_index(drop= True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ca5e456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes</td>\n",
       "      <td>Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network via Graph Powering</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerful tools for graph-structured\\ndata. However, they have been recently shown to be vulnerable to topological\\nattacks. To enhance adversarial robustness, we go beyond spectral graph theory\\nto robust graph theory. By challenging the classical graph Laplacian, we\\npropose a new convolution operator that is provably robust in the spectral\\ndomain and is incorporated in the GCN architecture to improve expressivity and\\ninterpretability. By extending the original graph to a sequence of graphs, we\\nalso propose a robust training paradigm that encourages transferability across\\ngraphs that span a range of spatial and spectral characteristics. The proposed\\napproaches are demonstrated in extensive experiments to simultaneously improve\\nperformance in both benign and adversarial situations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Differential Privacy Guarantees</td>\n",
       "      <td>With the increasing popularity of Graph Neural Networks (GNNs) in several\\nsensitive applications like healthcare and medicine, concerns have been raised\\nover the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to\\nprivacy attacks, such as membership inference attacks, even if only blackbox\\naccess to the trained model is granted. To build defenses, differential privacy\\nhas emerged as a mechanism to disguise the sensitive data in training datasets.\\nFollowing the strategy of Private Aggregation of Teacher Ensembles (PATE),\\nrecent methods leverage a large ensemble of teacher models. These teachers are\\ntrained on disjoint subsets of private data and are employed to transfer\\nknowledge to a student model, which is then released with privacy guarantees.\\nHowever, splitting graph data into many disjoint training sets may destroy the\\nstructural information and adversely affect accuracy. We propose a new\\ngraph-specific scheme of releasing a student GNN, which avoids splitting\\nprivate training data altogether. The student GNN is trained using public data,\\npartly labeled privately using the teacher GNN models trained exclusively for\\neach query node. We theoretically analyze our approach in the R\\`{e}nyi\\ndifferential privacy framework and provide privacy guarantees. Besides, we show\\nthe solid experimental performance of our method compared to several baselines,\\nincluding the PATE baseline adapted for graph-structured data. Our anonymized\\ncode is available.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification</td>\n",
       "      <td>Machine learning solutions for pattern classification problems are nowadays\\nwidely deployed in society and industry. However, the lack of transparency and\\naccountability of most accurate models often hinders their safe use. Thus,\\nthere is a clear need for developing explainable artificial intelligence\\nmechanisms. There exist model-agnostic methods that summarize feature\\ncontributions, but their interpretability is limited to predictions made by\\nblack-box models. An open challenge is to develop models that have intrinsic\\ninterpretability and produce their own explanations, even for classes of models\\nthat are traditionally considered black boxes like (recurrent) neural networks.\\nIn this paper, we propose a Long-Term Cognitive Network for interpretable\\npattern classification of structured data. Our method brings its own mechanism\\nfor providing explanations by quantifying the relevance of each feature in the\\ndecision process. For supporting the interpretability without affecting the\\nperformance, the model incorporates more flexibility through a quasi-nonlinear\\nreasoning rule that allows controlling nonlinearity. Besides, we propose a\\nrecurrence-aware decision model that evades the issues posed by unique fixed\\npoints while introducing a deterministic learning method to compute the tunable\\nparameters. The simulations show that our interpretable model obtains\\ncompetitive results when compared to the state-of-the-art white and black-box\\nmodels.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41100</th>\n",
       "      <td>['stat.ML']</td>\n",
       "      <td>An experimental study of graph-based semi-supervised classification with additional node information</td>\n",
       "      <td>The volume of data generated by internet and social networks is increasing\\nevery day, and there is a clear need for efficient ways of extracting useful\\ninformation from them. As those data can take different forms, it is important\\nto use all the available data representations for prediction.\\n  In this paper, we focus our attention on supervised classification using both\\nregular plain, tabular, data and structural information coming from a network\\nstructure. 14 techniques are investigated and compared in this study and can be\\ndivided in three classes: the first one uses only the plain data to build a\\nclassification model, the second uses only the graph structure and the last\\nuses both information sources. The relative performances in these three cases\\nare investigated. Furthermore, the effect of using a graph embedding and\\nwell-known indicators in spatial statistics is also studied.\\n  Possible applications are automatic classification of web pages or other\\nlinked documents, of people in a social network or of proteins in a biological\\ncomplex system, to name a few.\\n  Based on our comparison, we draw some general conclusions and advices to\\ntackle this particular classification task: some datasets can be better\\nexplained by their graph structure (graph-driven), or by their feature set\\n(features-driven). The most efficient methods are discussed in both cases.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41101</th>\n",
       "      <td>['stat.ML', 'cs.LG']</td>\n",
       "      <td>Bayesian Differential Privacy through Posterior Sampling</td>\n",
       "      <td>Differential privacy formalises privacy-preserving mechanisms that provide\\naccess to a database. We pose the question of whether Bayesian inference itself\\ncan be used directly to provide private access to data, with no modification.\\nThe answer is affirmative: under certain conditions on the prior, sampling from\\nthe posterior distribution can be used to achieve a desired level of privacy\\nand utility. To do so, we generalise differential privacy to arbitrary dataset\\nmetrics, outcome spaces and distribution families. This allows us to also deal\\nwith non-i.i.d or non-tabular datasets. We prove bounds on the sensitivity of\\nthe posterior to the data, which gives a measure of robustness. We also show\\nhow to use posterior sampling to provide differentially private responses to\\nqueries, within a decision-theoretic framework. Finally, we provide bounds on\\nthe utility and on the distinguishability of datasets. The latter are\\ncomplemented by a novel use of Le Cam's method to obtain lower bounds. All our\\ngeneral results hold for arbitrary database metrics, including those for the\\ncommon definition of differential privacy. For specific choices of the metric,\\nwe give a number of examples satisfying our assumptions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41102</th>\n",
       "      <td>['cs.CV', 'cs.IR']</td>\n",
       "      <td>Mining Spatio-temporal Data on Industrialization from Historical Registries</td>\n",
       "      <td>Despite the growing availability of big data in many fields, historical data\\non socioevironmental phenomena are often not available due to a lack of\\nautomated and scalable approaches for collecting, digitizing, and assembling\\nthem. We have developed a data-mining method for extracting tabulated, geocoded\\ndata from printed directories. While scanning and optical character recognition\\n(OCR) can digitize printed text, these methods alone do not capture the\\nstructure of the underlying data. Our pipeline integrates both page layout\\nanalysis and OCR to extract tabular, geocoded data from structured text. We\\ndemonstrate the utility of this method by applying it to scanned manufacturing\\nregistries from Rhode Island that record 41 years of industrial land use. The\\nresulting spatio-temporal data can be used for socioenvironmental analyses of\\nindustrialization at a resolution that was not previously possible. In\\nparticular, we find strong evidence for the dispersion of manufacturing from\\nthe urban core of Providence, the state's capital, along the Interstate 95\\ncorridor to the north and south.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41103</th>\n",
       "      <td>['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']</td>\n",
       "      <td>Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</td>\n",
       "      <td>This paper presents a simple end-to-end model for speech recognition,\\ncombining a convolutional network based acoustic model and a graph decoding. It\\nis trained to output letters, with transcribed speech, without the need for\\nforce alignment of phonemes. We introduce an automatic segmentation criterion\\nfor training from sequence annotation without alignment that is on par with CTC\\nwhile being simpler. We show competitive results in word error rate on the\\nLibrispeech corpus with MFCC features, and promising results from raw waveform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41104</th>\n",
       "      <td>['stat.ML', 'cs.LG', 'math.OC']</td>\n",
       "      <td>Generalized Low Rank Models</td>\n",
       "      <td>Principal components analysis (PCA) is a well-known technique for\\napproximating a tabular data set by a low rank matrix. Here, we extend the idea\\nof PCA to handle arbitrary data sets consisting of numerical, Boolean,\\ncategorical, ordinal, and other data types. This framework encompasses many\\nwell known techniques in data analysis, such as nonnegative matrix\\nfactorization, matrix completion, sparse and robust PCA, $k$-means, $k$-SVD,\\nand maximum margin matrix factorization. The method handles heterogeneous data\\nsets, and leads to coherent schemes for compressing, denoising, and imputing\\nmissing entries across all data types simultaneously. It also admits a number\\nof interesting interpretations of the low rank factors, which allow clustering\\nof examples or of features. We propose several parallel algorithms for fitting\\ngeneralized low rank models, and describe implementations and numerical\\nresults.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41105 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             terms  \\\n",
       "0                                        ['cs.LG']   \n",
       "1                               ['cs.LG', 'cs.AI']   \n",
       "2                    ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3                               ['cs.LG', 'cs.CR']   \n",
       "4                                        ['cs.LG']   \n",
       "...                                            ...   \n",
       "41100                                  ['stat.ML']   \n",
       "41101                         ['stat.ML', 'cs.LG']   \n",
       "41102                           ['cs.CV', 'cs.IR']   \n",
       "41103  ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']   \n",
       "41104              ['stat.ML', 'cs.LG', 'math.OC']   \n",
       "\n",
       "                                                                                                                 titles  \\\n",
       "0      Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities   \n",
       "1           Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes   \n",
       "2                                                       Power up! Robust Graph Convolutional Network via Graph Powering   \n",
       "3                                                  Releasing Graph Neural Networks with Differential Privacy Guarantees   \n",
       "4                                   Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification   \n",
       "...                                                                                                                 ...   \n",
       "41100              An experimental study of graph-based semi-supervised classification with additional node information   \n",
       "41101                                                          Bayesian Differential Privacy through Posterior Sampling   \n",
       "41102                                       Mining Spatio-temporal Data on Industrialization from Historical Registries   \n",
       "41103                                                 Wav2Letter: an End-to-End ConvNet-based Speech Recognition System   \n",
       "41104                                                                                       Generalized Low Rank Models   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              abstracts  \n",
       "0      Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.  \n",
       "1                                                                                                                                                                  Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Graph convolutional networks (GCNs) are powerful tools for graph-structured\\ndata. However, they have been recently shown to be vulnerable to topological\\nattacks. To enhance adversarial robustness, we go beyond spectral graph theory\\nto robust graph theory. By challenging the classical graph Laplacian, we\\npropose a new convolution operator that is provably robust in the spectral\\ndomain and is incorporated in the GCN architecture to improve expressivity and\\ninterpretability. By extending the original graph to a sequence of graphs, we\\nalso propose a robust training paradigm that encourages transferability across\\ngraphs that span a range of spatial and spectral characteristics. The proposed\\napproaches are demonstrated in extensive experiments to simultaneously improve\\nperformance in both benign and adversarial situations.  \n",
       "3                                                                                                                                                                                                                        With the increasing popularity of Graph Neural Networks (GNNs) in several\\nsensitive applications like healthcare and medicine, concerns have been raised\\nover the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to\\nprivacy attacks, such as membership inference attacks, even if only blackbox\\naccess to the trained model is granted. To build defenses, differential privacy\\nhas emerged as a mechanism to disguise the sensitive data in training datasets.\\nFollowing the strategy of Private Aggregation of Teacher Ensembles (PATE),\\nrecent methods leverage a large ensemble of teacher models. These teachers are\\ntrained on disjoint subsets of private data and are employed to transfer\\nknowledge to a student model, which is then released with privacy guarantees.\\nHowever, splitting graph data into many disjoint training sets may destroy the\\nstructural information and adversely affect accuracy. We propose a new\\ngraph-specific scheme of releasing a student GNN, which avoids splitting\\nprivate training data altogether. The student GNN is trained using public data,\\npartly labeled privately using the teacher GNN models trained exclusively for\\neach query node. We theoretically analyze our approach in the R\\`{e}nyi\\ndifferential privacy framework and provide privacy guarantees. Besides, we show\\nthe solid experimental performance of our method compared to several baselines,\\nincluding the PATE baseline adapted for graph-structured data. Our anonymized\\ncode is available.  \n",
       "4                                                                                                                                                                                                                                               Machine learning solutions for pattern classification problems are nowadays\\nwidely deployed in society and industry. However, the lack of transparency and\\naccountability of most accurate models often hinders their safe use. Thus,\\nthere is a clear need for developing explainable artificial intelligence\\nmechanisms. There exist model-agnostic methods that summarize feature\\ncontributions, but their interpretability is limited to predictions made by\\nblack-box models. An open challenge is to develop models that have intrinsic\\ninterpretability and produce their own explanations, even for classes of models\\nthat are traditionally considered black boxes like (recurrent) neural networks.\\nIn this paper, we propose a Long-Term Cognitive Network for interpretable\\npattern classification of structured data. Our method brings its own mechanism\\nfor providing explanations by quantifying the relevance of each feature in the\\ndecision process. For supporting the interpretability without affecting the\\nperformance, the model incorporates more flexibility through a quasi-nonlinear\\nreasoning rule that allows controlling nonlinearity. Besides, we propose a\\nrecurrence-aware decision model that evades the issues posed by unique fixed\\npoints while introducing a deterministic learning method to compute the tunable\\nparameters. The simulations show that our interpretable model obtains\\ncompetitive results when compared to the state-of-the-art white and black-box\\nmodels.  \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...  \n",
       "41100                                                                                                                                                                                                                                                                                                                                The volume of data generated by internet and social networks is increasing\\nevery day, and there is a clear need for efficient ways of extracting useful\\ninformation from them. As those data can take different forms, it is important\\nto use all the available data representations for prediction.\\n  In this paper, we focus our attention on supervised classification using both\\nregular plain, tabular, data and structural information coming from a network\\nstructure. 14 techniques are investigated and compared in this study and can be\\ndivided in three classes: the first one uses only the plain data to build a\\nclassification model, the second uses only the graph structure and the last\\nuses both information sources. The relative performances in these three cases\\nare investigated. Furthermore, the effect of using a graph embedding and\\nwell-known indicators in spatial statistics is also studied.\\n  Possible applications are automatic classification of web pages or other\\nlinked documents, of people in a social network or of proteins in a biological\\ncomplex system, to name a few.\\n  Based on our comparison, we draw some general conclusions and advices to\\ntackle this particular classification task: some datasets can be better\\nexplained by their graph structure (graph-driven), or by their feature set\\n(features-driven). The most efficient methods are discussed in both cases.  \n",
       "41101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Differential privacy formalises privacy-preserving mechanisms that provide\\naccess to a database. We pose the question of whether Bayesian inference itself\\ncan be used directly to provide private access to data, with no modification.\\nThe answer is affirmative: under certain conditions on the prior, sampling from\\nthe posterior distribution can be used to achieve a desired level of privacy\\nand utility. To do so, we generalise differential privacy to arbitrary dataset\\nmetrics, outcome spaces and distribution families. This allows us to also deal\\nwith non-i.i.d or non-tabular datasets. We prove bounds on the sensitivity of\\nthe posterior to the data, which gives a measure of robustness. We also show\\nhow to use posterior sampling to provide differentially private responses to\\nqueries, within a decision-theoretic framework. Finally, we provide bounds on\\nthe utility and on the distinguishability of datasets. The latter are\\ncomplemented by a novel use of Le Cam's method to obtain lower bounds. All our\\ngeneral results hold for arbitrary database metrics, including those for the\\ncommon definition of differential privacy. For specific choices of the metric,\\nwe give a number of examples satisfying our assumptions.  \n",
       "41102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Despite the growing availability of big data in many fields, historical data\\non socioevironmental phenomena are often not available due to a lack of\\nautomated and scalable approaches for collecting, digitizing, and assembling\\nthem. We have developed a data-mining method for extracting tabulated, geocoded\\ndata from printed directories. While scanning and optical character recognition\\n(OCR) can digitize printed text, these methods alone do not capture the\\nstructure of the underlying data. Our pipeline integrates both page layout\\nanalysis and OCR to extract tabular, geocoded data from structured text. We\\ndemonstrate the utility of this method by applying it to scanned manufacturing\\nregistries from Rhode Island that record 41 years of industrial land use. The\\nresulting spatio-temporal data can be used for socioenvironmental analyses of\\nindustrialization at a resolution that was not previously possible. In\\nparticular, we find strong evidence for the dispersion of manufacturing from\\nthe urban core of Providence, the state's capital, along the Interstate 95\\ncorridor to the north and south.  \n",
       "41103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This paper presents a simple end-to-end model for speech recognition,\\ncombining a convolutional network based acoustic model and a graph decoding. It\\nis trained to output letters, with transcribed speech, without the need for\\nforce alignment of phonemes. We introduce an automatic segmentation criterion\\nfor training from sequence annotation without alignment that is on par with CTC\\nwhile being simpler. We show competitive results in word error rate on the\\nLibrispeech corpus with MFCC features, and promising results from raw waveform.  \n",
       "41104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Principal components analysis (PCA) is a well-known technique for\\napproximating a tabular data set by a low rank matrix. Here, we extend the idea\\nof PCA to handle arbitrary data sets consisting of numerical, Boolean,\\ncategorical, ordinal, and other data types. This framework encompasses many\\nwell known techniques in data analysis, such as nonnegative matrix\\nfactorization, matrix completion, sparse and robust PCA, $k$-means, $k$-SVD,\\nand maximum margin matrix factorization. The method handles heterogeneous data\\nsets, and leads to coherent schemes for compressing, denoising, and imputing\\nmissing entries across all data types simultaneously. It also admits a number\\nof interesting interpretations of the low rank factors, which allow clustering\\nof examples or of features. We propose several parallel algorithms for fitting\\ngeneralized low rank models, and describe implementations and numerical\\nresults.  \n",
       "\n",
       "[41105 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "arxiv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b71915",
   "metadata": {},
   "source": [
    "## Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5712474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce14f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "514a5734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AmiMistry\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe embeddings can be used for various natural language processing (NLP) tasks, \\nsuch as similarity search, clustering\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This imports the SentenceTransformer class from the Sentence Transformers library.\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "# we load all-MiniLM-L6-v2, which is a MiniLM model fine tuned on a large dataset of over \n",
    "# 1 billion training pairs.\n",
    "#This initializes the 'all-MiniLM-L6-v2' model from Sentence Transformers. \n",
    "# This model is capable of encoding sentences into fixed-size vectors (embeddings).\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#Our sentences we like to encode\n",
    "sentences = arxiv_data['titles']\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "\"\"\"\n",
    "The embeddings can be used for various natural language processing (NLP) tasks, \n",
    "such as similarity search, clustering\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c502c4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06643408, -0.049546  ,  0.06388088, ...,  0.001063  ,\n",
       "        -0.12156384, -0.06962776],\n",
       "       [ 0.09212258, -0.07606939,  0.06572871, ..., -0.08565163,\n",
       "        -0.09266548,  0.00725296],\n",
       "       [-0.0816268 ,  0.02428928,  0.01888745, ...,  0.0080616 ,\n",
       "        -0.0512953 , -0.05873991],\n",
       "       ...,\n",
       "       [ 0.01227985, -0.08568835, -0.02782775, ..., -0.05257969,\n",
       "        -0.10806689,  0.07843314],\n",
       "       [-0.07258201, -0.12690918, -0.00535555, ...,  0.03597698,\n",
       "        -0.03986151, -0.0597103 ],\n",
       "       [ 0.0076887 , -0.10124177,  0.08909855, ..., -0.0819987 ,\n",
       "        -0.05649741,  0.09007055]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db125c7d",
   "metadata": {},
   "source": [
    "## Why select all-MiniLM-L6-v2?\n",
    "\n",
    "All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs. Source\n",
    "\n",
    "Its small in size 80 MB with good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ffc70",
   "metadata": {},
   "source": [
    "## Print the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0af0618d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Power up! Robust Graph Convolutional Network via Graph Powering\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Releasing Graph Neural Networks with Differential Privacy Guarantees\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Lifelong Graph Learning\n",
      "Embedding length: 384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "#This loop iterates over pairs of sentences and their corresponding embeddings. \n",
    "#zip is used to iterate over both lists simultaneously.\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding length:\", len(embedding)) # list of floats\n",
    "    print(\"\")\n",
    "    # Breaks out of the loop after printing information for the first 5 sentences.\n",
    "    if c >=5:\n",
    "        break\n",
    "    c +=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cdb484",
   "metadata": {},
   "source": [
    "## Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a802f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Saving sentences and corresponding embeddings\n",
    "with open('embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "with open('sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)\n",
    "    \n",
    "with open('rec_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcea57f",
   "metadata": {},
   "source": [
    "## Recommendation for similar papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcb1a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load save files\n",
    "embeddings = pickle.load(open('embeddings.pkl','rb'))\n",
    "sentences = pickle.load(open('sentences.pkl','rb'))\n",
    "rec_model = pickle.load(open('rec_model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84ffe218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def recommendation(input_paper):\n",
    "    # Calculate cosine similarity scores between the embeddings of input_paper and all papers in the dataset.\n",
    "    cosine_scores = util.cos_sim(embeddings, rec_model.encode(input_paper))\n",
    "    \n",
    "    # Get the indices of the top-k most similar papers based on cosine similarity.\n",
    "    top_similar_papers = torch.topk(cosine_scores, dim=0, k=5, sorted=True)\n",
    "                                 \n",
    "    # Retrieve the titles of the top similar papers.\n",
    "    papers_list = []\n",
    "    for i in top_similar_papers.indices:\n",
    "        papers_list.append(sentences[i.item()])\n",
    "    \n",
    "    return papers_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12c90388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommend to read this paper............\n",
      "=============================================\n",
      "Attention that does not Explain Away\n",
      "Area Attention\n",
      "Pay Attention when Required\n",
      "Long Short-Term Attention\n",
      "Attention as Activation\n"
     ]
    }
   ],
   "source": [
    "# exampel usage 1: (use this paper as input (\\Attention is all you need))\n",
    "input_paper = input(\"Enter the title of any paper you like\")\n",
    "recommend_papers = recommendation(input_paper)\n",
    "\n",
    "\n",
    "print(\"We recommend to read this paper............\")\n",
    "print(\"=============================================\")\n",
    "for paper in recommend_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92d70ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommend to read this paper............\n",
      "=============================================\n",
      "BEiT: BERT Pre-Training of Image Transformers\n",
      "VL-BERT: Pre-training of Generic Visual-Linguistic Representations\n",
      "Sketch-BERT: Learning Sketch Bidirectional Encoder Representation from Transformers by Self-supervised Learning of Sketch Gestalt\n",
      "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning\n",
      "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping\n"
     ]
    }
   ],
   "source": [
    "# exampel usage 2: (use this paper as input (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding))\n",
    "input_paper = input(\"Enter the title of any paper you like\")\n",
    "recommend_papers = recommendation(input_paper)\n",
    "\n",
    "\n",
    "print(\"We recommend to read this paper............\")\n",
    "print(\"=============================================\")\n",
    "for paper in recommend_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe398bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommend to read this paper............\n",
      "=============================================\n",
      "A Review of Deep Learning with Special Emphasis on Architectures, Applications and Recent Trends\n",
      "Review of Deep Learning\n",
      "Deep Convolutional Neural Networks: A survey of the foundations, selected improvements, and some current applications\n",
      "A Survey of the Recent Architectures of Deep Convolutional Neural Networks\n",
      "A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects\n"
     ]
    }
   ],
   "source": [
    "# exampel usage 3: (use this paper as input (Review of deep learning: concepts, CNN architectures, challenges, applications, future directions))\n",
    "input_paper = input(\"Enter the title of any paper you like\")\n",
    "recommend_papers = recommendation(input_paper)\n",
    "\n",
    "\n",
    "print(\"We recommend to read this paper............\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "for paper in recommend_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "026a1eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n",
      "2.7.0\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# install this versions\n",
    "import sentence_transformers\n",
    "import tensorflow\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(sentence_transformers.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813491da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
